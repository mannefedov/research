{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def n_grams(a, n):\n",
    "    z = (islice(a, i, None) for i in range(n))\n",
    "    return zip(*z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xml = open('data/1.1.text.xml', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = etree.fromstring(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = tree.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "for text in texts:\n",
    "    title, abstract = text.getchildren()\n",
    "    \n",
    "    title = title.text\n",
    "    ab_text = abstract.itertext()\n",
    "    abstract = list(abstract)\n",
    "    sequence = []\n",
    "    for ab in ab_text:\n",
    "        for el in abstract:\n",
    "            if el.text == ab:\n",
    "                sequence.append((ab, el.get('id')))\n",
    "                _ = abstract.pop(abstract.index(el))\n",
    "                break\n",
    "        else:\n",
    "            sequence.append((ab, 'O'))\n",
    "    articles.append((title, sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(articles, open('train_articles.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Activity detection for information access to oral communication',\n",
       " [(' ', 'O'),\n",
       "  ('Oral communication', 'H01-1001.1'),\n",
       "  (' is ubiquitous and carries important information yet it is also time consuming to document. Given the development of ',\n",
       "   'O'),\n",
       "  ('storage media and networks', 'H01-1001.2'),\n",
       "  (' one could just record and store a ', 'O'),\n",
       "  ('conversation', 'H01-1001.3'),\n",
       "  (' for documentation. The question is, however, how an interesting information piece would be found in a ',\n",
       "   'O'),\n",
       "  ('large database', 'H01-1001.4'),\n",
       "  (' . Traditional ', 'O'),\n",
       "  ('information retrieval techniques', 'H01-1001.5'),\n",
       "  (' use a ', 'O'),\n",
       "  ('histogram', 'H01-1001.6'),\n",
       "  (' of ', 'O'),\n",
       "  ('keywords', 'H01-1001.7'),\n",
       "  (' as the ', 'O'),\n",
       "  ('document representation', 'H01-1001.8'),\n",
       "  (' but ', 'O'),\n",
       "  ('oral communication', 'H01-1001.9'),\n",
       "  (' may offer additional ', 'O'),\n",
       "  ('indices', 'H01-1001.10'),\n",
       "  (' such as the time and place of the rejoinder and the attendance. An alternative ',\n",
       "   'O'),\n",
       "  ('index', 'H01-1001.11'),\n",
       "  (' could be the activity such as discussing, planning, informing, story-telling, etc. This paper addresses the problem of the ',\n",
       "   'O'),\n",
       "  ('automatic detection', 'H01-1001.12'),\n",
       "  (' of those activities in meeting situation and everyday rejoinders. Several extensions of this basic idea are being discussed and/or evaluated: Similar to activities one can define subsets of larger ',\n",
       "   'O'),\n",
       "  ('database', 'H01-1001.13'),\n",
       "  (' and detect those automatically which is shown on a large ', 'O'),\n",
       "  ('database', 'H01-1001.14'),\n",
       "  (' of ', 'O'),\n",
       "  ('TV shows', 'H01-1001.15'),\n",
       "  (' . ', 'O'),\n",
       "  ('Emotions', 'H01-1001.16'),\n",
       "  (' and other ', 'O'),\n",
       "  ('indices', 'H01-1001.17'),\n",
       "  (' such as the ', 'O'),\n",
       "  ('dominance distribution of speakers', 'H01-1001.18'),\n",
       "  (' might be available on the ', 'O'),\n",
       "  ('surface', 'H01-1001.19'),\n",
       "  (' and could be used directly. Despite the small size of the ', 'O'),\n",
       "  ('databases', 'H01-1001.20'),\n",
       "  (' used some results about the effectiveness of these ', 'O'),\n",
       "  ('indices', 'H01-1001.21'),\n",
       "  (' can be obtained. ', 'O')])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spans= sent_detector.span_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_detector._params.abbrev_types.add('i.e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relations = []\n",
    "for article in articles:\n",
    "    text = ''.join([text[0] for text in article[1]])\n",
    "    i = 0\n",
    "    entities = []\n",
    "    for seq in article[1]:\n",
    "        if seq[1] != 'O':\n",
    "            entities.append((i, i+len(seq[0]), seq[1]))\n",
    "        i += len(seq[0])\n",
    "    sent_spans = sent_detector.span_tokenize(text)\n",
    "    article_rels = []\n",
    "    for span in sent_spans:\n",
    "        sent_ents = []\n",
    "        for entity in entities:\n",
    "            span_range = set(range(span[0], span[1]))\n",
    "            entity_range = set(range(entity[0], entity[1]))\n",
    "            if (span_range&entity_range) and not span_range.issuperset(entity_range):\n",
    "                print('entity overlaps sentences')\n",
    "                print(text[span[0]:span[1]])\n",
    "                print(text[entity[0]:entity[1]])\n",
    "                print(entity)\n",
    "                raise KeyError\n",
    "            elif (span_range & entity_range) and span_range.issuperset(entity_range):\n",
    "                sent_ents.append((text[entity[0]:entity[1]], entity[2]))\n",
    "        article_rels.append((text[span[0]:span[1]], sent_ents))\n",
    "    relations.append((article[0], article_rels))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relations[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rels = {}\n",
    "for article in relations:\n",
    "    title, sents = article\n",
    "    for sent in sents:\n",
    "        if len(sent[1]) < 2:\n",
    "            continue\n",
    "        for i in range(len(sent[1])):\n",
    "            for j in range(1, len(sent[1][i+1:])+1):\n",
    "                start = sent[0].find(sent[1][i][0]) + len(sent[1][i][0])\n",
    "                end = sent[0].find(sent[1][i+j][0])\n",
    "                if start > end:\n",
    "                    context = sent[0][end:start - len(sent[1][i][0])]\n",
    "                else:\n",
    "                    context = sent[0][start:end]\n",
    "                rels[(sent[1][i][1], sent[1][i+j][1])] = [sent[0], context, sent[1][i][0], sent[1][i+j][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = ''.join([text[0] for text in articles[0][1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for article in articles:\n",
    "    text = []\n",
    "    for t in article[1]:\n",
    "        if t[1] == 'O':\n",
    "            text.append(t[0])\n",
    "        else:\n",
    "            text.append(t[0].replace(' ', '_') + '_' + t[1])\n",
    "    text = ''.join(text)\n",
    "    sents = sent_detector.tokenize(text)\n",
    "    texts.append((article[0], sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'_precision_P05-1076.20' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-b6904ef9033c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#                     end = sent.find(entities[i+j])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: '_precision_P05-1076.20' is not in list"
     ]
    }
   ],
   "source": [
    "relations = []\n",
    "bad = []\n",
    "for article in texts:\n",
    "    \n",
    "    for sent in article[1]:\n",
    "#         entities = re.findall('[\\w\\.\\-\\(\\)\\[\\]\\'\\/\\,\\=\\#]+(?:_[\\w\\.\\-\\(\\)\\[\\]\\'\\/\\,\\=\\#]+)+', sent)\n",
    "        entities = re.findall('[\\w\\.\\-\\(\\)\\[\\]\\'\\/\\,\\=\\#\\*\"\\+\\&\\%]+(?:_[\\w\\.\\-\\(\\)\\[\\]\\'\\/\\,\\=\\#\\*\"\\+\\&\\%]+)*?(?:_+[A-Z][0-9][0-9]\\-[0-9]{3,5}\\.[0-9]{1,3}(?:[\\w\\,\\;\\:\\(\\)\\\"\\*]*)?)', sent)\n",
    "        rel = []\n",
    "        words = sent.split()\n",
    "        if len(entities) > 1:\n",
    "            for i in range(len(entities)):\n",
    "                for j in range(1, len(entities[i+1:])):\n",
    "#                     start = sent.find(entities[i]) + len(entities[i]) + 1\n",
    "#                     end = sent.find(entities[i+j])\n",
    "                    start = words.index(entities[i]) + 1\n",
    "                    end = words.index(entities[i+j])\n",
    "\n",
    "                    context = words[start:end]\n",
    "                    clear_context = []\n",
    "                    for word in context:\n",
    "                        if '_' in word:\n",
    "                            w = word.split('_')[:-1]\n",
    "                            clear_context += w\n",
    "                        else:\n",
    "                            clear_context += [word]\n",
    "                    rel.append((sent, entities[i], context, entities[i+j]))\n",
    "        \n",
    "        relations += rel\n",
    "#         else:\n",
    "#             bad.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'experiments_P05-1076.17',\n",
       " 'show',\n",
       " 'that',\n",
       " 'the',\n",
       " 'system_P05-1076.18',\n",
       " 'is',\n",
       " 'able',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'scf_types_P05-1076.19',\n",
       " 'with',\n",
       " '70%_precision_P05-1076.20',\n",
       " 'and',\n",
       " '66%_recall_rate_P05-1076.21.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('bad.txt', 'w', encoding='utf8')\n",
    "for rel in bad:\n",
    "#     f.write('#\\t#'.join(rel)+'\\n')\n",
    "    f.write(rel+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_relations = [re.findall('([A-Z]+)\\(([\\w\\.\\-]+)\\,([\\w\\.\\-]+)(?:\\,([\\w\\.\\-]+))?\\)', text) \n",
    "                  for text in open('data/1.1.relations.txt', encoding='utf8').read().splitlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_dict_golden = {}\n",
    "for rel in gold_relations:\n",
    "    rel = rel[0]\n",
    "    rel_dict_golden[(rel[1], rel[2])] = [rel[0], rel[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_rels = {}\n",
    "for rel in rels:\n",
    "    if rel in rel_dict_golden:\n",
    "        train_rels[rel] = rels[rel] + rel_dict_golden[rel]\n",
    "    else:\n",
    "        train_rels[rel] = rels[rel] + ['NO_RELATION', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train_data_view.tsv', 'w', encoding='utf8')\n",
    "f.write('tag\\tentity1\\tentity2\\tcontext\\tsent\\treverse\\n')\n",
    "for rel in train_rels:\n",
    "    s = re.sub('\\n', '', '###'.join([train_rels[rel][4], train_rels[rel][2], train_rels[rel][3], \n",
    "                       train_rels[rel][1], train_rels[rel][0], train_rels[rel][5]]))\n",
    "#     if len(s.split('\\t')) != 6:\n",
    "#         print(rel)\n",
    "#         print(len(s.split('\\t')))\n",
    "#         raise KeyError\n",
    "    f.write(s+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel = ('E06-1004.11', 'E06-1004.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable.',\n",
       " ' greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of ',\n",
       " 'lexicon',\n",
       " 'lexical probabilities',\n",
       " 'NO_RELATION',\n",
       " '']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rels[rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "entitys1 = []\n",
    "entitys2 = []\n",
    "contexts = []\n",
    "sents = []\n",
    "reverse = []\n",
    "for rel in train_rels:\n",
    "    targets.append(train_rels[rel][4])\n",
    "    entitys1.append(train_rels[rel][2])\n",
    "    entitys2.append(train_rels[rel][3])\n",
    "    contexts.append(train_rels[rel][1])\n",
    "    sents.append(train_rels[rel][0])\n",
    "    reverse.append(train_rels[rel][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'target':targets, 'ent1':entitys1, 'ent2':entitys2,\n",
    "                     'context': contexts, 'sent':sents, 'reverse':reverse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>ent1</th>\n",
       "      <th>ent2</th>\n",
       "      <th>reverse</th>\n",
       "      <th>sent</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and average retrieval times for looking up</td>\n",
       "      <td>computational complexity</td>\n",
       "      <td>phrase translations</td>\n",
       "      <td></td>\n",
       "      <td>We detail the computational complexity and ave...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, we have examined a</td>\n",
       "      <td>compositional classes of paraphrases</td>\n",
       "      <td>class-oriented framework</td>\n",
       "      <td></td>\n",
       "      <td>\\nTowards deep analysis of compositional class...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>), and the state of focus of attention (calle...</td>\n",
       "      <td>intentional structure</td>\n",
       "      <td>attentional state</td>\n",
       "      <td></td>\n",
       "      <td>In this theory, discourse structure is compose...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>, building</td>\n",
       "      <td>text</td>\n",
       "      <td>unambiguous structures</td>\n",
       "      <td>REVERSE</td>\n",
       "      <td>Sequences of cascades of rules deterministical...</td>\n",
       "      <td>FEATURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>designed for</td>\n",
       "      <td>features</td>\n",
       "      <td>pronoun resolution</td>\n",
       "      <td></td>\n",
       "      <td>We present a set of features designed for pron...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0        and average retrieval times for looking up    \n",
       "1                              , we have examined a    \n",
       "2   ), and the state of focus of attention (calle...   \n",
       "3                                        , building    \n",
       "4                                      designed for    \n",
       "\n",
       "                                   ent1                      ent2  reverse  \\\n",
       "0              computational complexity       phrase translations            \n",
       "1  compositional classes of paraphrases  class-oriented framework            \n",
       "2                 intentional structure         attentional state            \n",
       "3                                  text    unambiguous structures  REVERSE   \n",
       "4                              features        pronoun resolution            \n",
       "\n",
       "                                                sent       target  \n",
       "0  We detail the computational complexity and ave...  NO_RELATION  \n",
       "1  \\nTowards deep analysis of compositional class...  NO_RELATION  \n",
       "2  In this theory, discourse structure is compose...  NO_RELATION  \n",
       "3  Sequences of cascades of rules deterministical...      FEATURE  \n",
       "4  We present a set of features designed for pron...  NO_RELATION  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('train_data.tsv', sep='\\t', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('train_data.tsv', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>ent1</th>\n",
       "      <th>ent2</th>\n",
       "      <th>reverse</th>\n",
       "      <th>sent</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and average retrieval times for looking up</td>\n",
       "      <td>computational complexity</td>\n",
       "      <td>phrase translations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We detail the computational complexity and ave...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, we have examined a</td>\n",
       "      <td>compositional classes of paraphrases</td>\n",
       "      <td>class-oriented framework</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nTowards deep analysis of compositional cla...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>), and the state of focus of attention (calle...</td>\n",
       "      <td>intentional structure</td>\n",
       "      <td>attentional state</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In this theory, discourse structure is compose...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>, building</td>\n",
       "      <td>text</td>\n",
       "      <td>unambiguous structures</td>\n",
       "      <td>REVERSE</td>\n",
       "      <td>Sequences of cascades of rules deterministical...</td>\n",
       "      <td>FEATURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>designed for</td>\n",
       "      <td>features</td>\n",
       "      <td>pronoun resolution</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present a set of features designed for pron...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>words differs in</td>\n",
       "      <td>IR</td>\n",
       "      <td>words</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Since the significance of words differs in IR,...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of word n-grams (87.2%-96.7% accuracy dependi...</td>\n",
       "      <td>frequency counts</td>\n",
       "      <td>classification method</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nThis paper shows that it is very often pos...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>are presented, along with a control structure...</td>\n",
       "      <td>entity-oriented language definition</td>\n",
       "      <td>entity-oriented parser</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Representative samples from an entity-oriented...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on the</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>Penn Treebank WSJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Using these ideas together, the resulting tagg...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>description that serves a specific purpose in ...</td>\n",
       "      <td>automatically generated text</td>\n",
       "      <td>description</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We discuss the underlying relationship between...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>can attain accuracies with 400 unlabeled exam...</td>\n",
       "      <td>supervised methods</td>\n",
       "      <td>semi-supervised methods</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In both domains, we found that unsupervised me...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in a</td>\n",
       "      <td>parsing-as-deduction</td>\n",
       "      <td>resource sensitive logic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our logical definition leads to a neat relatio...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>features are not enough to distinguish terms f...</td>\n",
       "      <td>non-terms</td>\n",
       "      <td>features</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The algorithms always face the dilemma that fe...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>is an integral and inescapable process in</td>\n",
       "      <td>metaphors</td>\n",
       "      <td>human understanding of natural language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n Interpreting metaphors is an integral an...</td>\n",
       "      <td>WHOLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>, AS-closed, HK-open, HK-closed,</td>\n",
       "      <td>AS-open</td>\n",
       "      <td>MSR-open</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The system participated in all the tracks of t...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>natural languages to their associated</td>\n",
       "      <td>semantics</td>\n",
       "      <td>natural language</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The formalism's intended usage is to relate ex...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>by Konolige, which is a formalization of defe...</td>\n",
       "      <td>argumentation system</td>\n",
       "      <td>defeat rules</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\n This paper proposes that sentence analys...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>by a model that a word consists of a sequence...</td>\n",
       "      <td>Arabic's rich morphology</td>\n",
       "      <td>morphemes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We approximate Arabic's rich morphology by a ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>extracted from corpora: given a set of terms,...</td>\n",
       "      <td>terms</td>\n",
       "      <td>corpus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nTerminology structuring has been the subje...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nInstances of a word drawn from different d...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>) from a list of</td>\n",
       "      <td>word or semantic error rate</td>\n",
       "      <td>word strings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The oracle knows the reference word string and...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>is limited to the</td>\n",
       "      <td>noun</td>\n",
       "      <td>type of unit classifier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Registration of classifier for each noun is li...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>, and (2) correcting</td>\n",
       "      <td>semantic distance</td>\n",
       "      <td>real-word spelling errors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We show that the newly proposed concept-distan...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>, and the output summary is in</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We consider the case of multi-document summari...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>and approximation error introduced by the lan...</td>\n",
       "      <td>insufficient training data</td>\n",
       "      <td>ambiguities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Owing to the problem of insufficient training ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>of the main parser for a language L are direc...</td>\n",
       "      <td>non-deterministic parsing choices</td>\n",
       "      <td>shared derivation forest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The non-deterministic parsing choices of the m...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>directly on word sense disambiguation perform...</td>\n",
       "      <td>Chinese-to-English SMT model</td>\n",
       "      <td>datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present the first known empirical test of ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>can be useful in a number of Natural Language...</td>\n",
       "      <td>Topic signatures</td>\n",
       "      <td>Text Summarisation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Topic signatures can be useful in a number of ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>is then transformed by a planning algorithm i...</td>\n",
       "      <td>logical expression</td>\n",
       "      <td>Prolog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The resulting logical expression is then trans...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>is shown as well as improvement on several</td>\n",
       "      <td>word alignment techniques</td>\n",
       "      <td>machine translation tests</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Significant improvement over traditional word ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8645</th>\n",
       "      <td>incorporate an adapted version of a</td>\n",
       "      <td>text-level anaphora</td>\n",
       "      <td>Grosz-Sidner-style focus model</td>\n",
       "      <td>REVERSE</td>\n",
       "      <td>Criteria for anaphora resolution within senten...</td>\n",
       "      <td>FEATURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8646</th>\n",
       "      <td>judgments on</td>\n",
       "      <td>translation accuracy and quality</td>\n",
       "      <td>judgments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The current methodology optimizes the inherent...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8647</th>\n",
       "      <td>against one of the best existing robust proba...</td>\n",
       "      <td>head-to-head tests</td>\n",
       "      <td>P-CFG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In head-to-head tests against one of the best ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8648</th>\n",
       "      <td>that exemplify these themes: a text-image edi...</td>\n",
       "      <td>PARC</td>\n",
       "      <td>wordspotter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This paper discusses three research initiative...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8649</th>\n",
       "      <td>that is natural in terms of the</td>\n",
       "      <td>language definition</td>\n",
       "      <td>task domain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A construction-specific approach also aids in ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8650</th>\n",
       "      <td>, and it would make the treebank more valuable...</td>\n",
       "      <td>treebank</td>\n",
       "      <td>source of data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We argue that a more sophisticated and fine-gr...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651</th>\n",
       "      <td>outperform traditional distributional word-di...</td>\n",
       "      <td>concept-distance measures</td>\n",
       "      <td>semantic distance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We show that the newly proposed concept-distan...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8652</th>\n",
       "      <td>for orthographic variants caused by</td>\n",
       "      <td>detection method</td>\n",
       "      <td>transliteration</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nWe propose a detection method for orthogra...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>NaN</td>\n",
       "      <td>terms</td>\n",
       "      <td>terms</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r\\nTerminology structuring has been the subje...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8654</th>\n",
       "      <td>sentences , called a Treebank , in combination...</td>\n",
       "      <td>decision tree building</td>\n",
       "      <td>sentence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We use a corpus of bracketed sentences , calle...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8655</th>\n",
       "      <td>, and provide evaluation results involving</td>\n",
       "      <td>character and word error rate</td>\n",
       "      <td>automatic extraction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We present an implementation of the model base...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8656</th>\n",
       "      <td>is proposed which accounts for the effect of ...</td>\n",
       "      <td>evaluation scheme</td>\n",
       "      <td>undisambiguated SCF data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A novel evaluation scheme is proposed which ac...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8657</th>\n",
       "      <td>to users has been extensively studied by the ...</td>\n",
       "      <td>system response</td>\n",
       "      <td>dialog systems</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The issue of system response to users has been...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658</th>\n",
       "      <td>generated with the</td>\n",
       "      <td>word graph</td>\n",
       "      <td>bigram</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A second forward pass, which makes use of a wo...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>is provided with a probabilistic context-free...</td>\n",
       "      <td>MORphological PArser MORPA</td>\n",
       "      <td>ungrammatical segmentations</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In order to deal with ambiguity, the MORpholog...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8660</th>\n",
       "      <td>of machine translation and document summariza...</td>\n",
       "      <td>automatic evaluation</td>\n",
       "      <td>automatically evaluating answers to definition...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Following recent developments in the automati...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8661</th>\n",
       "      <td>for several problematic examples of</td>\n",
       "      <td>predictions</td>\n",
       "      <td>ellipsis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The correct predictions for several problemati...</td>\n",
       "      <td>FEATURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8662</th>\n",
       "      <td>and generation, an information-state model of...</td>\n",
       "      <td>understanding</td>\n",
       "      <td>collaborative problem solving</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our contributions include a concise, modular a...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>or a stem can convert the word from a nominal...</td>\n",
       "      <td>word</td>\n",
       "      <td>verbal structure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Morphemes added to a root word or a stem can c...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8664</th>\n",
       "      <td>we propose and use a technique exploiting the...</td>\n",
       "      <td>MWEs</td>\n",
       "      <td>salience</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To obtain a more complete list of MWEs we prop...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665</th>\n",
       "      <td>can be constructed in a</td>\n",
       "      <td>Node-based inference rules</td>\n",
       "      <td>semantic network</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Node-based inference rules can be constructed ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8666</th>\n",
       "      <td>has constructed several TIPSTER applications ...</td>\n",
       "      <td>Computing Research Laboratory (CRL)</td>\n",
       "      <td>Graphical User Interface (GUI) functions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>However, the Computing Research Laboratory (CR...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8667</th>\n",
       "      <td>, and run each over both character- and word-...</td>\n",
       "      <td>bag-of-words and segment order-sensitive strin...</td>\n",
       "      <td>local segment contiguity models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We take a selection of both bag-of-words and s...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8668</th>\n",
       "      <td>, recognizing the intentions expressed in the...</td>\n",
       "      <td>segments</td>\n",
       "      <td>attentional state</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discourse processing requires recognizing how ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8669</th>\n",
       "      <td>models, which either use arbitrary windows to ...</td>\n",
       "      <td>lexical affinity</td>\n",
       "      <td>models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In comparison with previous models, which eith...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>initially designed for</td>\n",
       "      <td>extracting subcategorization frames</td>\n",
       "      <td>written texts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our experiments also show that current technol...</td>\n",
       "      <td>USAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>NaN</td>\n",
       "      <td>LMs</td>\n",
       "      <td>LM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The method amounts to tagging LMs with confide...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>which can sometimes alleviate the unpleasant ...</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>intractability</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We show that the crucial operation of consiste...</td>\n",
       "      <td>USAGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>supports the adaptation of the generic core t...</td>\n",
       "      <td>PAKTUS</td>\n",
       "      <td>discourse patterns</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PAKTUS supports the adaptation of the generic ...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>greatly impacts the accuracy that can be achi...</td>\n",
       "      <td>lexicon</td>\n",
       "      <td>lexical probabilities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Observing that the quality of the lexicon grea...</td>\n",
       "      <td>NO_RELATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                context  \\\n",
       "0           and average retrieval times for looking up    \n",
       "1                                 , we have examined a    \n",
       "2      ), and the state of focus of attention (calle...   \n",
       "3                                           , building    \n",
       "4                                         designed for    \n",
       "5                                     words differs in    \n",
       "6      of word n-grams (87.2%-96.7% accuracy dependi...   \n",
       "7      are presented, along with a control structure...   \n",
       "8                                               on the    \n",
       "9     description that serves a specific purpose in ...   \n",
       "10     can attain accuracies with 400 unlabeled exam...   \n",
       "11                                                in a    \n",
       "12    features are not enough to distinguish terms f...   \n",
       "13           is an integral and inescapable process in    \n",
       "14                    , AS-closed, HK-open, HK-closed,    \n",
       "15               natural languages to their associated    \n",
       "16     by Konolige, which is a formalization of defe...   \n",
       "17     by a model that a word consists of a sequence...   \n",
       "18     extracted from corpora: given a set of terms,...   \n",
       "19                                                  NaN   \n",
       "20                                    ) from a list of    \n",
       "21                                   is limited to the    \n",
       "22                                , and (2) correcting    \n",
       "23                      , and the output summary is in    \n",
       "24     and approximation error introduced by the lan...   \n",
       "25     of the main parser for a language L are direc...   \n",
       "26     directly on word sense disambiguation perform...   \n",
       "27     can be useful in a number of Natural Language...   \n",
       "28     is then transformed by a planning algorithm i...   \n",
       "29          is shown as well as improvement on several    \n",
       "...                                                 ...   \n",
       "8645               incorporate an adapted version of a    \n",
       "8646                                      judgments on    \n",
       "8647   against one of the best existing robust proba...   \n",
       "8648   that exemplify these themes: a text-image edi...   \n",
       "8649                   that is natural in terms of the    \n",
       "8650  , and it would make the treebank more valuable...   \n",
       "8651   outperform traditional distributional word-di...   \n",
       "8652               for orthographic variants caused by    \n",
       "8653                                                NaN   \n",
       "8654  sentences , called a Treebank , in combination...   \n",
       "8655        , and provide evaluation results involving    \n",
       "8656   is proposed which accounts for the effect of ...   \n",
       "8657   to users has been extensively studied by the ...   \n",
       "8658                                generated with the    \n",
       "8659   is provided with a probabilistic context-free...   \n",
       "8660   of machine translation and document summariza...   \n",
       "8661               for several problematic examples of    \n",
       "8662   and generation, an information-state model of...   \n",
       "8663   or a stem can convert the word from a nominal...   \n",
       "8664   we propose and use a technique exploiting the...   \n",
       "8665                           can be constructed in a    \n",
       "8666   has constructed several TIPSTER applications ...   \n",
       "8667   , and run each over both character- and word-...   \n",
       "8668   , recognizing the intentions expressed in the...   \n",
       "8669  models, which either use arbitrary windows to ...   \n",
       "8670                            initially designed for    \n",
       "8671                                                NaN   \n",
       "8672   which can sometimes alleviate the unpleasant ...   \n",
       "8673   supports the adaptation of the generic core t...   \n",
       "8674   greatly impacts the accuracy that can be achi...   \n",
       "\n",
       "                                                   ent1  \\\n",
       "0                              computational complexity   \n",
       "1                  compositional classes of paraphrases   \n",
       "2                                 intentional structure   \n",
       "3                                                  text   \n",
       "4                                              features   \n",
       "5                                                    IR   \n",
       "6                                      frequency counts   \n",
       "7                   entity-oriented language definition   \n",
       "8                                              accuracy   \n",
       "9                          automatically generated text   \n",
       "10                                   supervised methods   \n",
       "11                                 parsing-as-deduction   \n",
       "12                                            non-terms   \n",
       "13                                            metaphors   \n",
       "14                                              AS-open   \n",
       "15                                            semantics   \n",
       "16                                 argumentation system   \n",
       "17                             Arabic's rich morphology   \n",
       "18                                                terms   \n",
       "19                                                 word   \n",
       "20                          word or semantic error rate   \n",
       "21                                                 noun   \n",
       "22                                    semantic distance   \n",
       "23                                               Arabic   \n",
       "24                           insufficient training data   \n",
       "25                    non-deterministic parsing choices   \n",
       "26                         Chinese-to-English SMT model   \n",
       "27                                     Topic signatures   \n",
       "28                                   logical expression   \n",
       "29                            word alignment techniques   \n",
       "...                                                 ...   \n",
       "8645                                text-level anaphora   \n",
       "8646                   translation accuracy and quality   \n",
       "8647                                 head-to-head tests   \n",
       "8648                                               PARC   \n",
       "8649                                language definition   \n",
       "8650                                           treebank   \n",
       "8651                          concept-distance measures   \n",
       "8652                                   detection method   \n",
       "8653                                              terms   \n",
       "8654                             decision tree building   \n",
       "8655                      character and word error rate   \n",
       "8656                                  evaluation scheme   \n",
       "8657                                    system response   \n",
       "8658                                         word graph   \n",
       "8659                         MORphological PArser MORPA   \n",
       "8660                               automatic evaluation   \n",
       "8661                                        predictions   \n",
       "8662                                      understanding   \n",
       "8663                                               word   \n",
       "8664                                               MWEs   \n",
       "8665                         Node-based inference rules   \n",
       "8666                Computing Research Laboratory (CRL)   \n",
       "8667  bag-of-words and segment order-sensitive strin...   \n",
       "8668                                           segments   \n",
       "8669                                   lexical affinity   \n",
       "8670                extracting subcategorization frames   \n",
       "8671                                                LMs   \n",
       "8672                                         algorithms   \n",
       "8673                                             PAKTUS   \n",
       "8674                                            lexicon   \n",
       "\n",
       "                                                   ent2  reverse  \\\n",
       "0                                   phrase translations      NaN   \n",
       "1                              class-oriented framework      NaN   \n",
       "2                                     attentional state      NaN   \n",
       "3                                unambiguous structures  REVERSE   \n",
       "4                                    pronoun resolution      NaN   \n",
       "5                                                 words      NaN   \n",
       "6                                 classification method      NaN   \n",
       "7                                entity-oriented parser      NaN   \n",
       "8                                     Penn Treebank WSJ      NaN   \n",
       "9                                           description      NaN   \n",
       "10                              semi-supervised methods      NaN   \n",
       "11                             resource sensitive logic      NaN   \n",
       "12                                             features      NaN   \n",
       "13              human understanding of natural language      NaN   \n",
       "14                                             MSR-open      NaN   \n",
       "15                                     natural language      NaN   \n",
       "16                                         defeat rules      NaN   \n",
       "17                                            morphemes      NaN   \n",
       "18                                               corpus      NaN   \n",
       "19                                                 word      NaN   \n",
       "20                                         word strings      NaN   \n",
       "21                              type of unit classifier      NaN   \n",
       "22                            real-word spelling errors      NaN   \n",
       "23                                              English      NaN   \n",
       "24                                          ambiguities      NaN   \n",
       "25                             shared derivation forest      NaN   \n",
       "26                                             datasets      NaN   \n",
       "27                                   Text Summarisation      NaN   \n",
       "28                                               Prolog      NaN   \n",
       "29                            machine translation tests      NaN   \n",
       "...                                                 ...      ...   \n",
       "8645                     Grosz-Sidner-style focus model  REVERSE   \n",
       "8646                                          judgments      NaN   \n",
       "8647                                              P-CFG      NaN   \n",
       "8648                                        wordspotter      NaN   \n",
       "8649                                        task domain      NaN   \n",
       "8650                                     source of data      NaN   \n",
       "8651                                  semantic distance      NaN   \n",
       "8652                                    transliteration      NaN   \n",
       "8653                                              terms      NaN   \n",
       "8654                                           sentence      NaN   \n",
       "8655                               automatic extraction      NaN   \n",
       "8656                           undisambiguated SCF data      NaN   \n",
       "8657                                     dialog systems      NaN   \n",
       "8658                                             bigram      NaN   \n",
       "8659                        ungrammatical segmentations      NaN   \n",
       "8660  automatically evaluating answers to definition...      NaN   \n",
       "8661                                           ellipsis      NaN   \n",
       "8662                      collaborative problem solving      NaN   \n",
       "8663                                   verbal structure      NaN   \n",
       "8664                                           salience      NaN   \n",
       "8665                                   semantic network      NaN   \n",
       "8666           Graphical User Interface (GUI) functions      NaN   \n",
       "8667                    local segment contiguity models      NaN   \n",
       "8668                                  attentional state      NaN   \n",
       "8669                                             models      NaN   \n",
       "8670                                      written texts      NaN   \n",
       "8671                                                 LM      NaN   \n",
       "8672                                     intractability      NaN   \n",
       "8673                                 discourse patterns      NaN   \n",
       "8674                              lexical probabilities      NaN   \n",
       "\n",
       "                                                   sent       target  \n",
       "0     We detail the computational complexity and ave...  NO_RELATION  \n",
       "1     \\r\\nTowards deep analysis of compositional cla...  NO_RELATION  \n",
       "2     In this theory, discourse structure is compose...  NO_RELATION  \n",
       "3     Sequences of cascades of rules deterministical...      FEATURE  \n",
       "4     We present a set of features designed for pron...  NO_RELATION  \n",
       "5     Since the significance of words differs in IR,...  NO_RELATION  \n",
       "6     \\r\\nThis paper shows that it is very often pos...  NO_RELATION  \n",
       "7     Representative samples from an entity-oriented...  NO_RELATION  \n",
       "8     Using these ideas together, the resulting tagg...  NO_RELATION  \n",
       "9     We discuss the underlying relationship between...  NO_RELATION  \n",
       "10    In both domains, we found that unsupervised me...  NO_RELATION  \n",
       "11    Our logical definition leads to a neat relatio...  NO_RELATION  \n",
       "12    The algorithms always face the dilemma that fe...  NO_RELATION  \n",
       "13     \\r\\n Interpreting metaphors is an integral an...        WHOLE  \n",
       "14    The system participated in all the tracks of t...  NO_RELATION  \n",
       "15    The formalism's intended usage is to relate ex...  NO_RELATION  \n",
       "16     \\r\\n This paper proposes that sentence analys...  NO_RELATION  \n",
       "17     We approximate Arabic's rich morphology by a ...  NO_RELATION  \n",
       "18    \\r\\nTerminology structuring has been the subje...  NO_RELATION  \n",
       "19    \\r\\nInstances of a word drawn from different d...  NO_RELATION  \n",
       "20    The oracle knows the reference word string and...  NO_RELATION  \n",
       "21    Registration of classifier for each noun is li...  NO_RELATION  \n",
       "22    We show that the newly proposed concept-distan...  NO_RELATION  \n",
       "23    We consider the case of multi-document summari...  NO_RELATION  \n",
       "24    Owing to the problem of insufficient training ...  NO_RELATION  \n",
       "25    The non-deterministic parsing choices of the m...  NO_RELATION  \n",
       "26     We present the first known empirical test of ...  NO_RELATION  \n",
       "27    Topic signatures can be useful in a number of ...  NO_RELATION  \n",
       "28    The resulting logical expression is then trans...  NO_RELATION  \n",
       "29    Significant improvement over traditional word ...  NO_RELATION  \n",
       "...                                                 ...          ...  \n",
       "8645  Criteria for anaphora resolution within senten...      FEATURE  \n",
       "8646  The current methodology optimizes the inherent...  NO_RELATION  \n",
       "8647  In head-to-head tests against one of the best ...  NO_RELATION  \n",
       "8648  This paper discusses three research initiative...  NO_RELATION  \n",
       "8649  A construction-specific approach also aids in ...  NO_RELATION  \n",
       "8650  We argue that a more sophisticated and fine-gr...  NO_RELATION  \n",
       "8651  We show that the newly proposed concept-distan...  NO_RELATION  \n",
       "8652  \\r\\nWe propose a detection method for orthogra...  NO_RELATION  \n",
       "8653  \\r\\nTerminology structuring has been the subje...  NO_RELATION  \n",
       "8654  We use a corpus of bracketed sentences , calle...  NO_RELATION  \n",
       "8655  We present an implementation of the model base...  NO_RELATION  \n",
       "8656  A novel evaluation scheme is proposed which ac...  NO_RELATION  \n",
       "8657  The issue of system response to users has been...  NO_RELATION  \n",
       "8658  A second forward pass, which makes use of a wo...  NO_RELATION  \n",
       "8659  In order to deal with ambiguity, the MORpholog...  NO_RELATION  \n",
       "8660   Following recent developments in the automati...  NO_RELATION  \n",
       "8661  The correct predictions for several problemati...      FEATURE  \n",
       "8662  Our contributions include a concise, modular a...  NO_RELATION  \n",
       "8663  Morphemes added to a root word or a stem can c...  NO_RELATION  \n",
       "8664  To obtain a more complete list of MWEs we prop...  NO_RELATION  \n",
       "8665  Node-based inference rules can be constructed ...  NO_RELATION  \n",
       "8666  However, the Computing Research Laboratory (CR...  NO_RELATION  \n",
       "8667  We take a selection of both bag-of-words and s...  NO_RELATION  \n",
       "8668  Discourse processing requires recognizing how ...  NO_RELATION  \n",
       "8669  In comparison with previous models, which eith...  NO_RELATION  \n",
       "8670  Our experiments also show that current technol...        USAGE  \n",
       "8671  The method amounts to tagging LMs with confide...  NO_RELATION  \n",
       "8672  We show that the crucial operation of consiste...        USAGE  \n",
       "8673  PAKTUS supports the adaptation of the generic ...  NO_RELATION  \n",
       "8674  Observing that the quality of the lexicon grea...  NO_RELATION  \n",
       "\n",
       "[8675 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NO_RELATION    7450\n",
       "USAGE           482\n",
       "FEATURE         326\n",
       "WHOLE           232\n",
       "COMPARE          95\n",
       "RESULT           72\n",
       "TOPIC            18\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e4edb54f10f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'COMPARE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d['sent'][d['target'] == 'COMPARE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Неразмеченные тексты\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import flashtext\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "punkt_param = PunktParameters()\n",
    "abbreviation = ['al', 'fig', 'w.r.t', 'eq', 'e.g', 'i.e']\n",
    "punkt_param.abbrev_types = set(abbreviation)\n",
    "sent_tokenize = PunktSentenceTokenizer(punkt_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['unlabeled/'+file for file in os.listdir('unlabeled')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(files[1], encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [re.sub('\\n', '', sent) for sent in sent_tokenize.tokenize(text) if len(sent) > 40 and len(re.findall('\\n', sent)) < 4 and not sent.startswith('[')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sent_tokenize.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Less is More: Micro-expression Recognition from Video using Apex FrameSze-Teng Lionga, John Seeb, KokSheik Wongc,∗, Raphael C.-W.',\n",
       " 'Conventional feature extraction approaches for micro-expression video considereither the whole video sequence or a part of it, for representation.',\n",
       " 'However, with the high-speed video capture of micro-expressions (100-200 fps), are all frames necessary to provide a suﬃciently meaningful representation?',\n",
       " 'Is the luxury ofdata a bane to accurate recognition?',\n",
       " 'A novel proposition is presented in this paper, whereby we utilize only two imagesper video, namely, the apex frame and the onset frame.',\n",
       " 'The apex frame of a video contains the highest intensity ofexpression changes among all frames, while the onset is the perfect choice of a reference frame with neutral expression.',\n",
       " 'A new feature extractor, Bi-Weighted Oriented Optical Flow (Bi-WOOF) is proposed to encode essential expressivenessof the apex frame.',\n",
       " 'We evaluated the proposed method on ﬁve micro-expression databases— CAS(ME)2, CASME II,SMIC-HS, SMIC-NIR and SMIC-VIS.',\n",
       " 'Our experiments lend credence to our hypothesis, with our proposed techniqueachieving a state-of-the-art F1-score recognition performance of 0.61 and 0.62 in the high frame rate CASME II andSMIC-HS databases respectively.',\n",
       " 'Keywords: Micro-expressions, emotion, apex, optical ﬂow, optical strain, recognition1.',\n",
       " 'IntroductionHave you ever thought that someone was lying to you,but have no evidence to prove it?',\n",
       " 'Or have you alwaysfound it diﬃcult to interpret one’s emotion?',\n",
       " 'Recognizingmicro-expressions could help to solve these doubts.',\n",
       " 'Micro-expression is a very brief and rapid facial emo-tion that is provoked involuntarily [1], revealing a person’strue feelings.',\n",
       " 'Akin to normal facial expression, also knownas macro-expression, it can be categorized into six basicemotions: happy, fear, sad, surprise, anger and disgust.',\n",
       " 'However, macro-expressions are easily identiﬁed in real-time situations with the naked eye as it occurs between2–3 seconds and can be found over the entire face region.',\n",
       " 'On the other hand, a micro-expression is both micro (shortduration) and subtle (small intensity) [2] in nature.',\n",
       " 'It lastsbetween 1/5 to 1/25 of a second and usually occurs in onlya few parts of the face.',\n",
       " 'These are the main reasons whypeople are sometimes unable to realize or recognize thegenuine emotion shown on a person’s face [3, 4].',\n",
       " 'Hence, theability to recognize micro-expressions is beneﬁcial in bothour mundane lives and also society at large.',\n",
       " 'Also, analyzing a person’s emotions can help facilitateunderstanding of our social relationships, while we are in-creasingly awareness of the emotional states of our ownselfs and of the people around us.',\n",
       " 'More essentially, recog-nizing these micro-expressions is useful in a wide range ofapplications, including psychological and clinical diagno-sis, police interrogation and national security [5–7].',\n",
       " 'Micro-expression was ﬁrst discovered by psychologists,Ekman and Friesen [1] in 1969, from a case where a pa-tient was trying to conceal his sad feeling by covering upwith smile.',\n",
       " 'They detected the patient’s genuine feelingby carefully observing the subtle movements on his face,and found out that the patient was actually planning tocommit suicide.',\n",
       " 'Later on, they established Facial ActionCoding System (FACS) [8] to determine the relationshipbetween facial muscle changes and emotional states.',\n",
       " 'Thissystem can be used to identify the exact time each actionunit (AU) begins and ends.',\n",
       " 'The occurrence of the ﬁrstvisible AU is called the onset, while that of the disappear-ance of the AU is the oﬀset.',\n",
       " 'Apex is the point when theAU reaches the peak or the highest intensity of the facialmotion.',\n",
       " 'The timings of the onset, oﬀset and apex for theAUs may diﬀer for the same emotion type.',\n",
       " 'Figure 1 showsa sample sequence containing frames of a surprise expres-sion from a micro-expression database, with the indicationof onset, apex and oﬀset frames.',\n",
       " 'Currently, there are lessthan ﬁfty micro-expressions related research papers pub-lished since 2009.',\n",
       " 'While databases for normal facial ex-pressions are widely available [10], facial micro-expressiondata, particularly those of spontaneous nature, is some-what limited for a number of reasons.',\n",
       " 'Firstly, the elicita-tion process demands for good choice of emotional stimulithat has high ecological validity.',\n",
       " 'Post-capture, the labelingof these micro-expression samples require the veriﬁcationof psychologists or trained experts.',\n",
       " 'Thus, the lack of spontaneousmicro-expression databases had hindered the progress ofmicro-expression research.',\n",
       " 'There are two primary tasks in an automated micro-expression system, i.e., spotting and recognition.',\n",
       " 'Meanwhile, the latter classiﬁes the expression typegiven the “spotted” micro-expression video sequence.',\n",
       " 'Figure 2 illustrates the optical ﬂow magnitudeand optical strain magnitude computed between the onset(assumed as neutral expression) and subsequent frames.',\n",
       " 'It is observed that the apex frames (middle and bottomrows in Figure 2) are the frames with the highest motionchanges (bright region) among the video sequence.',\n",
       " 'Micro-expression databases are pre-processed before re-leasing to the public.',\n",
       " 'This process includes face registra-tion, face alignment and ground-truth labeling (i.e., AU,emotion type, frame indices of onset, apex and oﬀset).',\n",
       " 'In the two most popular spontaneous micro-expressiondatabases, namely the CASME II [9] and SMIC [14], theﬁrst two processes (face registration and alignment) wereachieved automatically.',\n",
       " 'However, the lastprocess, i.e., ground-truth labeling, is not automatic andrequires the help of psychologists or trained experts.',\n",
       " 'Inother words, the annotated ground-truth labels may varydepending on the coders.',\n",
       " 'As such, the reliability and con-sistency of the markings are less than ideal, which mayaﬀect the recognition accuracy of the system.',\n",
       " 'Subsequently, a number of LBPvariants [21–23] were proposed to improve on the usageof LBP-TOP.',\n",
       " 'There were other methods proposed that deriveduseful features directly from color spaces [27] and opticalﬂow orientations [28].',\n",
       " 'By raw,we refer to video clips in its original form, without anypre-processing.',\n",
       " 'In [36], the authors searched for the frameindices that contain micro-expressions.',\n",
       " 'They utilized Chi-Squared dissimilarity to calculate the distribution diﬀer-ence between the Local Binary Pattern (LBP) histogram ofthe current feature frame and the averaged feature frame.',\n",
       " 'The frames which yield score greater than a predeterminedthreshold were regarded as frames with micro-expression.',\n",
       " 'A similar approach was carried out by [37], except that:(1) a denoising method was added before extracting thefeatures, and; (2) the Histogram of Gradient was usedinstead of LBP.',\n",
       " 'However, the database they tested onwas not publicly available.',\n",
       " 'Since the benchmark videosequences used in this paper [37] and that in [36] are dif-ferent, their performances cannot be compared directly.',\n",
       " 'Both papers claimed that the eye blinking movement isone type of the micro-expression.',\n",
       " 'However, it was not de-tailed in the ground-truth and hence the frames containingeye blinking movements were annotated manually.',\n",
       " 'A re-cent work by Wang et al. [38] proposed main directionalmaximal diﬀerence analysis for spotting facial movementsfrom long-term videos.',\n",
       " 'In the recognition task, they em-ployed motion magniﬁcation technique and proposed anew feature extractor - the Histograms of Image Gradi-ent Orientation.',\n",
       " 'However, the recognition performancewas poor compared to the state-of-the-art.',\n",
       " 'Besides, theframe rate of the database is 25 fps, which means that themaximum frame number in a raw video sequence is only1/5 s × 25 fps = 5.',\n",
       " 'Yan et al. [40] publishedthe ﬁrst work in spotting the apex frame.',\n",
       " 'They employedtwo feature extractors (i.e., LBP and Constraint LocalModels) and reported the average frame distance betweenthe spotted apex and the ground-truth apex.',\n",
       " 'The framethat has the highest feature diﬀerence between the ﬁrstframe and the subsequent frames is deﬁned to be the apex.',\n",
       " 'long video refers to the rawvideo sequence which may include the frames with micro-expressions as well as irrelevant motion that are presentbefore the onset and after the oﬀset.',\n",
       " 'On the other hand,short video is a sub-sequence of the long video starting fromthe onset and ending with the oﬀset.',\n",
       " 'In other words, allframes before the onset frame and after the oﬀset frameare excluded.',\n",
       " 'How meaningful is the so-called apexframe?',\n",
       " 'Ekman [44] asserted that a “snapshot taken at anpoint when the expression is at its apex can easily conveythe emotion message”.',\n",
       " 'A similar observation by Espos-ito [45] earmarked the apex as “the instant at which theindicators of emotion are most marked”.',\n",
       " 'Hence we can hy-pothesize that the apex frame oﬀers the strongest signalthat depicts the “momentary conﬁguration” [44] of facialcontraction.',\n",
       " 'In this paper, we propose a novel approach to micro-expression recognition, where for each video sequence, weencode features from the representative apex frame withthe onset frame as the reference frame.',\n",
       " 'To solve the lack of apex in-formation in SMIC, a binary search strategy was employedto spot the apex frame [41].',\n",
       " 'We renamed binary search todivide-and-conquer for a more general terminology to thisscheme.',\n",
       " 'The histogram of opticalﬂow orientations is weighted twice at diﬀerent representa-tion scales, namely, bins by the magnitudes of optical ﬂow,and block regions by the magnitudes of optical strain.',\n",
       " 'Weestablish our proposition by proving empirically througha comprehensive evaluation that was carried out on fournotable databases.',\n",
       " 'The rest of this paper is organized as follows.',\n",
       " 'Section 3explains the proposed algorithm in detail.',\n",
       " 'The descrip-tions of the databases used are discussed in Section 4,followed by Section 5 that reports the experiment resultsand discussion for the recognition of micro-expressions.',\n",
       " 'Fi-nally, conclusion is drawn in Section 6.',\n",
       " 'The architectureoverview of the system is illustrated in Figure 3.',\n",
       " 'The fol-lowing subsections detail the steps involved.',\n",
       " 'Speciﬁcally, the procedures of divide-and-conquermethodology are: (A) The frame index of the peaks/ lo-cal maximum in the video sequence are detected by usinga peak detector.',\n",
       " '(B) The frame sequence is divided intotwo equal halves (e.g., a 40 frames video sequence is splitinto two sub-sequences containing frame 1-20 and 21-40).',\n",
       " '(C) Magnitudes of the detected peaks are summed up foreach of the sub-sequence.',\n",
       " '(D) The sub-sequence with thehigher magnitude will be considered for the next compu-tation step while the other sub-sequence will be discarded.',\n",
       " '(E) Steps (B) to (D) are repeated until the ﬁnal peak (alsoknown as apex frame) is found.',\n",
       " 'Liong et al. [41] reportedthat the average estimated apex frame is 13 frames awayfrom the ground-truths apex frames for divide-and-conquermethodology.',\n",
       " 'Note that the micro-expression video has anaverage length of 68 frames.',\n",
       " 'Figure 4 illustrates the apexframe spotting approach in a sample video.',\n",
       " 'It can be seenthat, the ground-truth apex (frame #63) and the spottedapex (frame #64) diﬀer only by one frame.',\n",
       " 'It encodes the motion of an object in vec-tor notation, which indicates the direction and intensity ofthe ﬂow of each image pixel.',\n",
       " '(1)where (dx, dy) indicate the changes along the horizontaland vertical dimensions, and dt is the change in time.',\n",
       " 'We ﬁrst introduce and describe the notations which areused in the subsequent sections.',\n",
       " 'Foreach video sequence, there is only one apex frame, fi,a ∈fi,1, .',\n",
       " ', fi,F i, and it can be located at any frame index.',\n",
       " 'The optical ﬂow vectors of the onset (assumed as neu-tral expression) and the apex frames are predicted thendenoted by fi,1 and fi,a , respectively.',\n",
       " 'n. Here, (ux,y, vx,y) are the displacementvectors in the horizontal and vertical directions respec-tively.',\n",
       " 'The next step is to compute the optical strain, ε, basedon the optical ﬂow vectors.',\n",
       " 'For a suﬃciently small facialpixel’s movement, it is able to approximate the deforma-tion intensity, also known as the inﬁnitesimal strain tensor.',\n",
       " 'In brief, the inﬁnitesimal strain tensor is derived from theLagrangian and Eulerian strain tensor after performing ageometric linearisation [49] .',\n",
       " 'The three characteristic images are partitioned equallyinto N × N non-overlapping blocks.',\n",
       " 'For each block, theorientations θx,y∈[−π, π] are binned and locally weightedaccording to its magnitude ρx,y.',\n",
       " ', C}, and C denotes the total num-ber of histogram bins.',\n",
       " ', N , X × Y is the dimensions(viz., width-by-height) of the video frame.',\n",
       " 'Lastly, the coeﬃcients of ζb1,b2 are multiplied with thelocally weighted histogram bins to their correspondingblocks.',\n",
       " 'The histogram bins of each block are concatenatedto form the resultant feature histogram.',\n",
       " 'In contrast to the conventional Histogram of OrientedOptical Flow (HOOF) [50], our proposed orientation his-togram bins have equal votes.',\n",
       " 'Here, we consider boththe magnitude and optical strain values as the weightingschemes to highlight the importance of each optical ﬂow.',\n",
       " 'Hence, a larger intensity of the pixel’s movement or defor-mation contributes more eﬀect to the histogram, whereasnoisy optical ﬂows with small intensities reduce the signif-icance of the features.',\n",
       " 'In each block,the values of ρ for each pixel are treated as local weights to multiply with their respective θ histogram bins; (b) It forms a locally weightedHOOF with feature size of N × N × C; (c) ζb1,b2 denotes the global weighting matrix, which is derived from ε image; (d) Finally, ζb1,b2 aremultiplied with their corresponding locally weighted HOOF.',\n",
       " 'CASME II [9], SMIC-HS [14], SMIC-VIS [14] and SMIC-NIR [14].',\n",
       " 'Note that all these databases are recorded ina constrained laboratory condition due to the subtlety ofmicro-expressions.',\n",
       " 'Eachvideo clip contains only one micro-expression.',\n",
       " 'Thus, thereis a total of 246 video sequences.',\n",
       " 'The emotion labels weremarked by two coders with the reliability of 0.85.',\n",
       " 'The ex-pressions were elicited from 26 subjects with the mean ageof 22 years old, and recorded using the camera - Point GreyGRAS-03K2C.',\n",
       " 'The video resolution and frame rate of thecamera are 640× 480 pixels and 200 fps respectively.',\n",
       " 'Thisdatabase provides the cropped video sequences, where onlythe face region is shown while the unnecessary backgroundhas been eliminated.',\n",
       " 'The cropped images have an aver-age spatial resolution of 170 × 140 pixels, and each videoconsists of 68 frames (viz., 0.34s).',\n",
       " 'The video with thehighest and lowest number of frames are 141 (viz., 0.71s)and 24 (viz., 0.12s), respectively.',\n",
       " 'The frame index (i.e.,frame number) for onset, apex and oﬀset of each video se-quence are provided.',\n",
       " 'To perform the recognition task onthis micro-expression dataset, the block-based LBP-TOPfeature was considered.',\n",
       " 'The features were then classiﬁedby a Support Vector Machine (SVM) with leave-one-video-out cross-validation (LOVOCV) protocol.',\n",
       " 'SMICSMIC includes three sub-datasets, which are SMIC-HS,SMIC-VIS and SMIC-NIR.',\n",
       " 'The data composition of thesedatasets are detailed in Table 1 .',\n",
       " 'It is noteworthy thatall eight participants who appeared in the VIS and NIRdatasets were also involved in HS dataset elicitation.',\n",
       " 'Dur-ing the recording process, three cameras (i.e., HS, VIS andNIR) were recording simultaneously.',\n",
       " 'The cameras wereplaced parallel to each other at the middle-top of the mon-itor.',\n",
       " 'The groundtruth of the frame indices of onset andoﬀset for each video clip in SMIC are given, but not theapex frame.',\n",
       " 'CAS(ME)2CAS(ME)2 dataset has two major parts (A and B).',\n",
       " 'Part A consists of 87 long videos, containing both spon-taneous macro-expressions and micro-expressions.',\n",
       " 'To evaluate the proposed method, we only con-sider the cropped micro-expression videos (i.e., 57 sam-ples in total).',\n",
       " 'However, we discovered three samples aremissing from the dataset provided.',\n",
       " 'Hence, 54 micro-expression video clips are used in the experiment.',\n",
       " 'Themicro-expression video sequences are elicited from 14 par-ticipants.',\n",
       " 'This dataset provides the cropped face videosequence.',\n",
       " 'The videos are recorded using Logitech ProC920 camera with a temporal resolution of 30 fps andspatial resolution of 640 × 480 pixels.',\n",
       " 'It composes of fourclasses of expressions: negative (21 samples), others (19samples), surprise (8 samples) and positive (6 samples).',\n",
       " 'We resized the images to 170 × 140 pixels for experimentpurpose.',\n",
       " 'The average number of frames of the micro-expression video sequences is 6 frames (viz., 0.2s).',\n",
       " 'Thevideo with the highest and lowest number of frames are 10(viz., 0.33s) and 4 (viz., 0.13s), respectively.',\n",
       " 'The ground-truth frame indices for onset, apex and oﬀset of each videosequence are also provided.',\n",
       " 'To annotate the emotion labelfor each video sequence, a combination of AUs, emotiontypes of expression-elicitation video and self-reported areconsidered.',\n",
       " 'The highest accuracy for the four-class recog-nition task reported in the original paper [16] is 40.95%.',\n",
       " 'Itis obtained by adopting LBP-TOP feature extractor andSVM-LOSOCV classiﬁer.',\n",
       " 'Therefore, it is necessary to measure therecognition performance of the proposed method usingF-measure, which was also suggested in [51].',\n",
       " 'On the other hand, to avoid person dependent issue inthe classiﬁcation process, we employed LOSOCV strategyin the linear SVM classiﬁer setting.',\n",
       " 'In LOSOCV, the fea-tures of the sample videos in one subject are treated asthe testing data and the remaining features from rest ofthe subjects become the training data.',\n",
       " 'Then, this processis repeated for k times, where k is the number of subjectsin the database.',\n",
       " 'Finally, the recognition results for all thesubjects are averaged to compute the recognition rate.',\n",
       " 'Since CAS(ME)2 was onlymade public recently, there is still no method designedand tested on this dataset in the literature.',\n",
       " 'Hence, wereport the recognition results for various block sizes us-ing the baseline LBP-TOP and our proposed Bi-WOOFmethods.',\n",
       " 'We also examine the computational eﬃ-ciency of our proposed method, and lay down some keypropositions derived from observations in this work.',\n",
       " 'We record both the F-measure and Accuracy measure-ments for diﬀerent blocks sizes, including 5 × 5, 6 × 6,7 × 7 and 8 × 8 for both feature extraction methods.',\n",
       " 'The best F-measure performance achieved by LBP-TOPis 41%, while Bi-WOOF method achieves 47%.',\n",
       " 'Both re-sults are obtained when block size is set to 6 × 6.',\n",
       " 'Note that the sequence-based methods #1to #13 considered all frames in the video sequence (i.e.,frames from onset to oﬀset).',\n",
       " 'Meanwhile, methods #14 to#19 consider only information from the apex and onsetframes, whereby only two images are processed to extractfeatures.',\n",
       " 'Essentially, our proposed apex-based approach requiresdetermining the apex frame for each video sequence.',\n",
       " 'For CASME II, the ground-truth apex frameindices are already provided, so we can use them directly.',\n",
       " 'Features are then computed using the apex/ ran-dom frame and the onset (reference) frame using LBP ,HOOF and Bi-WOOF descriptors.',\n",
       " 'We observe that the uti-lization of the apex frame always yields better recognitionresults when compared to using random frames.',\n",
       " 'As such,it can be concluded that the apex frame plays an impor-tant role in forming discriminative features.',\n",
       " 'For method #1 (i.e., LBP-TOP), also referred to asthe baseline, we reproduced the experiments for the fourdatasets based on the original papers [9, 14].',\n",
       " 'The recog-nition rates for methods #2 to #11 are reported fromtheir respective works of the same experimental proto-col.',\n",
       " 'Besides, we replicated method #12 and evaluate iton CASME II database.',\n",
       " 'This is because the original pa-per [28] classiﬁes the emotion into 4 types (i.e., positive,negative, surprise and others).',\n",
       " 'For method #13, Bi-WOOF is applied on all framesin the video sequence.',\n",
       " 'The features were computed byﬁrst estimating the three characteristics of the optical ﬂow(i.e., orientation, magnitude and strain) between the onsetand each subsequent frame (i.e., {fi,1, fi,j}, j ∈ 2, .',\n",
       " 'Next, Bi-WOOF was computed for each pair of framesto obtain the resultant histogram.',\n",
       " 'LBP was applied on the diﬀerence image to computethe features in methods #14 and #15.',\n",
       " 'Note that the im-age subtraction process is only applicable for methods #14(LBP - random & onset) and #15 (LBP - apex & onset).',\n",
       " 'This is because LBP feature extractor can only capturethe spatial features of an image and it is incapable of ex-tracting the temporal features of two images.',\n",
       " 'Speciﬁcally,the spatial features extracted from the apex frame andthe onset frame are not correlated.',\n",
       " 'Hence, we performan image subtraction process in order to generate a singleimage from two images (i.e., apex / random frame andonset frame).',\n",
       " 'This image subtraction process can removea person’s identity while preserving the characteristics offacial micro-movements.',\n",
       " 'Table 3 suggests that the proposed algorithm (viz., #19)achieves promising results in all four datasets.',\n",
       " 'More pre-cisely, it outperformed all the other methods in CASMEII.',\n",
       " 'In addition, for SMIC-VIS and SMIC-NIR, the resultsof the proposed method are comparable to those of #9,viz., FDM method.',\n",
       " 'Analysis and DiscussionTo further analyze the recognition performances, weprovide the confusion matrices for the selected databases.',\n",
       " 'Firstly, for CAS(ME)2, as tabulated in Table 4, it canbe seen that the recognition rate using Bi-WOOF methodoutperforms LBP-TOP method for all block sizes.',\n",
       " 'There-fore, it can be concluded that the Bi-WOOF method issuperior compared to the baseline method.',\n",
       " 'This is because most works in the literature testedon these two spontaneous micro-expression databases,making performance comparisons possible.',\n",
       " 'The confusion matrices are recorded in Tables 5 and 6for CASME II and SMIC-HS, respectively.',\n",
       " 'More concretely, in CASME II, the recognition rateof surprise, disgust, repression, happiness and other ex-pressions were improved by 44%, 30%, 22%, 13% and 4%,respectively.',\n",
       " 'Furthermore, for SMIC-HS, the recognitionrate of the expressions of negative, surprise and positivewere improved by 31%, 19% and 18%, respectively.',\n",
       " 'Figure 7 exempliﬁes the components derived from op-tical ﬂow using onset and apex frames of the video sample“s04 sur 01” in SMIC-HS, where the micro-expression ofsurprise is shown.',\n",
       " 'Referring to the labeling criteria of theemotion in [9], the changes in facial muscles are center-ing at the eyebrow regions.',\n",
       " 'We can hardly tell the fa-cial movements in Figure 7a, 7b and 7c.',\n",
       " 'For Figure 7d,a noticeable amount of the muscular changes are occur-ring at the upper part of the face, whereas in Figure 7e,the eyebrows regions have obvious facial movement.',\n",
       " 'Sincemagnitude information emphasizes the amplitude of thefacial changes, we exploit it as local weight.',\n",
       " 'Due to thecomputation of higher order derivatives in obtaining theoptical strain magnitudes, optical strain has the abilityto remove the noise and preserve large motion changes.',\n",
       " 'In addition, [24] demonstrated that optical strain globallyweighted on the LBP-TOP features produced better recog-nition results when compared to results obtained withoutthe weighting.',\n",
       " 'The number of histogram bins C in Eq. (10) is empir-ically determined to be 8 for both the CASME II andSMIC-HS databases.',\n",
       " 'Table 7 quantitatively illustrates therelationship between the recognition performance and thehistogram bins.',\n",
       " 'It can be seen that with histogram bin =8, the Bi-WOOF feature extractor achieves the best recog-nition results on both CASME II and SMIC-HS databases.',\n",
       " 'We provide in Table 8 a closer look into the eﬀects ofapplying (and not applying) the global and local weight-ing schemes on the Bi-WOOF features.',\n",
       " 'Results are the poorest when no global weight-ing is applied, which shows the importance of altering theprominence of features in diﬀerent blocks.',\n",
       " 'Both experiments were carried out on anIntel Core i7-4770 CPU 3.40GHz processor.',\n",
       " 'Results sug-gest that the case of two images is ∼33 times faster thanthe case of whole sequence.',\n",
       " 'The apex frame is the most important framein a micro-expression clip, that it contains themost intense or expressive micro-expression informa-tion.',\n",
       " 'Control experiments using random frame selec-tion (as the supposed apex frame) substantiates thisfact.',\n",
       " 'Also, further insights into locating the apicesof speciﬁc facial Action Units (AUs) could possiblyprovide even better discrimination between types ofmicro-expressions.',\n",
       " 'A majority of recent state-of-the-art methods promote the use of the entire videosequence, or a reduced set of frames [14, 29].',\n",
       " 'At this juncture, it is premature to as-certain speciﬁc reasons behind this ﬁnding.',\n",
       " 'In this paper, we demonstrated that it is suﬃcient toencode facial micro-expression features by utilizing onlythe apex frame (and onset frame as reference frame).',\n",
       " 'Tothe best of our knowledge, this is the ﬁrst attempt at rec-ognizing micro-expressions in video using only the apexframe.',\n",
       " 'For databases that do not provide apex frame anno-tations, the apex frame can be acquired by automatic spot-ting method based on a divide-and-conquer search strat-egy proposed in our recent work [41].',\n",
       " 'Experiments conducted on ﬁve publicly available micro-expression databases, namely, CAS(ME)2, CASME II,SMIC-HS, SMIC-NIR and SMIC-VIS, demonstrated theeﬀectiveness and eﬃciency of the proposed approach.',\n",
       " 'This workwas supported in part by Telekom Malaysia R&D Grant(Project 2beAware), Multimedia University and Univer-sity of Malaya.',\n",
       " 'Chen,X. Fu, CASME II: An improved spontaneous micro-expressiondatabase and the baseline evaluation, PLoS ONE 9 (2014)e86041.',\n",
       " 'on Computer Vision and PatternRecognition, 2013, pp.',\n",
       " 'Phan, J. See, Spontaneous subtle ex-pression recognition: Imbalanced databases and solutions, in:Asian Conference on Computer Vision, Springer, 2014, pp.',\n",
       " 'Tan, Spontaneous subtle expression detectionand recognition based on facial strain, Signal Processing: ImageCommunication 47 (2016) 170–182.',\n",
       " 'Phan, J. See, Y. H. Oh, K. Wong, Opticalstrain based recognition of subtle emotions, in: InternationalSymposium on Intelligent Signal Processing and Communica-tion Systems, 2014, pp.',\n",
       " 'Oh,K. Wong, Subtle expression recognition using optical strainweighted features, in: Asian Conference on Computer Vision,Springer, 2014, pp.',\n",
       " 'Wang, G. Zhao, X. Fu,A main directional mean optical ﬂow feature for spontaneousmicro-expression recognition, IEEE Transactions on AﬀectiveComputing 7 (4) (2016) 299–310.',\n",
       " 'on Circuits andSystems for Video Technology 22 (10) (2012) 1420–1432.',\n",
       " 'Liong, J. See, K. Wong, A. C. Le Ngo, Y.-H.',\n",
       " 'Oh, R. Phan,Automatic apex frame spotting in micro-expression database,in: Pattern Recognition (ACPR), 2015 3rd IAPR Asian Con-ference on, 2015, pp.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "keyword_processor = KeywordProcessor()\n",
    "# keyword_processor.add_keyword(<unclean name>, <standardised name>)\n",
    "keyword_processor.add_keyword('used for')\n",
    "\n",
    "#keywords_found = keyword_processor.extract_keywords('I love Big Apple and Bay Area.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_wl = []\n",
    "for file in files:\n",
    "    text = open(file, encoding='utf8').read()\n",
    "    sents = [re.sub('\\n', '', sent) for sent in sent_tokenize.tokenize(text) if len(sent) > 40 and len(re.findall('\\n', sent)) < 4 and not sent.startswith('[')]\n",
    "    for sent in sents:\n",
    "        if keyword_processor.extract_keywords(sent):\n",
    "            context_wl.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We follow a 5-fold cross-validation scheme whereeleven subjects are selected for training and the remaining two subjects are used for testing.',\n",
       " 'The left plot of Fig. 7 shows the phase transition of iPursuitin the plane of n1 and n2 and (IP) is used for subspaceidentiﬁcation.',\n",
       " 'There are manyconstructions that can be used for this purpose, with different constructions leading to different adaptive algorithms [26]–[29].',\n",
       " 'This asymmetry will be shown later, e.g., in Example VII.4, to be problematic when the consensusstrategy is used for adaptation and learning over networks.',\n",
       " 'Nevertheless, we are not aware of similar coresets for poseestimation of kinematic data, or low-cost wireless trackingsystems that can be used for hovering of a very unstablequadcopter in dozens of frames per second.',\n",
       " 'Notation: Lowercase boldface letters represent vectors (x)and uppercase boldface letters are used for matrices (X).',\n",
       " 'All these M-step results and similar such results for many other models canthus directly be used for TV-EM.',\n",
       " 'The sets ˜K(n) were then directly used for the estimation of expectation values (4).',\n",
       " '‘Hard EM’ approaches have been used for many types of data models including deep models, andthey were often observed to work very well in practice.',\n",
       " 'For temperatures T > 1, the values of annealed posteriors become increasingly similar,and such posteriors are used for annealed EM.',\n",
       " 'These estimates are used for model selection via hypothesistesting, as we present in Section 7.',\n",
       " 'Furthermore, we also ﬁnd them called Markovnetworks [Pearl, 1988], from researchers in artiﬁcial intelligence, as a parallel to the terminologyBayesian networks, used for acyclic directed Markov ﬁelds.',\n",
       " 'Lastly, when Markov models were being more intensely developed, Cox and Wermuth [1993]gave a top-down, unifying view of some graphical models used for representing linear dependen-cies.',\n",
       " 'This correspondence can be used for providing a direct interpretationof Markov properties, both in the undirected and directed case, allowing an enhanced manipu-lation of these Markov models.',\n",
       " 'Other families of continuous distribu-tions that have been used for parametrizing Markov and Bayesian networks are nonparametricGaussian copulas [Liu et al., 2009] and elliptical distributions [Vogel and Fried, 2011], both ofwhich generalize the Gaussian distribution.',\n",
       " 'Martínez-Álvarez et al. (2011b) also modiﬁed the original PSF algorithm to be used for outlieroccurrence forecasting in time series.',\n",
       " 'One subset is used for analysis and theother one for validating the analysis.',\n",
       " 'For both datasets, all the recorded values except for the ﬁnal year are considered as training data,and the last year is used for testing purposes.',\n",
       " 'Moreprecisely, given b-th tree in the forest, let us denote n(t)b the number of times the pair (η(y(t)),τ (t)) is repeatedin the bootstrap sample that is used for building the b-th tree.',\n",
       " 'The predict methodcan be used for this purpose.',\n",
       " 'The de-biased Whittle likelihood can also be used for discrete-time process models, as (10) can becomputed from the theoretical autocovariance sequence of the discrete process in exactly the same way.',\n",
       " 'Note that Z(X n, Y n) is used for only the non-redundant case, and p(xi, y(11)ZXY11(X n, Y n11) has two diﬀerent deﬁnitions.',\n",
       " 'We also believe the ideas and techniques developed in this work can be be applied to othersettings such as personalized pricing where information about the buyers can be used for pricediﬀerentiation or optimizing reserve prices in online ad auctions.',\n",
       " 'Theoperations in appnext(I, B) on the other hand are used for changing knowledge basesover time.',\n",
       " 'They are not used for computing the current semantics but are applied inthe next point in time depending on the current semantics.',\n",
       " 'The ﬁrst rule is used for initialization ensuring that if no time information is yet avail-able, the logical time is set to the value 0.',\n",
       " 'We generically modelsuch scenario with an rMCS with a context Cd, which is used for emergency detectionin such a dynamic environment, and an input language ILs, which represents the pos-sible observations.',\n",
       " 'Then, we consider EVOLP [2], a framework that focuses on dynamicsin the form of updates in the restricted setting of generalized logic programs buildingon similar notions as the operator next used for rMCSs.',\n",
       " 'The former is intendedto be used for input streams, and the latter for intermediate and output streams.',\n",
       " 'In [15], the k-nearestneighbor algorithm is used for probabilistic forecasts in the frame ofthe Global Energy Forecasting Competition 2014.',\n",
       " 'The training setconsists in around 7200 instant-points, satisfying a ratio of 83% of thedata dedicated to learning and 17% used for test.',\n",
       " 'The pass-word dataset uses a validation procedure similar to Killourhy and Maxion [29],except only samples from the 4th session (repetitions 150-200) are used for train-ing and sessions 5-8 (repetitions 201-400) for testing.',\n",
       " 'In each fold, one sample from each user isretained as a query and the remaining samples are used for training.',\n",
       " 'Similar to the HMM, the EM algorithm can be used for estimatingthe parameters of the partially-HMM [43].',\n",
       " 'It may also be used for retraining the last layer of a pre-trained deepneural network: given a new task unseen during the full network training and given limited amountof training data, data augmentation may be indeed crucial to obtain good prediction and S-MISOcan help accelerate learning in this setting.',\n",
       " 'Now, proximal ADMM-m is ready to be used for solving (4.2) because AN +1 = I and y isunconstrained.',\n",
       " 'A separate subspace is used for each of thetraining examples, that is, for each of the input Gaussians.',\n",
       " 'At each step of the cross validation, one video wasused as the test-set and the rest were used for training.',\n",
       " 'It is used for computing an upper bound on the probability that the algorithmthinks arm j is the best.',\n",
       " 'It has also been used for some biomet-ric recognition tasks such as iris, ﬁngerprint and palmprintrecognition [28]-[30].',\n",
       " 'Principal component analysis (PCA) is a pow-erful algorithm used for dimensionality reduction [32].',\n",
       " 'Different machine learningalgorithms can be used for classiﬁcation.',\n",
       " 'Then PCA is applied to allfeatures and the ﬁrst K PCA features are used for recognition.',\n",
       " 'Multi-class SVM is used for the classiﬁcation.',\n",
       " 'Choice of Parameters for LiSSA: To pick the parameters for our algorithm, we observe that itexhibits smooth behavior even in the case of S1 = 1, so this is used for the experiments.',\n",
       " 'Forthose limitation and large computing cost, those advancedmethods can not be used for large 3D point clouds.',\n",
       " 'method is designed to be efﬁcient, robust to point den-sity variation and can be used for many large point cloudsprocessing, including visualisation.',\n",
       " 'However, Bereuter (2015) recently gave an overview ofhow quad tree can be used for point generalisation.',\n",
       " 'We also use MidOc at the table level to reduce the numberof patches used for visualisation.',\n",
       " 'First it can be used for graphicalLOD, as a service for point cloud visualisation.',\n",
       " 'Third the ordering can be used for pointcloud generalisation, as a service for processing methodsthat may only be able to deal with a fraction of the points.',\n",
       " 'Section 3 intro-duces the simple features and SVR regression method used for experiments in this paper.',\n",
       " 'The training set is constructed by manually collecting30-40 frames from the video, and the rest frames are used for testing.',\n",
       " 'For UCSD datasets, frames 601-1400 are used for trainingand the rest for testing.',\n",
       " 'For Fudan datasets, frames 1-300 are used for training and the rest for testing.',\n",
       " 'Assumptions within A0 (fixed shape and amplitude pulses and instantaneous decoding) imply that the only information available to a recipient of j are the pulse-arrival-times, or equivalently, the IPIs, assuming an available timer; moreover, these IPIs cannot be used for block-coding that uses delays.',\n",
       " 'The default training/testsplit is used for the time series datasets.',\n",
       " 'We noted importantlythat the EP methods also can be used for more general forward model ensembles (A), whilethe AMP-based methods assume a Gaussian i.i.d.',\n",
       " 'Mobile phone metadata is increasingly used for humanitar-ian purposes in developing countries as traditional data is scarce.',\n",
       " 'Bayesianoptimization is used for tuning seven of these as proposed in [20], covering e.g.the learning rate, L2 regularization, and the number of ﬁlters in the horizontalconv.',\n",
       " 'We only consider the individualbandicoot features as our ConvNet does not capture location and movementinformation used for the mobility features.',\n",
       " 'The kdda dataset has been used for the KDD Cup 2010data mining competition.',\n",
       " 'The OpenMP library is used for parallelprogramming.',\n",
       " 'Figures 4 (c) and 4 (f) show the object classiﬁcation ac-curacy versus the number of views used for the predictionin case (i) and case (ii), respectively.',\n",
       " 'The number in each image indicatesthe number of views used for predictions.',\n",
       " 'Furthermore, the abductive interpretation approach has been used for ar-rhythmia detection in short single-lead ECG records, focusing on atrial ﬁb-rillation [45].',\n",
       " 'In this work, we aim to study nonlinear supervised dimension-ality reduction methods and present performance bounds based on the properties of theembedding and the interpolation function used for generalizing the embedding.',\n",
       " 'Smooth functions are commonly used for out-of-sampleinterpolation, e.g. as in (Qiao et al., 2013), (Peherstorfer et al., 2011).',\n",
       " 'The notation h2(p) := −p log p− (1− p) log(1− p)is used for the binary entropy function, a ∗ b := a(1 − b) + (1 − a)b is the binary convolution operation and thesymbol ⊕ denotes binary addition.',\n",
       " 'In computer science,especially in artiﬁcial intelligence, they have been used for 30years for learning purposes.',\n",
       " 'Note that we used the same scriptas that used for the BOSSBase in order to transform the RAW fullresolution color images into grey-level images6.',\n",
       " 'Note also that we used thesame script as that used for BOSSBase in order to generate thegrey-level images.',\n",
       " 'Those ﬁlters areused to deﬁne projections that will then be used for computing ahistogram leading to a feature vector.',\n",
       " 'Their scenario is different since it is used for theforensic-steganalysis.',\n",
       " 'Instead of densely-sampled point trajectories, super-voxel hierarchies were used for target candidates.',\n",
       " '4.1 AssumptionsWe make the following assumptions for the analysis of fLSTD-SA:(A1) The set C := {θ ∈ Rd | (cid:107)θ(cid:107)2 ≤ H} used for projection via Υ satisﬁes H >(A2) Bounded features: (cid:107)φ(si)(cid:107)2 ≤ Φmax < ∞, for i = 1, .',\n",
       " 'Iteration 1for fLSPI-SA is used for reporting the tracking error and we observed similar behavior across iterations,i.e., we observed that fLSTD-SA iterate θτ is close to the corresponding LSTDQ solution in each iterationof fLSPI-SA.',\n",
       " 'We describe the HL-MRFs used for our experiments using the PSLrules that deﬁne them.',\n",
       " 'Joachims et al. (2009) extend thisformulation to a “one slack” formulation, in which a single slack variable is used for all theconstraints across all training examples, which is more eﬃcient.',\n",
       " 'It is worth-mentioning that this new data augmentation strategy can alsobe used for boosting performances of deep models for staticsaliency estimation.',\n",
       " 'NCC is a general measure used for assessing image simi-larity.',\n",
       " 'Note thoughthat our bounds can be directly used for ERM, which can be implemented without knowledge of η.We also leave untouched the fact that for parametric models, Zhang’s bounds lead to an un-necessary log n-factor in the convergence rates.',\n",
       " 'The  pre-processing  however  creates  the  batch  rows,  only  1  for each category and so much fewer row numbers are subsequently used for training.',\n",
       " 'Different loss functions are used for trainingthe tasks of face detection,landmark localization, poseestimation and gender classiﬁcation.',\n",
       " 'These models are described as follows:is used for face detectiontask.',\n",
       " 'R-CNN Fiducial: This model is used for locating thefacial landmarks.',\n",
       " 'Onlyregion proposals which have an overlap> 0.5 with theground truth bounding box are used for training.',\n",
       " 'R-CNN Pose: This model is used for head pose estima-tion task.',\n",
       " 'R-CNN Gender: This model is used for face genderrecognition task.',\n",
       " 'The remaining1000 images were used for testing.',\n",
       " 'The detection boxes used for evaluatingthe landmark localization task are used here as well for ini-tialization.',\n",
       " 'These features can be used for landmarklocalization and pose estimation tasks.',\n",
       " 'The following benchmark datasets were used for comparison.',\n",
       " 'The previous equations are used for the chromosome, and the worst gene of this chromosome that exhibits the maximum distance is used for the mutation operation.',\n",
       " 'Some of the proposed mutations can be used for other problems with some modifications and not only oriented to the TSP problem, such as the knapsack problem.',\n",
       " 'However, even with several thousands of cells (∼5000) in each run, only less than half ofthe cells typically contain enough transcribed genes, that can be used for statistical analysis.',\n",
       " 'We have trained a deep CNN-based event classiﬁer(‘Event CNN’ in Table 3) to be used for generating the atten-tion maps.',\n",
       " 'Tables 1 and 2, respectively, summarize the data sets used for these tasks.',\n",
       " '“Var.” refersto variants used for h(x) and C, i.e., “Box” for box-constrained and “(cid:96)1” for (cid:96)1-regularized variants, asmentioned in Section 4.',\n",
       " '“Var.” refersto variants used for h(x) and C, i.e., “Box” for box-constrained and “(cid:96)1” for (cid:96)1-regularized variants, asmentioned in Section 4.',\n",
       " 'The 2,000 molecules used for training theGaussian process were selected to be maximally diverse.',\n",
       " 'In addition, our proposed approach also has ability togenerate a compact set of parameters in a robust model thatcan later be used for classiﬁcation.',\n",
       " 'The shape modelis used to learn the facial shape structure while texture modelis used for texture variations.',\n",
       " 'Then this relationship can be used for ﬁtting process.',\n",
       " 'Dif-ferent magniﬁcation factors α are used for evaluating thequality of DAMs reconstructions.',\n",
       " 'RMSE is acommon metric that is usually used for evaluating image re-covery task.',\n",
       " 'The cosine distance is used for the matching score.',\n",
       " 'The remaining 200 images were used for testing.',\n",
       " 'Overall, running OK-FEB on N data and with a valueof ǫ such that ˇN ≤ N data are used for updates requiresO( ˇN (Br(B + r) + r3) + N (B(D + r) + r2)).',\n",
       " 'Here we prove SRC consistencyfor the stochastic block model [36], [37], [38], which is apopular network model commonly used for classiﬁcationand clustering.',\n",
       " 'Different proximal combiners can used for computing proxr (4.2).',\n",
       " 'Here we focus again on theanisotropic TV case to show how the presented solvers can also be used for this image task.',\n",
       " 'This technique is broadly used for signalprocessing and the analysis of its suitability in the context of some applications is a topic ofresent research [22–25].',\n",
       " 'When processing large signals, however, the storage requirement ofthe implementations by direct methods frequently exceed the memory capacity of a personalcomputer used for research purposes.',\n",
       " '2.2 Convergence rate of the self projection stepsWe start by recalling some properties of symmetric matrices, which will be used for the analysis.',\n",
       " 'This inequality will be used for the analysis of the convergence rate of the self-projection step.',\n",
       " 'Example frames from the video sequences used for training are shown in Figure 5.',\n",
       " 'Note that the subset of randomly selected 1 million samples used for training FC4-1M and FC7-1Mwas the same.',\n",
       " 'They involve a set of videos that were used for dictionarytraining in [22], provided by the authors, as well as the “Basketball” video sequence used by [44].',\n",
       " 'For classes where the total number of observations is lessthan 20 or 200, for the 10Ex or 100Ex condition, respectively,half of the observations are used for training, and the restfor testing.',\n",
       " 'The same constant isalso used for the regularization of ill-posed kernel (AKDA,AKSDA), within-class or total scatter matrix (PCA, LDA,KDA, KSDA).',\n",
       " 'The data was sampled at 50 Hz, andthe ﬁrst 8s (N = 400) were used for identiﬁcation and the last7s for validation.',\n",
       " 'The ﬁrst 1250 sampleswhere used for identiﬁcation, and the last 1250 samples forvalidation.',\n",
       " 'The authors would like to thank Vladimir Koltchinskii for valuableremarks and Will Crampton for providing the data set used for the real data example.',\n",
       " 'When such architectures are used for fine-tuning, the issues caused by such deep and heavy architectures such as computational, memory and time overhead, are also imposed.',\n",
       " 'It consists of 630,420 32x32 color images of which 73,257 images are used for training, 26,032 images are used for testing and the other 531,131 images are used for extra training.',\n",
       " 'Wehighlight the fact that the errors and the residuals shown in this ﬁgure are measured respectively tothe distribution of interest µ, and not the distribution dµ,π∗ used for the optimization.',\n",
       " 'Notice that here again, the integrated error and residual are deﬁned with respect to µ, the distributionof interest, and not να, the sampling distribution used for optimization.',\n",
       " 'These arethe same data that where used for Fig. 1, presented in a different manner.',\n",
       " 'Nonetheless non-linear performance measures are widely used for the evaluationof learning algorithms.',\n",
       " 'Multiple classiﬁer systems are not widely used for F -measure maximization, and are stillin nascent stages.',\n",
       " 'LIBSVM (Chang and Lin, 2011) library was used for the kernelSVM.',\n",
       " 'Following [11], the convolutional siamese nets were trained on a same-or-differenttask of the original training data set and then the last layer was used for nearest neighbour matching.',\n",
       " 'It is noteworthy to mention that loopclosure and failure recovery revolve around the same problemand solutions put forward to any of them could be used for theother.',\n",
       " 'The descriptor used for data associationis extracted from the 2D image from which the 3D landmarkwas ﬁrst observed.',\n",
       " 'The place recognition module implemented inORB SLAM, used for both loop detection and failure recovery,relies on bags of words, as frames observing the same sceneshare a big number of common visual vocabulary.',\n",
       " '(2) The type of method (e.g., mutual information, correlation) used for network inference.',\n",
       " 'This results in 21200 positive samples, half ofwhich were used for traininig (with equal amount of sampled negative examples) and therest were used for testing.',\n",
       " 'In the case without occlusion (A),real targets can be reached (although only virtual targetswere used for training).',\n",
       " 'Each  of  these  images  is provided with a  special means and used for some special usages.',\n",
       " 'Looking at our experimental results as well as the resultsof state-of-the-art methods [3, 7, 16], it becomes clear thatthe current numerical metrics used for evaluating estimateddepth are not always consistent.',\n",
       " 'Special thanks to Ethan Schoonover, creator of the Solarized colorscheme,1 whose colors were used for the ﬁgures.',\n",
       " 'Notably, the kernel w deﬁnes both the matrices C and CT used for theforward and backward passes.',\n",
       " 'Algorithm 1 was used for the Dubins Carexample with the nonlinear solver Ipopt [29].',\n",
       " 'Then, the “clean” frontalaligned faces are used for training data to perform faceidentiﬁcation with occluded test images.',\n",
       " 'For large matrices P, iterative algorithmscan be used for solving this linear system of equations whenmatrix inversion is not feasible.',\n",
       " 'The active set method [37] used for solving thenonnegative least squares problem becomes also slow due tothe computation of the pseudoinverse of the system matrix ineach iteration.',\n",
       " 'images used for the block occlusionFigure 5(b) and (c) respectively, block occlusion was tested byrandomly placing the objects on each test image.',\n",
       " 'Similarly to the previous Extended Yale Bsettings, Subsets 1 and 2 of Extended Yale B were used fortraining and Subset 3 was used for testing.',\n",
       " 'It turns out that the times ofthe data used for applications in Section 5 is well ﬁtted by Weibull distributions.']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(zip(np.random.uniform(0, 1, 100), np.random.uniform(10, 29, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = iter(range(100))\n",
    "f = lambda: next(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So, for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[d['target'] != 'NO_RELATION']['sent'].iloc[f()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'This correspondence can be used for providing a direct interpretationof Markov properties, both in the undirected and directed case, allowing an enhanced manipu-lation of these Markov models.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = nlp(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_conll(sent):\n",
    "    t = []\n",
    "    for i, word in enumerate(sent):\n",
    "#         if word.head is word:\n",
    "#             head_idx = 0\n",
    "#         else:\n",
    "#              head_idx = word.i-sent[i].i+1\n",
    "        head_idx = word.head.i+1\n",
    "        t.append((\n",
    "            str(i+1),\n",
    "            word.lemma_,\n",
    "            '_',# There's a word.i attr that's position in *doc*\n",
    "            word.pos_,\n",
    "            '_',# Coarse-grained tag\n",
    "            word.tag_, # Fine-grained tag\n",
    "            str(head_idx) if word.dep_ != 'ROOT' else str(0),\n",
    "            word.dep_, # Relation\n",
    "            '_', '_'))\n",
    "    return '\\n'.join(['\\t'.join(s) for s in t])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('trees_golden.txt', 'w')\n",
    "for line in d[d['target'] != 'NO_RELATION']['sent']:\n",
    "    l = re.sub('[\\r\\n]', '', line)\n",
    "    l = l.lstrip(' ')\n",
    "    doc = nlp(l)\n",
    "    \n",
    "    conll = to_conll(doc)\n",
    "    f.write(conll+'\\n\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This correspondence,\n",
       " a direct interpretationof Markov properties,\n",
       " the undirected and directed case,\n",
       " an enhanced manipu-lation,\n",
       " these Markov models]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computational complexity'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.ent1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'phrase translations'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.ent2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mnefedov/Рабочий стол/phd'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p.print_tree()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "applied"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.sents)[0].root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "t = to_nltk_tree(list(p.sents)[0].root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for sent in p.sents:\n",
    "    for i, word in enumerate(sent):\n",
    "#         if word.head is word:\n",
    "#             head_idx = 0\n",
    "#         else:\n",
    "#              head_idx = word.i-sent[i].i+1\n",
    "        head_idx = word.head.i+1\n",
    "        t.append((\n",
    "            str(i+1),\n",
    "            word.lemma_,\n",
    "            '_',# There's a word.i attr that's position in *doc*\n",
    "            word.pos_,\n",
    "            '_',# Coarse-grained tag\n",
    "            word.tag_, # Fine-grained tag\n",
    "            str(head_idx) if word.dep_ != 'ROOT' else str(0),\n",
    "            word.dep_, # Relation\n",
    "            '_', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.head.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = '\\n'.join(['\\t'.join(s) for s in t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'this', '_', 'DET', '_', 'DT', '2', 'det', '_', '_'),\n",
       " ('2', 'correspondence', '_', 'NOUN', '_', 'NN', '5', 'nsubjpass', '_', '_'),\n",
       " ('3', 'can', '_', 'VERB', '_', 'MD', '5', 'aux', '_', '_'),\n",
       " ('4', 'be', '_', 'VERB', '_', 'VB', '5', 'auxpass', '_', '_'),\n",
       " ('5', 'use', '_', 'VERB', '_', 'VBN', '0', 'ROOT', '_', '_'),\n",
       " ('6', 'for', '_', 'ADP', '_', 'IN', '5', 'prep', '_', '_'),\n",
       " ('7', 'provide', '_', 'VERB', '_', 'VBG', '6', 'pcomp', '_', '_'),\n",
       " ('8', 'a', '_', 'DET', '_', 'DT', '12', 'det', '_', '_'),\n",
       " ('9', 'direct', '_', 'ADJ', '_', 'JJ', '12', 'amod', '_', '_'),\n",
       " ('10', 'interpretationof', '_', 'NOUN', '_', 'NN', '12', 'amod', '_', '_'),\n",
       " ('11', 'markov', '_', 'PROPN', '_', 'NNP', '12', 'compound', '_', '_'),\n",
       " ('12', 'property', '_', 'NOUN', '_', 'NNS', '7', 'dobj', '_', '_'),\n",
       " ('13', ',', '_', 'PUNCT', '_', ',', '7', 'punct', '_', '_'),\n",
       " ('14', 'both', '_', 'CCONJ', '_', 'CC', '15', 'advmod', '_', '_'),\n",
       " ('15', 'in', '_', 'ADP', '_', 'IN', '7', 'prep', '_', '_'),\n",
       " ('16', 'the', '_', 'DET', '_', 'DT', '20', 'det', '_', '_'),\n",
       " ('17', 'undirected', '_', 'ADJ', '_', 'JJ', '20', 'amod', '_', '_'),\n",
       " ('18', 'and', '_', 'CCONJ', '_', 'CC', '17', 'cc', '_', '_'),\n",
       " ('19', 'direct', '_', 'VERB', '_', 'VBN', '17', 'conj', '_', '_'),\n",
       " ('20', 'case', '_', 'NOUN', '_', 'NN', '15', 'pobj', '_', '_'),\n",
       " ('21', ',', '_', 'PUNCT', '_', ',', '7', 'punct', '_', '_'),\n",
       " ('22', 'allow', '_', 'VERB', '_', 'VBG', '7', 'advcl', '_', '_'),\n",
       " ('23', 'an', '_', 'DET', '_', 'DT', '27', 'det', '_', '_'),\n",
       " ('24', 'enhanced', '_', 'ADJ', '_', 'JJ', '27', 'amod', '_', '_'),\n",
       " ('25', 'manipu', '_', 'ADJ', '_', 'JJ', '27', 'compound', '_', '_'),\n",
       " ('26', '-', '_', 'PUNCT', '_', 'HYPH', '27', 'punct', '_', '_'),\n",
       " ('27', 'lation', '_', 'NOUN', '_', 'NN', '22', 'dobj', '_', '_'),\n",
       " ('28', 'of', '_', 'ADP', '_', 'IN', '27', 'prep', '_', '_'),\n",
       " ('29', 'these', '_', 'DET', '_', 'DT', '31', 'det', '_', '_'),\n",
       " ('30', 'markov', '_', 'PROPN', '_', 'NNP', '31', 'compound', '_', '_'),\n",
       " ('31', 'model', '_', 'NOUN', '_', 'NNS', '28', 'pobj', '_', '_'),\n",
       " ('32', '.', '_', 'PUNCT', '_', '.', '5', 'punct', '_', '_')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps_on_root = []\n",
    "root = ''\n",
    "for i, word, _, pos, _, _, head, rel, _, _ in t:\n",
    "    if rel == 'ROOT':\n",
    "        root = i\n",
    "\n",
    "for i, word, _, pos, _, _, head, rel, _, _ in t:\n",
    "    if head == root:\n",
    "        if rel != 'punct':\n",
    "            deps_on_root.append((i, str(word), rel))\n",
    "            \n",
    "def subtree(deps, h):\n",
    "    if h not in deps:\n",
    "        return h\n",
    "    else:\n",
    "        return [subtree(deps, sh) for sh in deps[h]] + [h]\n",
    "    \n",
    "\n",
    "def flatten(ll):\n",
    "    if not isinstance(ll, list):\n",
    "        return [ll]\n",
    "    \n",
    "    fl = []\n",
    "    for el in ll:\n",
    "        if not isinstance(el, list):\n",
    "            fl += [el]\n",
    "        else:\n",
    "            fl += flatten(el)\n",
    "    return sorted(fl, key=lambda x: int(x))\n",
    "\n",
    "def get_seqs(subtrees):\n",
    "    seqs = []\n",
    "    for st in subtrees:\n",
    "        seqs.append([id2words[j] for j in flatten(st)])\n",
    "    return seqs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = defaultdict(list)\n",
    "id2words = {}\n",
    "id2rel = {}\n",
    "for i, word, _, pos, _, _, head, rel, _, _ in t:\n",
    "#     if rel == 'punct':\n",
    "#         continue\n",
    "    deps[head].append(i)\n",
    "    id2words[i] = word\n",
    "\n",
    "deps = dict(deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 'correspondence', 'nsubjpass'),\n",
       " ('3', 'can', 'aux'),\n",
       " ('4', 'be', 'auxpass'),\n",
       " ('6', 'for', 'prep')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps_on_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['provide',\n",
       "  'a',\n",
       "  'direct',\n",
       "  'interpretationof',\n",
       "  'markov',\n",
       "  'property',\n",
       "  ',',\n",
       "  'both',\n",
       "  'in',\n",
       "  'the',\n",
       "  'undirected',\n",
       "  'and',\n",
       "  'direct',\n",
       "  'case',\n",
       "  ',',\n",
       "  'allow',\n",
       "  'an',\n",
       "  'enhanced',\n",
       "  'manipu',\n",
       "  '-',\n",
       "  'lation',\n",
       "  'of',\n",
       "  'these',\n",
       "  'markov',\n",
       "  'model'],\n",
       " ['for']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seqs(subtree(deps, '6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['8', '9', '10', '11', '12'],\n",
       "  '13',\n",
       "  ['14', ['16', ['18', '19', '17'], '20'], '15'],\n",
       "  '21',\n",
       "  [['23', '24', '25', '26', [['29', '30', '31'], '28'], '27'], '22'],\n",
       "  '7'],\n",
       " '6']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtree(deps, '6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = flatten(['14', ['16', ['18', '19', '17'], '20'], '15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['both', 'in', 'the', 'undirected', 'and', 'direct', 'case']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[id2words[j] for j in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp = '1\\t-PRON-\\t_\\tPRON\\t_\\tPRP\\t1\\tnsubj\\t_\\t_\\n2\\tdetail\\t_\\tVERB\\t_\\tVBP\\t0\\tROOT\\t_\\t_\\n3\\tthe\\t_\\tDET\\t_\\tDT\\t3\\tdet\\t_\\t_\\n4\\tcomputational\\t_\\tADJ\\t_\\tJJ\\t4\\tamod\\t_\\t_\\n5\\tcomplexity\\t_\\tNOUN\\t_\\tNN\\t5\\tdobj\\t_\\t_\\n6\\tand\\t_\\tCCONJ\\t_\\tCC\\t6\\tcc\\t_\\t_\\n7\\taverage\\t_\\tADJ\\t_\\tJJ\\t7\\tamod\\t_\\t_\\n8\\tretrieval\\t_\\tNOUN\\t_\\tNN\\t8\\tconj\\t_\\t_\\n9\\ttime\\t_\\tNOUN\\t_\\tNNS\\t9\\tnpadvmod\\t_\\t_\\n10\\tfor\\t_\\tADP\\t_\\tIN\\t10\\tprep\\t_\\t_\\n11\\tlook\\t_\\tVERB\\t_\\tVBG\\t11\\tpcomp\\t_\\t_\\n12\\tup\\t_\\tPART\\t_\\tRP\\t12\\tprt\\t_\\t_\\n13\\tphrase\\t_\\tNOUN\\t_\\tNN\\t13\\tcompound\\t_\\t_\\n14\\ttranslation\\t_\\tNOUN\\t_\\tNNS\\t14\\tdobj\\t_\\t_\\n15\\tin\\t_\\tADP\\t_\\tIN\\t15\\tprep\\t_\\t_\\n16\\t-PRON-\\t_\\tADJ\\t_\\tPRP$\\t16\\tposs\\t_\\t_\\n17\\tsuffix\\t_\\tADJ\\t_\\tJJ\\t17\\tamod\\t_\\t_\\n18\\tarray\\t_\\tNOUN\\t_\\tNN\\t18\\tnpadvmod\\t_\\t_\\n19\\t-\\t_\\tPUNCT\\t_\\tHYPH\\t19\\tpunct\\t_\\t_\\n20\\tbase\\t_\\tVERB\\t_\\tVBN\\t20\\tamod\\t_\\t_\\n21\\tdata\\t_\\tNOUN\\t_\\tNN\\t21\\tcompound\\t_\\t_\\n22\\tstructure\\t_\\tNOUN\\t_\\tNN\\t22\\tpobj\\t_\\t_\\n23\\t.\\t_\\tPUNCT\\t_\\t.\\t23\\tpunct\\t_\\t_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import (DependencyGraph, ProjectiveDependencyParser, NonprojectiveDependencyParser)\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DependencyGraph(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(HMM, 45)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.left_children(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function nltk.parse.dependencygraph.DependencyGraph.__init__.<locals>.<lambda>>,\n",
       "            {0: {'address': 0,\n",
       "              'ctag': 'TOP',\n",
       "              'deps': defaultdict(list, {'ROOT': [5]}),\n",
       "              'feats': None,\n",
       "              'head': None,\n",
       "              'lemma': None,\n",
       "              'rel': None,\n",
       "              'tag': 'TOP',\n",
       "              'word': None},\n",
       "             1: {'address': 1,\n",
       "              'ctag': 'DET',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'DT',\n",
       "              'head': 2,\n",
       "              'lemma': '_',\n",
       "              'rel': 'det',\n",
       "              'tag': '_',\n",
       "              'word': 'this'},\n",
       "             2: {'address': 2,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list, {'det': [1]}),\n",
       "              'feats': 'NN',\n",
       "              'head': 5,\n",
       "              'lemma': '_',\n",
       "              'rel': 'nsubjpass',\n",
       "              'tag': '_',\n",
       "              'word': 'correspondence'},\n",
       "             3: {'address': 3,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'MD',\n",
       "              'head': 5,\n",
       "              'lemma': '_',\n",
       "              'rel': 'aux',\n",
       "              'tag': '_',\n",
       "              'word': 'can'},\n",
       "             4: {'address': 4,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'VB',\n",
       "              'head': 5,\n",
       "              'lemma': '_',\n",
       "              'rel': 'auxpass',\n",
       "              'tag': '_',\n",
       "              'word': 'be'},\n",
       "             5: {'address': 5,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'aux': [3],\n",
       "                           'auxpass': [4],\n",
       "                           'nsubjpass': [2],\n",
       "                           'prep': [6],\n",
       "                           'punct': [32]}),\n",
       "              'feats': 'VBN',\n",
       "              'head': 0,\n",
       "              'lemma': '_',\n",
       "              'rel': 'ROOT',\n",
       "              'tag': '_',\n",
       "              'word': 'use'},\n",
       "             6: {'address': 6,\n",
       "              'ctag': 'ADP',\n",
       "              'deps': defaultdict(list, {'pcomp': [7]}),\n",
       "              'feats': 'IN',\n",
       "              'head': 5,\n",
       "              'lemma': '_',\n",
       "              'rel': 'prep',\n",
       "              'tag': '_',\n",
       "              'word': 'for'},\n",
       "             7: {'address': 7,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'advcl': [22],\n",
       "                           'dobj': [12],\n",
       "                           'prep': [15],\n",
       "                           'punct': [13, 21]}),\n",
       "              'feats': 'VBG',\n",
       "              'head': 6,\n",
       "              'lemma': '_',\n",
       "              'rel': 'pcomp',\n",
       "              'tag': '_',\n",
       "              'word': 'provide'},\n",
       "             8: {'address': 8,\n",
       "              'ctag': 'DET',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'DT',\n",
       "              'head': 12,\n",
       "              'lemma': '_',\n",
       "              'rel': 'det',\n",
       "              'tag': '_',\n",
       "              'word': 'a'},\n",
       "             9: {'address': 9,\n",
       "              'ctag': 'ADJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'JJ',\n",
       "              'head': 12,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': '_',\n",
       "              'word': 'direct'},\n",
       "             10: {'address': 10,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'NN',\n",
       "              'head': 12,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': '_',\n",
       "              'word': 'interpretationof'},\n",
       "             11: {'address': 11,\n",
       "              'ctag': 'PROPN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'NNP',\n",
       "              'head': 12,\n",
       "              'lemma': '_',\n",
       "              'rel': 'compound',\n",
       "              'tag': '_',\n",
       "              'word': 'markov'},\n",
       "             12: {'address': 12,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'amod': [9, 10], 'compound': [11], 'det': [8]}),\n",
       "              'feats': 'NNS',\n",
       "              'head': 7,\n",
       "              'lemma': '_',\n",
       "              'rel': 'dobj',\n",
       "              'tag': '_',\n",
       "              'word': 'property'},\n",
       "             13: {'address': 13,\n",
       "              'ctag': 'PUNCT',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': ',',\n",
       "              'head': 7,\n",
       "              'lemma': '_',\n",
       "              'rel': 'punct',\n",
       "              'tag': '_',\n",
       "              'word': ','},\n",
       "             14: {'address': 14,\n",
       "              'ctag': 'CCONJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'CC',\n",
       "              'head': 15,\n",
       "              'lemma': '_',\n",
       "              'rel': 'advmod',\n",
       "              'tag': '_',\n",
       "              'word': 'both'},\n",
       "             15: {'address': 15,\n",
       "              'ctag': 'ADP',\n",
       "              'deps': defaultdict(list, {'advmod': [14], 'pobj': [20]}),\n",
       "              'feats': 'IN',\n",
       "              'head': 7,\n",
       "              'lemma': '_',\n",
       "              'rel': 'prep',\n",
       "              'tag': '_',\n",
       "              'word': 'in'},\n",
       "             16: {'address': 16,\n",
       "              'ctag': 'DET',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'DT',\n",
       "              'head': 20,\n",
       "              'lemma': '_',\n",
       "              'rel': 'det',\n",
       "              'tag': '_',\n",
       "              'word': 'the'},\n",
       "             17: {'address': 17,\n",
       "              'ctag': 'ADJ',\n",
       "              'deps': defaultdict(list, {'cc': [18], 'conj': [19]}),\n",
       "              'feats': 'JJ',\n",
       "              'head': 20,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': '_',\n",
       "              'word': 'undirected'},\n",
       "             18: {'address': 18,\n",
       "              'ctag': 'CCONJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'CC',\n",
       "              'head': 17,\n",
       "              'lemma': '_',\n",
       "              'rel': 'cc',\n",
       "              'tag': '_',\n",
       "              'word': 'and'},\n",
       "             19: {'address': 19,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'VBN',\n",
       "              'head': 17,\n",
       "              'lemma': '_',\n",
       "              'rel': 'conj',\n",
       "              'tag': '_',\n",
       "              'word': 'direct'},\n",
       "             20: {'address': 20,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list, {'amod': [17], 'det': [16]}),\n",
       "              'feats': 'NN',\n",
       "              'head': 15,\n",
       "              'lemma': '_',\n",
       "              'rel': 'pobj',\n",
       "              'tag': '_',\n",
       "              'word': 'case'},\n",
       "             21: {'address': 21,\n",
       "              'ctag': 'PUNCT',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': ',',\n",
       "              'head': 7,\n",
       "              'lemma': '_',\n",
       "              'rel': 'punct',\n",
       "              'tag': '_',\n",
       "              'word': ','},\n",
       "             22: {'address': 22,\n",
       "              'ctag': 'VERB',\n",
       "              'deps': defaultdict(list, {'dobj': [27]}),\n",
       "              'feats': 'VBG',\n",
       "              'head': 7,\n",
       "              'lemma': '_',\n",
       "              'rel': 'advcl',\n",
       "              'tag': '_',\n",
       "              'word': 'allow'},\n",
       "             23: {'address': 23,\n",
       "              'ctag': 'DET',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'DT',\n",
       "              'head': 27,\n",
       "              'lemma': '_',\n",
       "              'rel': 'det',\n",
       "              'tag': '_',\n",
       "              'word': 'an'},\n",
       "             24: {'address': 24,\n",
       "              'ctag': 'ADJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'JJ',\n",
       "              'head': 27,\n",
       "              'lemma': '_',\n",
       "              'rel': 'amod',\n",
       "              'tag': '_',\n",
       "              'word': 'enhanced'},\n",
       "             25: {'address': 25,\n",
       "              'ctag': 'ADJ',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'JJ',\n",
       "              'head': 27,\n",
       "              'lemma': '_',\n",
       "              'rel': 'compound',\n",
       "              'tag': '_',\n",
       "              'word': 'manipu'},\n",
       "             26: {'address': 26,\n",
       "              'ctag': 'PUNCT',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'HYPH',\n",
       "              'head': 27,\n",
       "              'lemma': '_',\n",
       "              'rel': 'punct',\n",
       "              'tag': '_',\n",
       "              'word': '-'},\n",
       "             27: {'address': 27,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list,\n",
       "                          {'amod': [24],\n",
       "                           'compound': [25],\n",
       "                           'det': [23],\n",
       "                           'prep': [28],\n",
       "                           'punct': [26]}),\n",
       "              'feats': 'NN',\n",
       "              'head': 22,\n",
       "              'lemma': '_',\n",
       "              'rel': 'dobj',\n",
       "              'tag': '_',\n",
       "              'word': 'lation'},\n",
       "             28: {'address': 28,\n",
       "              'ctag': 'ADP',\n",
       "              'deps': defaultdict(list, {'pobj': [31]}),\n",
       "              'feats': 'IN',\n",
       "              'head': 27,\n",
       "              'lemma': '_',\n",
       "              'rel': 'prep',\n",
       "              'tag': '_',\n",
       "              'word': 'of'},\n",
       "             29: {'address': 29,\n",
       "              'ctag': 'DET',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'DT',\n",
       "              'head': 31,\n",
       "              'lemma': '_',\n",
       "              'rel': 'det',\n",
       "              'tag': '_',\n",
       "              'word': 'these'},\n",
       "             30: {'address': 30,\n",
       "              'ctag': 'PROPN',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': 'NNP',\n",
       "              'head': 31,\n",
       "              'lemma': '_',\n",
       "              'rel': 'compound',\n",
       "              'tag': '_',\n",
       "              'word': 'markov'},\n",
       "             31: {'address': 31,\n",
       "              'ctag': 'NOUN',\n",
       "              'deps': defaultdict(list, {'compound': [30], 'det': [29]}),\n",
       "              'feats': 'NNS',\n",
       "              'head': 28,\n",
       "              'lemma': '_',\n",
       "              'rel': 'pobj',\n",
       "              'tag': '_',\n",
       "              'word': 'model'},\n",
       "             32: {'address': 32,\n",
       "              'ctag': 'PUNCT',\n",
       "              'deps': defaultdict(list, {}),\n",
       "              'feats': '.',\n",
       "              'head': 5,\n",
       "              'lemma': '_',\n",
       "              'rel': 'punct',\n",
       "              'tag': '_',\n",
       "              'word': '.'}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('use', 'VERB'), 'nsubjpass', ('correspondence', 'NOUN')),\n",
       " (('correspondence', 'NOUN'), 'det', ('this', 'DET')),\n",
       " (('use', 'VERB'), 'aux', ('can', 'VERB')),\n",
       " (('use', 'VERB'), 'auxpass', ('be', 'VERB')),\n",
       " (('use', 'VERB'), 'prep', ('for', 'ADP')),\n",
       " (('for', 'ADP'), 'pcomp', ('provide', 'VERB')),\n",
       " (('provide', 'VERB'), 'dobj', ('property', 'NOUN')),\n",
       " (('property', 'NOUN'), 'det', ('a', 'DET')),\n",
       " (('property', 'NOUN'), 'amod', ('direct', 'ADJ')),\n",
       " (('property', 'NOUN'), 'amod', ('interpretationof', 'NOUN')),\n",
       " (('property', 'NOUN'), 'compound', ('markov', 'PROPN')),\n",
       " (('provide', 'VERB'), 'punct', (',', 'PUNCT')),\n",
       " (('provide', 'VERB'), 'prep', ('in', 'ADP')),\n",
       " (('in', 'ADP'), 'advmod', ('both', 'CCONJ')),\n",
       " (('in', 'ADP'), 'pobj', ('case', 'NOUN')),\n",
       " (('case', 'NOUN'), 'det', ('the', 'DET')),\n",
       " (('case', 'NOUN'), 'amod', ('undirected', 'ADJ')),\n",
       " (('undirected', 'ADJ'), 'cc', ('and', 'CCONJ')),\n",
       " (('undirected', 'ADJ'), 'conj', ('direct', 'VERB')),\n",
       " (('provide', 'VERB'), 'punct', (',', 'PUNCT')),\n",
       " (('provide', 'VERB'), 'advcl', ('allow', 'VERB')),\n",
       " (('allow', 'VERB'), 'dobj', ('lation', 'NOUN')),\n",
       " (('lation', 'NOUN'), 'det', ('an', 'DET')),\n",
       " (('lation', 'NOUN'), 'amod', ('enhanced', 'ADJ')),\n",
       " (('lation', 'NOUN'), 'compound', ('manipu', 'ADJ')),\n",
       " (('lation', 'NOUN'), 'punct', ('-', 'PUNCT')),\n",
       " (('lation', 'NOUN'), 'prep', ('of', 'ADP')),\n",
       " (('of', 'ADP'), 'pobj', ('model', 'NOUN')),\n",
       " (('model', 'NOUN'), 'det', ('these', 'DET')),\n",
       " (('model', 'NOUN'), 'compound', ('markov', 'PROPN')),\n",
       " (('use', 'VERB'), 'punct', ('.', 'PUNCT'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dg.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "?dg.triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = dg.nx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "?g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[One way,\n",
       " the natural gradient,\n",
       " it,\n",
       " a steepestdescent direction,\n",
       " the negative gradient,\n",
       " respect,\n",
       " a metricthat,\n",
       " the distributions,\n",
       " the default Euclideanmetric,\n",
       " parameter space]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/runpy.py:184: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.5/runpy.py:184: DeprecationWarning: Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "tre = p.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way', 'show', '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x['lemma'] for x in tre[0]['modifiers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tre[0]['modifiers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 2,\n",
       " 'ctag': 'NOUN',\n",
       " 'deps': defaultdict(list, {'nummod': [1], 'relcl': [4]}),\n",
       " 'feats': 'NN',\n",
       " 'head': 8,\n",
       " 'lemma': '_',\n",
       " 'rel': 'nsubj',\n",
       " 'tag': '_',\n",
       " 'word': 'way'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.nodes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 7,\n",
       " 'ctag': 'NOUN',\n",
       " 'deps': defaultdict(list, {'amod': [6], 'det': [5]}),\n",
       " 'feats': 'NN',\n",
       " 'head': 4,\n",
       " 'lemma': '_',\n",
       " 'rel': 'dobj',\n",
       " 'tag': '_',\n",
       " 'word': 'gradient'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg.nodes[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(p.noun_chunks)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[One way,\n",
       " the natural gradient,\n",
       " it,\n",
       " a steepestdescent direction,\n",
       " the negative gradient,\n",
       " respect,\n",
       " a metricthat,\n",
       " the distributions,\n",
       " the default Euclideanmetric,\n",
       " parameter space]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the natural gradient"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('all_sents.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(text):\n",
    "    lines = []\n",
    "    for line in text:\n",
    "        lines.append(line)\n",
    "        if len(lines) > 3500:\n",
    "            yield ''.join(lines)\n",
    "            lines = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed -  0\n",
      "Processed -  35000\n",
      "Processed -  70000\n",
      "Processed -  105000\n",
      "Processed -  140000\n",
      "Processed -  175000\n",
      "Processed -  210000\n",
      "Processed -  245000\n",
      "Processed -  280000\n",
      "Processed -  315000\n",
      "Processed -  350000\n",
      "Processed -  385000\n",
      "Processed -  420000\n",
      "Processed -  455000\n",
      "Processed -  490000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d9681988cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sents_norm_.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processed - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m3500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.parse_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.get_batch_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         Yf = self.ops.xp.dot(X,\n\u001b[0;32m--> 148\u001b[0;31m             self.W.reshape((self.nF*self.nO*self.nP, self.nI)).T)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open('sents_norm_.txt', 'w')\n",
    "for i, batch in enumerate(batches(text)):\n",
    "    doc = nlp(batch)\n",
    "    if not i % 10:\n",
    "        print('Processed - ', i * 3500)\n",
    "    for sent in doc.sents:\n",
    "        f.write(' '.join([word.lemma_ for word in sent if not any([word.is_punct, word.is_space])]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('phrases.txt', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in sents:\n",
    "    p = nlp(sent)\n",
    "    f.write('\\t'.join([str(x).lower() for x in p.noun_chunks])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [x.split('\\t') for x in open('phrases.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834743"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['relative locations', 'major parts', 'the human brain\\n'],\n",
       " ['the cerebral cortical mantle',\n",
       "  'about thirty billion neurons',\n",
       "  'projections',\n",
       "  'the thalamus',\n",
       "  'reciprocal projections',\n",
       "  'the thalamocortical system\\n'],\n",
       " ['the mantle',\n",
       "  'three major cortical appendages',\n",
       "  'the basal ganglia',\n",
       "  'cerebellum',\n",
       "  'movement',\n",
       "  'the hippocampus',\n",
       "  'memory\\n'],\n",
       " ['them',\n",
       "  'the oldest part',\n",
       "  'the brain',\n",
       "  'evolutionary terms',\n",
       "  'several diffusely projecting value systems\\n'],\n",
       " ['bottom', 'synaptic connections', 'two neurons\\n'],\n",
       " ['an action potential',\n",
       "  'the axon',\n",
       "  'the presynaptic neuron',\n",
       "  'the release',\n",
       "  'a neurotransmitter',\n",
       "  'the synaptic cleft\\n'],\n",
       " ['the transmitter molecules',\n",
       "  'the postsynaptic membrane',\n",
       "  'the probability',\n",
       "  'the postsynaptic cell',\n",
       "  'turn',\n",
       "  'its own action potential\\n'],\n",
       " ['particular sequences', 'activity', 'the synapse', 'its efficacy\\n'],\n",
       " ['the number',\n",
       "  'different shapes',\n",
       "  'kinds',\n",
       "  'neurons',\n",
       "  'this drawing',\n",
       "  'a greatly simplified cartoon\\n'],\n",
       " ['1:55 pm',\n",
       "  '(figure',\n",
       "  'top',\n",
       "  'about thirty billion nerve cells',\n",
       "  'neurons',\n",
       "  'one million billion connections\\n'],\n",
       " ['the number',\n",
       "  'possible active pathways',\n",
       "  'such a structure',\n",
       "  'the number',\n",
       "  'elementary particles',\n",
       "  'the known universe\\n'],\n",
       " ['the place', 'detail', 'the brain', 'rise', 'consciousness\\n'],\n",
       " ['i', 'several books\\n'],\n",
       " ['i', 'a working picture', 'brain structure', 'activity\\n'],\n",
       " ['i',\n",
       "  'a mixture',\n",
       "  'earth',\n",
       "  'analogy',\n",
       "  'metaphor—',\n",
       "  'an idea',\n",
       "  'consciousness\\n'],\n",
       " ['’s', 'the fundamental cells', 'signals', 'the brain\\n'],\n",
       " ['the neurons',\n",
       "  'a treelike set',\n",
       "  'branches',\n",
       "  'dendrites',\n",
       "  'usually a single extended process',\n",
       "  'the axon',\n",
       "  'one neuron\\n'],\n",
       " ['this connection', 'a critical element', 'the function', 'brain circuits\\n'],\n",
       " ['electricity', 'the axon', 'little packets', 'chemicals', 'the synapse\\n'],\n",
       " ['these chemicals',\n",
       "  'the small distance',\n",
       "  'the synapse',\n",
       "  'bind',\n",
       "  'certain receptors',\n",
       "  'the dendrites',\n",
       "  'the receiving cell\\n'],\n",
       " ['the release', 'the process', 'signal', 'another cell\\n'],\n",
       " ['such a process',\n",
       "  'a myriad',\n",
       "  'synapses',\n",
       "  'you',\n",
       "  'an idea',\n",
       "  'modern methods',\n",
       "  'we',\n",
       "  'the otherconsciousness',\n",
       "  'body',\n",
       "  'brain',\n",
       "  'page 18 wise minute currents',\n",
       "  'potentials',\n",
       "  'the scalp\\n'],\n",
       " ['neurophysiologists',\n",
       "  'fact',\n",
       "  'single cells',\n",
       "  'the brain',\n",
       "  'microscopic electrodes',\n",
       "  'individual neurons\\n'],\n",
       " ['a key property',\n",
       "  'synapses',\n",
       "  'they',\n",
       "  'various activities',\n",
       "  'biochemical events',\n",
       "  'their strength\\n'],\n",
       " ['these changes', 'turn', 'neuronal pathways', 'signals\\n'],\n",
       " ['patterns', 'such changes', 'synaptic strength', 'a basis', 'memory\\n'],\n",
       " ['this point', 'it', 'synapses', 'two flavors\\n'],\n",
       " ['plasticity', 'they', 'the functioning signal pathways', 'the brain\\n'],\n",
       " ['an important next step',\n",
       "  'this bowdlerized account',\n",
       "  'the overall anatomical connections',\n",
       "  'pathways',\n",
       "  'the brain',\n",
       "  'a given animal species',\n",
       "  'evolution',\n",
       "  'development\\n'],\n",
       " ['the result',\n",
       "  'a stunning set',\n",
       "  'different brain areas',\n",
       "  'cell collections\\n'],\n",
       " ['both short-range', 'long-range inputs', 'outputs\\n'],\n",
       " ['us', 'the visual pathway', 'monkeys', 'an example\\n'],\n",
       " ['striking cells',\n",
       "  'the retina',\n",
       "  'the optic nerve',\n",
       "  'whose signals',\n",
       "  'a structure',\n",
       "  'a central player',\n",
       "  'our story\\n'],\n",
       " ['the thalamus',\n",
       "  'a small structure',\n",
       "  'great importance',\n",
       "  'any account',\n",
       "  'consciousness\\n'],\n",
       " ['thalamic neurons', 'vision', 'axons', 'an area', 'the cerebral cortex\\n'],\n",
       " ['from there, all kinds',\n",
       "  'pathways consciousness',\n",
       "  'body',\n",
       "  'brain',\n",
       "  '1:55 pm',\n",
       "  'the cortex',\n",
       "  'areas',\n",
       "  'others\\n'],\n",
       " ['at least thirty-three different cortical areas',\n",
       "  'one way',\n",
       "  'the process',\n",
       "  'vision\\n'],\n",
       " ['two important facts', 'several other sensory systems\\n'],\n",
       " ['each brain area',\n",
       "  'vision',\n",
       "  'orientation',\n",
       "  'objects',\n",
       "  'color',\n",
       "  'object motion\\n'],\n",
       " ['the second fact',\n",
       "  'no one area',\n",
       "  'the responses',\n",
       "  'all the rest',\n",
       "  'a complex visual signal',\n",
       "  'a colored moving object',\n",
       "  'particular shape\\n'],\n",
       " ['we',\n",
       "  'the brain',\n",
       "  'the segregated perceptual events',\n",
       "  'such a stimulus',\n",
       "  'the retina\\n'],\n",
       " ['the net result',\n",
       "  'such coordination',\n",
       "  'perceptual categorization',\n",
       "  'the carving',\n",
       "  'the world',\n",
       "  'inputs',\n",
       "  'objects',\n",
       "  'a given animal species’ recognition\\n'],\n",
       " ['the brain', 'pattern recognition\\n'],\n",
       " ['we',\n",
       "  'sensory systems',\n",
       "  'vision',\n",
       "  'the principles',\n",
       "  'their receptors',\n",
       "  'inputs\\n'],\n",
       " ['different sensory areas',\n",
       "  'higher” areas',\n",
       "  'the cortex',\n",
       "  'the brain',\n",
       "  'itself\\n'],\n",
       " ['one set',\n",
       "  'cortical areas',\n",
       "  'motor output signals',\n",
       "  'the spinal cord',\n",
       "  'thence',\n",
       "  'our muscles',\n",
       "  'various actions',\n",
       "  'movements\\n'],\n",
       " ['the cortex',\n",
       "  'additional inputs',\n",
       "  'yields',\n",
       "  'a number',\n",
       "  'subcortical structures',\n",
       "  'the thalamus\\n'],\n",
       " ['figure',\n",
       "  'the basal ganglia',\n",
       "  'cerebellum',\n",
       "  '1:55 pm page',\n",
       "  'movement',\n",
       "  'the hippocampus',\n",
       "  'long-term memory',\n",
       "  'events',\n",
       "  'episodes',\n",
       "  'the cortex\\n'],\n",
       " ['what', 'i', 'a system', 'an electronic device', 'a computer\\n'],\n",
       " ['many scientific circles',\n",
       "  'a widespread belief',\n",
       "  'the brain',\n",
       "  'a computer\\n']]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [x.split() for x in open('sents_norm.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(sents, size=400, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_1 = gensim.models.Word2Vec(sents, size=400, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_2 = gensim.models.Word2Vec(sents, size=500, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_3 = gensim.models.Word2Vec(sents, size=600, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.fasttext.FastText(sents, size=200, min_n=3, max_n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gensim.models.fasttext.FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nominative', 0.876392126083374),\n",
       " ('dative', 0.7972114086151123),\n",
       " ('absolutive', 0.7938792109489441),\n",
       " ('genitive', 0.7907388210296631),\n",
       " ('ergative', 0.7671347260475159),\n",
       " ('oblique', 0.7181569337844849),\n",
       " ('ablative', 0.7101474404335022),\n",
       " ('lative', 0.6539338231086731),\n",
       " ('instrumental', 0.6520599722862244),\n",
       " ('comitative', 0.6512896418571472),\n",
       " ('allative', 0.6487379670143127),\n",
       " ('benefactive', 0.6452040672302246),\n",
       " ('locative', 0.6448217630386353),\n",
       " ('obviative', 0.6369820237159729),\n",
       " ('postposition', 0.6352605223655701),\n",
       " ('vocative', 0.6351326107978821),\n",
       " ('unmark', 0.6299031376838684),\n",
       " ('unmarked', 0.6291846632957458),\n",
       " ('case’', 0.6261202096939087),\n",
       " ('-m', 0.6224660277366638)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('accusative', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nominative', 0.8737972974777222),\n",
       " ('dative', 0.8077637553215027),\n",
       " ('absolutive', 0.7807317972183228),\n",
       " ('genitive', 0.7784498333930969),\n",
       " ('ergative', 0.7567505240440369),\n",
       " ('oblique', 0.740955650806427),\n",
       " ('ablative', 0.6905251741409302),\n",
       " ('unmark', 0.6629302501678467),\n",
       " ('locative', 0.6494957208633423),\n",
       " ('marking', 0.64488685131073)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_1.most_similar('accusative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('nominative', 0.8805449604988098),\n",
       " ('dative', 0.7963292002677917),\n",
       " ('genitive', 0.7926821112632751),\n",
       " ('absolutive', 0.7780560851097107),\n",
       " ('ergative', 0.7724252343177795),\n",
       " ('oblique', 0.7572967410087585),\n",
       " ('ablative', 0.6995598077774048),\n",
       " ('comitative', 0.6681219339370728),\n",
       " ('allative', 0.6622324585914612),\n",
       " ('instrumental', 0.6574176549911499),\n",
       " ('case’', 0.650806188583374),\n",
       " ('unmark', 0.6372339725494385),\n",
       " ('antipassive', 0.6340838074684143),\n",
       " ('locative', 0.6323443651199341),\n",
       " ('marking', 0.6321450471878052),\n",
       " ('benefactive', 0.630612850189209),\n",
       " ('-m', 0.6254040598869324),\n",
       " ('ofnominative', 0.6181294918060303),\n",
       " ('vocative', 0.6134778261184692),\n",
       " ('postposition', 0.6074748635292053)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_2.most_similar('accusative', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-ro', 0.8631316423416138),\n",
       " ('ita', 0.8414170145988464),\n",
       " ('-tu', 0.8384491205215454),\n",
       " ('ι', 0.8383947014808655),\n",
       " ('emo', 0.8338407874107361),\n",
       " ('p3sg', 0.8324854969978333),\n",
       " ('-je', 0.8286639451980591),\n",
       " ('no=', 0.8277689814567566),\n",
       " ('fal', 0.8269727826118469),\n",
       " ('tºi', 0.8266384601593018),\n",
       " ('dy', 0.8252228498458862),\n",
       " ('-man', 0.8245456218719482),\n",
       " ('o-', 0.8240092992782593),\n",
       " ('gef', 0.8220754861831665),\n",
       " ('tə', 0.8220458626747131),\n",
       " ('eti', 0.82012540102005),\n",
       " ('ké', 0.8194481134414673),\n",
       " ('gv', 0.816381573677063),\n",
       " ('há', 0.8160685896873474),\n",
       " ('oand', 0.8147521615028381)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('sis', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('−accusative', 0.986430823802948),\n",
       " ('nominativeaccusative', 0.9578328132629395),\n",
       " ('unaccusative', 0.9475518465042114),\n",
       " ('accusative’', 0.9464986324310303),\n",
       " ('accusatif', 0.9159835577011108),\n",
       " ('acusativo', 0.9132360219955444),\n",
       " ('dative', 0.9010818004608154),\n",
       " ('unaccusative’', 0.8966334462165833),\n",
       " ('accusatively', 0.888617217540741),\n",
       " ('ergativeabsolutive', 0.8863722681999207),\n",
       " ('ergative', 0.8788683414459229),\n",
       " ('nominative', 0.8772831559181213),\n",
       " ('unergative', 0.8729730844497681),\n",
       " ('ofnominative', 0.8710689544677734),\n",
       " ('ofdative', 0.8701499104499817),\n",
       " ('unaccusatives', 0.8688222765922546),\n",
       " ('genitive', 0.865285336971283),\n",
       " ('akkusativ', 0.8575778007507324),\n",
       " ('accusativity', 0.8533416390419006),\n",
       " ('nominative’', 0.8457307815551758)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.most_similar('accusative', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft2 = gensim.models.fasttext.FastText(sents, size=400, window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('\\x01case', 0.9412144422531128),\n",
       " ('•\\x01case', 0.9404243230819702),\n",
       " ('þcase', 0.9396320581436157),\n",
       " ('^case', 0.9392638206481934),\n",
       " ('⟨{case', 0.9388969540596008),\n",
       " ('〈{case', 0.9380385279655457),\n",
       " ('k[ucase', 0.9280897974967957),\n",
       " ('case2', 0.9276347756385803),\n",
       " ('ucase', 0.9266526699066162),\n",
       " (\"case'\", 0.9232484698295593),\n",
       " ('case1', 0.9217522144317627),\n",
       " ('casep', 0.9107418656349182),\n",
       " ('ofcase', 0.9086319804191589),\n",
       " ('caseg', 0.9021592736244202),\n",
       " ('caseagr', 0.8774917125701904),\n",
       " ('cases’', 0.8653406500816345),\n",
       " ('suitcase', 0.838678240776062),\n",
       " ('casey', 0.8356001973152161),\n",
       " ('subcase', 0.8287577629089355),\n",
       " ('casert', 0.8243017792701721)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft2.most_similar('case', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Counter()\n",
    "for line in open('phrases.txt'):\n",
    "    ph = line.strip('\\n').split('\\t')\n",
    "    phrases.update(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = set()\n",
    "for phrase in phrases:\n",
    "    if phrases[phrase] > 10 and phrase not in stops:\n",
    "        filtered.add(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_3.save('w2v_600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load('w2v_600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered = list(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((41627, 200))\n",
    "for i, phrase in enumerate(ordered):\n",
    "    norm = nlp(phrase)\n",
    "    words = [w.lemma_ for w in norm if not w.is_stop and not w.is_punct]\n",
    "    if words:\n",
    "        try:\n",
    "            v = ft[' '.join(words)]\n",
    "        except KeyError:\n",
    "            continue\n",
    "#         if v:\n",
    "#             v = np.mean(v, axis=0)\n",
    "        X[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([w2v['l'], w2v['s'], w2v['x']], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = MiniBatchKMeans(3000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=3000, n_init=3, random_state=None,\n",
       "        reassignment_ratio=0.01, tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.47148389,  0.93895769, -0.9723177 , ..., -3.79149342,\n",
       "         1.52883148, -0.75833899],\n",
       "       [ 2.56352687, -1.2912581 ,  0.14725791, ...,  0.43182001,\n",
       "         0.52801687,  0.60488158],\n",
       "       ...,\n",
       "       [-1.6909157 ,  0.15794948,  0.32625058, ..., -0.44088709,\n",
       "         1.10764575, -2.94832015],\n",
       "       [ 0.26946443,  0.0091242 ,  0.73126578, ..., -0.13193665,\n",
       "         0.62722892, -0.29373828],\n",
       "       [-0.01878791,  1.073524  ,  0.53372538, ..., -1.30288756,\n",
       "         1.27855051, -1.04604042]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1185,  339,   73, ...,  433,  882, 1927], dtype=int32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jared s. klein', 'matthew s. dryer', 's. goldin-meadow']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_dict[1185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_dict = defaultdict(list)\n",
    "for i, phrase in enumerate(ordered):\n",
    "    lb_dict[labels[i]].append(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clusters.txt', 'w')\n",
    "for cl in lb_dict:\n",
    "    if cl != 449:\n",
    "        f.write(str(cl) + '\\n')\n",
    "    f.write('\\t'.join(lb_dict[cl]))\n",
    "    f.write('\\n\\n\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm[0].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open('opendata/all_dois_20180122T165326.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1016/0167-4781(85)90058-2,f,2,,,,,,,,Normal rate of DNA breakage in xeroderma pigmentosum complementation group E cells treated with 8-methoxypsoralen plus near-ultraviolet radiation,0167-4781,Biochimica et Biophysica Acta (BBA) - Gene Structure and Expression,f,Elsevier BV,1985,journal-article,2018-01-21 07:37:13.422973\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('opened.csv', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('opened.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields, delimiter=',', quotechar='\"')\n",
    "    writer.writeheader()\n",
    "\n",
    "\n",
    "    with open('opendata/all_dois_20180122T165326.csv', newline='\\n') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for line in reader:\n",
    "            if line['is_oa'] == 't':\n",
    "                writer.writerow(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(x):\n",
    "    keys = ['language', 'pronunciation', 'linguist', 'syntact', 'syntax', 'morpholog', 'semantic',\n",
    "           'clitic', 'gramma', 'phonolog', 'lexi', 'phonetic', 'translation', 'discourse',\n",
    "           'verb', 'noun', 'adjective', 'adverb', 'pronoun', 'lingua', 'broca', 'wernick',\n",
    "           'pronominal', 'speech', 'creol', 'syllab', 'prosod', 'aphasia', 'spelling', 'text',\n",
    "           'predicat', 'spoken', 'communic', 'metaphor', 'consonant', 'vowel', 'phonem', 'chomsky',\n",
    "           'word', 'vocabular', 'accusativ', 'dative', 'genetiv', 'phrase', 'preposition', 'dialect',\n",
    "           'corpus', 'corpora']\n",
    "    x = x.lower()\n",
    "    for key in keys:\n",
    "        if key in x:\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "with open('ling_possible.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields, delimiter=',', quotechar='\"')\n",
    "    writer.writeheader()\n",
    "    with open('opened.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "\n",
    "        for line in reader:\n",
    "            if check(line['title']):\n",
    "                writer.writerow(line)\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import grequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded -  1000\n",
      "Downloaded -  2000\n",
      "Downloaded -  3000\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "errors = []\n",
    "downloaded = set([x.replace('#', '/').split('.pdf')[0] for x in os.listdir('opendata/linguistic')])\n",
    "\n",
    "with open('ling_possible.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',', quotechar='\"', fieldnames=fields)\n",
    "    reqs2sent = []\n",
    "    for jj, line in enumerate(reader):\n",
    "        if jj < j:\n",
    "            continue\n",
    "        if line['doi'] in downloaded:\n",
    "#             print('skipping')\n",
    "            continue\n",
    "        \n",
    "        if not 'lingu' in line['title'] and not 'lingu' in line['journal_name']:\n",
    "            continue\n",
    "        url = line['best_oa_location_url_for_pdf']\n",
    "        if url and url not in reqs:\n",
    "#             if len(reqs2sent) > 100:\n",
    "#                 rs = (grequests.get(u, timeout=1) for u in reqs2sent)\n",
    "#                 rs = grequests.map(rs)\n",
    "#                 reqs2sent = []\n",
    "#             else:\n",
    "#                 reqs2sent.append(url)\n",
    "#                 reqs.add(url)\n",
    "#                 continue\n",
    "            \n",
    "#             for r in rs:\n",
    "#                 if not r:\n",
    "#                     continue\n",
    "            try:\n",
    "                r = requests.get(url, timeout=5)\n",
    "                reqs.add(url)\n",
    "            except Exception as e:\n",
    "                errors.append((url, e))\n",
    "#                 print(e)\n",
    "                continue\n",
    "            typ = r.headers.get('content-type')\n",
    "            if not typ:\n",
    "                continue\n",
    "\n",
    "            if r.status_code == 200 and 'application/pdf' in typ:\n",
    "                nf = open('opendata/linguistic//{}.pdf'.format(line['doi'].replace('/', '#')), 'wb')\n",
    "                nf.write(r.content)\n",
    "                nf.close()\n",
    "                downloaded.add(line['doi'])\n",
    "                i += 1\n",
    "                if not i % 1000:\n",
    "                    print('Downloaded - ', i)\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = (grequests.get(u, timeout=1) for u in reqs2sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrs = grequests.map(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [503]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [404]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [403]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [503]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [404]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [404]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [404]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [404]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " <Response [200]>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Response [200]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [403]>,\n",
       " <Response [404]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " <Response [404]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [404]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [404]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [403]>,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " <Response [403]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " <Response [200]>,\n",
       " None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "with open('ling_possible.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if row['doi'] in downloaded:\n",
    "            j = i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20205"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text/html;charset=UTF-8'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.headers.get('content-type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10.1002/9781118584194',\n",
       " '10.1002/aqc.2675',\n",
       " '10.1002/ejsp.240',\n",
       " '10.1002/humu.10023',\n",
       " '10.1002/neu.20096',\n",
       " '10.1002/yea.1503',\n",
       " '10.1007/11669487_3',\n",
       " '10.1007/11761679_34',\n",
       " '10.1007/11914952_47',\n",
       " '10.1007/978-3-319-02762-3_13',\n",
       " '10.1007/978-3-319-19662-6_15',\n",
       " '10.1007/978-3-319-21365-1_28',\n",
       " '10.1007/978-3-319-26647-3_133',\n",
       " '10.1007/978-3-476-03163-1_2',\n",
       " '10.1007/978-3-540-30213-1_15',\n",
       " '10.1007/978-3-540-30557-6_9',\n",
       " '10.1007/978-3-540-45209-6_144',\n",
       " '10.1007/978-3-540-72794-1_4',\n",
       " '10.1007/978-3-540-74456-6_50',\n",
       " '10.1007/978-3-540-78135-6_30',\n",
       " '10.1007/978-3-642-11284-3_7',\n",
       " '10.1007/978-3-642-20000-7_7',\n",
       " '10.1007/978-3-642-24455-1_2',\n",
       " '10.1007/978-3-642-40395-8_15',\n",
       " '10.1007/978-3-642-41335-3_37',\n",
       " '10.1007/978-3-662-49674-9_50',\n",
       " '10.1007/bf00170207',\n",
       " '10.1007/bf03005076',\n",
       " '10.1007/bf03168750',\n",
       " '10.1007/bf03168757',\n",
       " '10.1007/bfb0021144',\n",
       " '10.1007/s00125-006-0212-9',\n",
       " '10.1007/s10344-015-0927-3',\n",
       " '10.1007/s10681-015-1454-8',\n",
       " '10.1007/s11191-004-5157-0',\n",
       " '10.1016/j.anbehav.2015.09.010',\n",
       " '10.1016/j.nuclphysb.2007.05.025',\n",
       " '10.1016/j.pathophys.2014.05.003',\n",
       " '10.1016/j.specom.2007.06.004',\n",
       " '10.1016/j.stam.2005.08.007',\n",
       " '10.1016/j.susc.2007.03.006',\n",
       " '10.1016/j.tice.2004.03.003',\n",
       " '10.1016/s1071-5819(03)00088-0',\n",
       " '10.1017/s0007114507787457',\n",
       " '10.1017/s0007680517000393',\n",
       " '10.1017/s0020859000114257',\n",
       " '10.1017/s002211200600317x',\n",
       " '10.1017/s026114301500063x',\n",
       " '10.1017/s0958344009000214',\n",
       " '10.1017/s1355617711001251',\n",
       " '10.1017/s136672891100023x',\n",
       " '10.1017/s1366728916000997',\n",
       " '10.1021/ci0000676',\n",
       " '10.1021/jp0663917',\n",
       " '10.1021/ma9907587',\n",
       " '10.1037/0096-1523.26.6.1797',\n",
       " '10.1037/0893-164x.20.3.348',\n",
       " '10.1037/a0023195',\n",
       " '10.1037/a0025194',\n",
       " '10.1037/e538182010-001',\n",
       " '10.1037/h0022613',\n",
       " '10.1037/h0060856',\n",
       " '10.1038/jcbfm.2012.51',\n",
       " '10.1038/modpathol.3800299',\n",
       " '10.1038/ncomms10489',\n",
       " '10.1038/ncomms4137',\n",
       " '10.1038/ncomms9813',\n",
       " '10.1038/srep22191',\n",
       " '10.1038/srep26171',\n",
       " '10.1038/srep26434',\n",
       " '10.1038/srep27446',\n",
       " '10.1038/srep45287',\n",
       " '10.1042/an20090048',\n",
       " '10.1044/2016_jslhr-h-15-0371',\n",
       " '10.1049/iet-cps.2017.0051',\n",
       " '10.1051/0004-6361/200810836',\n",
       " '10.1051/0004-6361/200913425',\n",
       " '10.1051/0004-6361/201220998',\n",
       " '10.1051/0004-6361/201630069',\n",
       " '10.1051/jphyscol:19955135',\n",
       " '10.1051/matecconf/20130701017',\n",
       " '10.1051/matecconf/20152201034',\n",
       " '10.1051/matecconf/20153001012',\n",
       " '10.1051/matecconf/20165102005',\n",
       " '10.1051/matecconf/20165601001',\n",
       " '10.1051/matecconf/20165605011',\n",
       " '10.1051/matecconf/20166305018',\n",
       " '10.1051/matecconf/20167901048',\n",
       " '10.1051/medsci/2011273331',\n",
       " '10.1051/medsci/20163201010',\n",
       " '10.1051/mmnp/20116607',\n",
       " '10.1051/parasite/2011181003',\n",
       " '10.1051/pmed:2006020',\n",
       " '10.1051/pmed:2008019',\n",
       " '10.1051/radiopro/2009005',\n",
       " '10.1051/shsconf/20140801071',\n",
       " '10.1051/shsconf/20151702012',\n",
       " '10.1051/shsconf/20152001003',\n",
       " '10.1051/shsconf/20162402006',\n",
       " '10.1051/shsconf/20162712015',\n",
       " '10.1051/shsconf/20162715003',\n",
       " '10.1055/s-2007-985879',\n",
       " '10.1073/pnas.061032198',\n",
       " '10.1073/pnas.1308091110',\n",
       " '10.1073/pnas.1314851111',\n",
       " '10.1073/pnas.84.8.2277',\n",
       " '10.1073/pnas.96.26.15127',\n",
       " '10.1074/jbc.m111.232439',\n",
       " '10.1074/jbc.m402562200',\n",
       " '10.1074/jbc.m610643200',\n",
       " '10.1075/eurosla.7.11hul',\n",
       " '10.1080/00437956.1953.11659463',\n",
       " '10.1080/01587910903236544',\n",
       " '10.1080/02699200021000034958',\n",
       " '10.1080/08838151003737931',\n",
       " '10.1080/10926488.2014.924303',\n",
       " '10.1080/12538078.2003.10515995',\n",
       " '10.1080/13518470903314394',\n",
       " '10.1080/15389588.2013.818350',\n",
       " '10.1080/16823200903234901',\n",
       " '10.1080/17405900500283706',\n",
       " '10.1080/18756891.2015.1017377',\n",
       " '10.1080/19376529.2015.1015860',\n",
       " '10.1080/19420676.2013.777357',\n",
       " '10.1080/1943815x.2010.506487',\n",
       " '10.1080/20445911.2014.976226',\n",
       " '10.1080/20786204.2014.10844582',\n",
       " '10.1080/21645515.2015.1100912',\n",
       " '10.1080/21663831.2014.944676',\n",
       " '10.1080/87565649709540681',\n",
       " '10.1083/jcb.103.1.255',\n",
       " '10.1083/jcb.105.6.2803',\n",
       " '10.1083/jcb.141.4.887',\n",
       " '10.1083/jcb.143.4.957',\n",
       " '10.1084/jem.190.8.1081',\n",
       " '10.1088/0004-637x/785/2/140',\n",
       " '10.1088/0256-307x/26/2/028902',\n",
       " '10.1097/00000441-193806000-00023',\n",
       " '10.1097/aud.0b013e3182498c28',\n",
       " '10.1104/pp.70.2.610',\n",
       " '10.1109/acw.2003.1210206',\n",
       " '10.1109/date.2012.6176633',\n",
       " '10.1109/icra.2015.7139899',\n",
       " '10.1109/ipdps.2013.80',\n",
       " '10.1109/ipps.1994.288208',\n",
       " '10.1109/ivs.2010.5547954',\n",
       " '10.1109/mm.2009.6',\n",
       " '10.1109/slt.2008.4777841',\n",
       " '10.1109/slt.2008.4777889',\n",
       " '10.1109/tasl.2008.916519',\n",
       " '10.1109/tit.2010.2046251',\n",
       " '10.1111/j.1365-3059.2007.01740.x',\n",
       " '10.1111/j.1467-968x.2010.01246.x',\n",
       " '10.1111/jace.12023',\n",
       " '10.1111/pedi.12036',\n",
       " '10.1111/synt.12117',\n",
       " '10.1112/jlms/jdm070',\n",
       " '10.1112/plms/pdp031',\n",
       " '10.1121/1.1970571',\n",
       " '10.1124/mol.107.044396',\n",
       " '10.1128/jb.171.10.5322-5324.1989',\n",
       " '10.1128/mcb.06154-11',\n",
       " '10.1136/gut.40.4.561-c',\n",
       " '10.1136/oem.25.4.293',\n",
       " '10.1136/thx.2009.126730',\n",
       " '10.1137/110834512',\n",
       " '10.1145/1873951.1874215',\n",
       " '10.1145/2492002.2482544',\n",
       " '10.1145/2535838.2535883',\n",
       " '10.1145/2643164.2643172',\n",
       " '10.1145/2714064.2660201',\n",
       " '10.1145/2724704',\n",
       " '10.1145/2740908.2743003',\n",
       " '10.1161/circep.114.002453',\n",
       " '10.1161/circinterventions.108.847137',\n",
       " '10.1161/circulationaha.108.808543',\n",
       " '10.1161/circulationaha.110.987875',\n",
       " '10.1163/156855306776562251',\n",
       " '10.1177/0957926510373988',\n",
       " '10.1177/1084713810393751',\n",
       " '10.1177/1461445611421360',\n",
       " '10.1177/2158244016669552',\n",
       " '10.1177/9.2.176',\n",
       " '10.1186/1471-2296-14-40',\n",
       " '10.1186/1471-230x-14-12',\n",
       " '10.1186/1471-244x-8-3',\n",
       " '10.1186/1687-6180-2012-62',\n",
       " '10.1186/2041-1480-5-26',\n",
       " '10.1186/s12909-016-0818-7',\n",
       " '10.1186/s13660-016-0979-2',\n",
       " '10.1186/s40064-016-3235-9',\n",
       " '10.1186/s40555-015-0107-x',\n",
       " '10.1242/jeb.158600',\n",
       " '10.1257/jep.2.4.135',\n",
       " '10.1348/135910709x476972',\n",
       " '10.1353/sub.2007.0045',\n",
       " '10.1364/ome.3.000584',\n",
       " '10.14716/ijtech.v6i2.456',\n",
       " '10.14746/gl.2013.40.2.7',\n",
       " '10.1484/m.tcc-eb.3.3890',\n",
       " '10.1515/ci.2011.33.6.35a',\n",
       " '10.1515/lp-2013-0006',\n",
       " '10.1515/opth-2016-0039',\n",
       " '10.1525/aa.1963.65.3.02a00620',\n",
       " '10.15359/mhs.12-2.1',\n",
       " '10.1542/peds.2011-3760',\n",
       " '10.1587/elex.7.615',\n",
       " '10.1587/transinf.2016edp7357',\n",
       " '10.1590/s0100-204x2012000500001',\n",
       " '10.1590/s0102-86502014001700005',\n",
       " '10.1590/s0104-07072012000200004',\n",
       " '10.1590/s1980-85852014000100002',\n",
       " '10.17576/gema-2014-1401-04',\n",
       " '10.18007/gisap:ps.v0i7.939',\n",
       " '10.2134/agronj1988.00021962008000030001x',\n",
       " '10.2139/ssrn.338382',\n",
       " '10.2172/929198',\n",
       " '10.22495/cocv2i2p7',\n",
       " '10.2307/3311837',\n",
       " '10.2320/matertrans1960.26.115',\n",
       " '10.2320/matertrans1960.29.962',\n",
       " '10.2323/jgam.47.99',\n",
       " '10.2355/isijinternational.45.91',\n",
       " '10.25011/cim.v31i4.4778',\n",
       " '10.2979/jfolkrese.53.2.04',\n",
       " '10.3109/02841869109091838',\n",
       " '10.3109/17453674.2014.958808',\n",
       " '10.3109/17453679608994676',\n",
       " '10.3115/991146.991205',\n",
       " '10.3115/v1/p14-1054',\n",
       " '10.3115/v1/w14-2908',\n",
       " '10.3354/ab00234',\n",
       " '10.3354/meps07485',\n",
       " '10.3366/ccs.2005.2.1.23',\n",
       " '10.3389/fpsyt.2016.00103',\n",
       " '10.3390/molecules16053618',\n",
       " '10.3844/ajassp.2012.1166.1181',\n",
       " '10.4161/cc.7.16.6464',\n",
       " '10.5120/395-589',\n",
       " '10.5194/isprsarchives-xl-4-w5-67-2015',\n",
       " '10.5209/rev_cjes.2015.v23.51214',\n",
       " '10.5220/0002297801710176',\n",
       " '10.5271/sjweh.3362',\n",
       " '10.5511/plantbiotechnology.15.1103a',\n",
       " '10.5586/asbp.2015.001',\n",
       " '10.5604/12321966.1196877',\n",
       " '10.5772/54379',\n",
       " '10.5935/1678-9741.20150052',\n",
       " '10.7146/hjlcb.v27i54.22946',\n",
       " '10.7150/ijms.4440',\n",
       " '10.7202/004024ar',\n",
       " '10.7202/018089ar',\n",
       " '10.7202/1003515ar',\n",
       " '10.7314/apjcp.2013.14.9.5123',\n",
       " '10.7551/978-0-262-31050-5-ch022'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['doi', 'is_oa', 'data_standard', 'best_oa_location_url', \n",
    "          'best_oa_location_url_for_landing_page', 'best_oa_location_url_for_pdf',\n",
    "          'best_oa_location_host_type', 'best_oa_location_version',\n",
    "          'best_oa_location_license', 'best_oa_location_evidence', 'title',\n",
    "          'journal_issns', 'journal_name', 'journal_is_oa', 'publisher',\n",
    "          'year', 'genre', 'updated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = csv.DictReader(open('opendata/all_dois_20180122T165326.csv'), quotechar='\"', fieldnames=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_oa_location_version': 'best_oa_location_version', 'updated': 'updated', 'data_standard': 'data_standard', 'journal_name': 'journal_name', 'best_oa_location_evidence': 'best_oa_location_evidence', 'journal_issns': 'journal_issns', 'doi': 'doi', 'title': 'title', 'best_oa_location_host_type': 'best_oa_location_host_type', 'publisher': 'publisher', 'journal_is_oa': 'journal_is_oa', 'best_oa_location_url_for_pdf': 'best_oa_location_url_for_pdf', 'year': 'year', 'best_oa_location_license': 'best_oa_location_license', 'is_oa': 'is_oa', 'best_oa_location_url_for_landing_page': 'best_oa_location_url_for_landing_page', 'genre': 'genre', 'best_oa_location_url': 'best_oa_location_url'}\n"
     ]
    }
   ],
   "source": [
    "for line in r:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_oa_location_evidence': '',\n",
       " 'best_oa_location_host_type': '',\n",
       " 'best_oa_location_license': '',\n",
       " 'best_oa_location_url': '',\n",
       " 'best_oa_location_url_for_landing_page': '',\n",
       " 'best_oa_location_url_for_pdf': '',\n",
       " 'best_oa_location_version': '',\n",
       " 'data_standard': '1',\n",
       " 'doi': '10.4271/831455',\n",
       " 'genre': 'proceedings-article',\n",
       " 'is_oa': 'f',\n",
       " 'journal_is_oa': 'f',\n",
       " 'journal_issns': '',\n",
       " 'journal_name': 'SAE Technical Paper Series',\n",
       " 'publisher': 'SAE International',\n",
       " 'title': 'Efforts to Assure Anthropometric Cockpit Compatibility in Navy Aircraft',\n",
       " 'updated': '2018-01-21 21:50:08.440155',\n",
       " 'year': '1983'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doi',\n",
       " 'is_oa',\n",
       " 'data_standard',\n",
       " 'best_oa_location_url',\n",
       " 'best_oa_location_url_for_landing_page',\n",
       " 'best_oa_location_url_for_pdf',\n",
       " 'best_oa_location_host_type',\n",
       " 'best_oa_location_version',\n",
       " 'best_oa_location_license',\n",
       " 'best_oa_location_evidence',\n",
       " 'title',\n",
       " 'journal_issns',\n",
       " 'journal_name',\n",
       " 'journal_is_oa',\n",
       " 'publisher',\n",
       " 'year',\n",
       " 'genre',\n",
       " 'updated\\n']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a.readline().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1017/s027226310606027x,f,2,,,,,,,,REEXAMINING THE ROLE OF RECASTS IN SECOND LANGUAGE ACQUISITION,\"0272-2631,1470-1545\",Studies in Second Language Acquisition,f,Cambridge University Press (CUP),2006,journal-article,2017-12-24 02:50:14.592608\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1007/978-3-319-32489-0_2,f,2,,,,,,,,Rotational and Translational Diffusion in Ionic Liquids,\"2190-930X,2190-9318\",Dielectric Properties of Ionic Liquids,f,Springer International Publishing,2016,book-chapter,2018-01-18 07:23:44.610323\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('opened.csv', 'w')\n",
    "for line in a:\n",
    "    l = line.split(',')\n",
    "    if len(l) < 2:\n",
    "        continue\n",
    "    if l[1] == 't':\n",
    "        f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Internacional de Literatura i Cultura Medieval i\\n'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('batch.txt', 'w')\n",
    "full = open('opened.csv')\n",
    "\n",
    "for i in range(1000000):\n",
    "    f.write(full.readline())\n",
    "f.close()\n",
    "full.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = Counter()\n",
    "for line in open('opened.csv'):\n",
    "    j = line.split(',')\n",
    "    if len(j) < 18:\n",
    "        continue\n",
    "    if j[12]:\n",
    "        journals.update([j[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718541"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(journals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('journals.txt', 'w')\n",
    "for j, c in journals.most_common():\n",
    "    f.write('\\t'.join([j,str(c)])+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for line in open('batch.txt'):\n",
    "    t = line.split(',')\n",
    "    if len(t) < 18:\n",
    "        continue\n",
    "    if t[10]:\n",
    "        texts.append([x for x in word_tokenize(t[10].lower()) if x.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(64995 unique tokens: ['integrando', 'hadoop', 'azide', 'interlocked', 'phaeophyceae']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, keep_n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel([dictionary.doc2bow(x) for x in texts], id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [dictionary.doc2bow(x) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b57e4d1a728f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    745\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     )\n\u001b[0;32m--> 747\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m# Initialize the variational distribution q(theta|gamma) for the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mElogtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mexpElogtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElogtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36mdirichlet_expectation\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keep the same precision as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lda = gensim.models.LdaModel(tfidf[bow], 200, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/models/ldamodel.py:802: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda = gensim.models.LdaMulticore(tfidf[bow], 200, id2word=dictionary, iterations=20, chunksize=2000, workers=3, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"fibrosis\" + 0.046*\"matter\" + 0.045*\"world\" + 0.045*\"white\" + 0.043*\"order\" + 0.040*\"material\" + 0.036*\"near\" + 0.033*\"extract\" + 0.032*\"cystic\" + 0.032*\"transporter\"'),\n",
       " (1,\n",
       "  '0.061*\"events\" + 0.055*\"behaviour\" + 0.048*\"architecture\" + 0.046*\"extracellular\" + 0.045*\"dendritic\" + 0.042*\"law\" + 0.034*\"title\" + 0.033*\"promote\" + 0.031*\"mobile\" + 0.030*\"ischemia\"'),\n",
       " (2,\n",
       "  '0.053*\"modified\" + 0.051*\"adolescents\" + 0.046*\"school\" + 0.041*\"associations\" + 0.040*\"children\" + 0.031*\"video\" + 0.030*\"initiation\" + 0.021*\"dengue\" + 0.019*\"among\" + 0.017*\"and\"'),\n",
       " (3,\n",
       "  '0.065*\"advanced\" + 0.059*\"series\" + 0.030*\"co\" + 0.028*\"requirements\" + 0.028*\"node\" + 0.027*\"speed\" + 0.026*\"brasil\" + 0.025*\"focal\" + 0.022*\"lymph\" + 0.021*\"silencing\"'),\n",
       " (4,\n",
       "  '0.072*\"knowledge\" + 0.046*\"comment\" + 0.044*\"evolutionary\" + 0.043*\"elements\" + 0.038*\"robust\" + 0.031*\"basic\" + 0.030*\"set\" + 0.028*\"functional\" + 0.028*\"degeneration\" + 0.021*\"anomalous\"'),\n",
       " (5,\n",
       "  '0.106*\"networks\" + 0.046*\"sensor\" + 0.038*\"wireless\" + 0.037*\"fusion\" + 0.031*\"force\" + 0.031*\"monitoring\" + 0.023*\"areas\" + 0.022*\"flow\" + 0.021*\"all\" + 0.021*\"scheduling\"'),\n",
       " (6,\n",
       "  '0.049*\"malaria\" + 0.037*\"intravenous\" + 0.036*\"activator\" + 0.031*\"plasmodium\" + 0.025*\"plasminogen\" + 0.024*\"division\" + 0.022*\"falciparum\" + 0.021*\"orbital\" + 0.021*\"decline\" + 0.019*\"medial\"'),\n",
       " (7,\n",
       "  '0.059*\"thyroid\" + 0.052*\"der\" + 0.040*\"die\" + 0.036*\"pathology\" + 0.031*\"ph\" + 0.027*\"thoracic\" + 0.021*\"anisotropic\" + 0.019*\"finding\" + 0.017*\"papillary\" + 0.017*\"hereditary\"'),\n",
       " (8,\n",
       "  '0.058*\"optimal\" + 0.053*\"solar\" + 0.043*\"challenges\" + 0.041*\"waves\" + 0.039*\"films\" + 0.035*\"mental\" + 0.035*\"thin\" + 0.032*\"staphylococcus\" + 0.031*\"aureus\" + 0.028*\"silicon\"'),\n",
       " (9,\n",
       "  '0.070*\"reply\" + 0.048*\"solution\" + 0.039*\"insights\" + 0.039*\"differentiation\" + 0.039*\"leukemia\" + 0.033*\"causes\" + 0.031*\"myeloid\" + 0.030*\"into\" + 0.027*\"decision\" + 0.026*\"making\"'),\n",
       " (10,\n",
       "  '0.151*\"de\" + 0.095*\"e\" + 0.070*\"da\" + 0.063*\"do\" + 0.062*\"em\" + 0.044*\"o\" + 0.038*\"no\" + 0.036*\"na\" + 0.025*\"para\" + 0.023*\"com\"'),\n",
       " (11,\n",
       "  '0.039*\"involving\" + 0.034*\"robot\" + 0.033*\"hospitals\" + 0.032*\"atypical\" + 0.030*\"clostridium\" + 0.028*\"inner\" + 0.021*\"non\" + 0.020*\"planetary\" + 0.018*\"diarrhea\" + 0.018*\"predicted\"'),\n",
       " (12,\n",
       "  '0.080*\"simulation\" + 0.051*\"hybrid\" + 0.047*\"cervical\" + 0.044*\"discovery\" + 0.041*\"electronic\" + 0.037*\"sample\" + 0.035*\"artificial\" + 0.025*\"gold\" + 0.022*\"periodic\" + 0.018*\"monte\"'),\n",
       " (13,\n",
       "  '0.052*\"genomic\" + 0.044*\"modulates\" + 0.036*\"measure\" + 0.035*\"parkinson\" + 0.034*\"irradiation\" + 0.030*\"setting\" + 0.029*\"task\" + 0.027*\"affecting\" + 0.027*\"lipoprotein\" + 0.027*\"ecological\"'),\n",
       " (14,\n",
       "  '0.098*\"body\" + 0.081*\"insulin\" + 0.066*\"deficiency\" + 0.056*\"it\" + 0.051*\"morphology\" + 0.037*\"observed\" + 0.030*\"angiogenesis\" + 0.016*\"defective\" + 0.016*\"divergence\" + 0.016*\"index\"'),\n",
       " (15,\n",
       "  '0.045*\"fields\" + 0.045*\"enhances\" + 0.041*\"neuronal\" + 0.037*\"vector\" + 0.033*\"phenotype\" + 0.033*\"protective\" + 0.027*\"nutrition\" + 0.026*\"platelet\" + 0.024*\"inhibition\" + 0.024*\"infant\"'),\n",
       " (16,\n",
       "  '0.088*\"d\" + 0.055*\"vitamin\" + 0.048*\"derived\" + 0.042*\"substrate\" + 0.041*\"first\" + 0.033*\"regulate\" + 0.033*\"within\" + 0.027*\"principles\" + 0.020*\"from\" + 0.017*\"forces\"'),\n",
       " (17,\n",
       "  '0.092*\"mechanisms\" + 0.076*\"tuberculosis\" + 0.066*\"inhibits\" + 0.048*\"deep\" + 0.042*\"replacement\" + 0.032*\"mycobacterium\" + 0.031*\"operators\" + 0.026*\"salt\" + 0.025*\"thrombosis\" + 0.021*\"adverse\"'),\n",
       " (18,\n",
       "  '0.068*\"coli\" + 0.057*\"escherichia\" + 0.053*\"ovarian\" + 0.032*\"transgenic\" + 0.028*\"analytical\" + 0.026*\"visualization\" + 0.023*\"seed\" + 0.021*\"genes\" + 0.016*\"glutathione\" + 0.016*\"alignment\"'),\n",
       " (19,\n",
       "  '0.060*\"nanoparticles\" + 0.056*\"integration\" + 0.056*\"increases\" + 0.035*\"endogenous\" + 0.034*\"ligand\" + 0.034*\"location\" + 0.029*\"hypoxia\" + 0.023*\"lumbar\" + 0.017*\"deprivation\" + 0.016*\"distress\"'),\n",
       " (20,\n",
       "  '0.074*\"states\" + 0.054*\"recognition\" + 0.042*\"content\" + 0.040*\"united\" + 0.037*\"images\" + 0.028*\"dependent\" + 0.024*\"ct\" + 0.023*\"back\" + 0.023*\"support\" + 0.021*\"retrieval\"'),\n",
       " (21,\n",
       "  '0.070*\"diagnostic\" + 0.057*\"n\" + 0.037*\"necrosis\" + 0.036*\"contact\" + 0.033*\"tumour\" + 0.031*\"traits\" + 0.026*\"color\" + 0.023*\"maximum\" + 0.021*\"growing\" + 0.020*\"hypothesis\"'),\n",
       " (22,\n",
       "  '0.062*\"reactions\" + 0.054*\"concentrations\" + 0.052*\"diffusion\" + 0.032*\"regulating\" + 0.025*\"tertiary\" + 0.025*\"mixtures\" + 0.020*\"fertility\" + 0.017*\"doses\" + 0.016*\"qualidade\" + 0.015*\"nicotine\"'),\n",
       " (23,\n",
       "  '0.084*\"immune\" + 0.046*\"sequencing\" + 0.043*\"immunodeficiency\" + 0.038*\"targeted\" + 0.038*\"responses\" + 0.032*\"virus\" + 0.028*\"leaf\" + 0.026*\"human\" + 0.024*\"response\" + 0.022*\"disruption\"'),\n",
       " (24,\n",
       "  '0.088*\"comparative\" + 0.080*\"strategies\" + 0.034*\"severity\" + 0.026*\"fungal\" + 0.025*\"spectroscopic\" + 0.023*\"representations\" + 0.019*\"subunits\" + 0.019*\"leukocyte\" + 0.018*\"supernova\" + 0.018*\"psychology\"'),\n",
       " (25,\n",
       "  '0.123*\"mortality\" + 0.040*\"diffraction\" + 0.038*\"authors\" + 0.034*\"sepsis\" + 0.027*\"mucosal\" + 0.023*\"prepared\" + 0.016*\"nas\" + 0.016*\"monoxide\" + 0.015*\"motivation\" + 0.015*\"nanostructures\"'),\n",
       " (26,\n",
       "  '0.101*\"three\" + 0.053*\"transformation\" + 0.045*\"gt\" + 0.044*\"lt\" + 0.021*\"sympathetic\" + 0.019*\"sparse\" + 0.017*\"eeg\" + 0.016*\"question\" + 0.016*\"encoded\" + 0.014*\"could\"'),\n",
       " (27,\n",
       "  '0.068*\"variation\" + 0.048*\"cycle\" + 0.047*\"science\" + 0.044*\"populations\" + 0.042*\"neurons\" + 0.035*\"testing\" + 0.031*\"fish\" + 0.029*\"issues\" + 0.024*\"procedure\" + 0.023*\"fibroblasts\"'),\n",
       " (28,\n",
       "  '0.065*\"laser\" + 0.060*\"source\" + 0.038*\"observation\" + 0.035*\"produced\" + 0.026*\"attention\" + 0.025*\"sensory\" + 0.023*\"allocation\" + 0.018*\"organizational\" + 0.018*\"closed\" + 0.017*\"dimensional\"'),\n",
       " (29,\n",
       "  '0.081*\"rna\" + 0.071*\"biological\" + 0.047*\"suppression\" + 0.041*\"polymerase\" + 0.035*\"seasonal\" + 0.023*\"theories\" + 0.019*\"phospholipase\" + 0.019*\"step\" + 0.018*\"gauge\" + 0.016*\"hormones\"'),\n",
       " (30,\n",
       "  '0.086*\"information\" + 0.071*\"change\" + 0.052*\"observations\" + 0.047*\"perspective\" + 0.047*\"climate\" + 0.029*\"key\" + 0.028*\"japanese\" + 0.027*\"resource\" + 0.025*\"tracking\" + 0.022*\"encoding\"'),\n",
       " (31,\n",
       "  '0.063*\"mapping\" + 0.063*\"towards\" + 0.048*\"channels\" + 0.046*\"partial\" + 0.041*\"algorithms\" + 0.038*\"special\" + 0.038*\"accuracy\" + 0.030*\"predicts\" + 0.026*\"five\" + 0.024*\"graph\"'),\n",
       " (32,\n",
       "  '0.073*\"preparation\" + 0.066*\"people\" + 0.046*\"living\" + 0.041*\"samples\" + 0.037*\"i\" + 0.026*\"traditional\" + 0.023*\"fluctuations\" + 0.020*\"ovary\" + 0.019*\"italy\" + 0.018*\"polycystic\"'),\n",
       " (33,\n",
       "  '0.032*\"convergence\" + 0.027*\"enhancing\" + 0.024*\"therapies\" + 0.023*\"constant\" + 0.021*\"circuits\" + 0.019*\"locally\" + 0.016*\"edema\" + 0.016*\"convective\" + 0.016*\"ubiquitin\" + 0.016*\"the\"'),\n",
       " (34,\n",
       "  '0.075*\"does\" + 0.063*\"storage\" + 0.042*\"not\" + 0.036*\"west\" + 0.029*\"extreme\" + 0.026*\"influence\" + 0.025*\"vegetation\" + 0.023*\"duct\" + 0.023*\"rational\" + 0.019*\"aerobic\"'),\n",
       " (35,\n",
       "  '0.066*\"book\" + 0.042*\"ring\" + 0.031*\"reviews\" + 0.028*\"moderate\" + 0.027*\"oncology\" + 0.026*\"review\" + 0.023*\"strategic\" + 0.022*\"plan\" + 0.021*\"regime\" + 0.020*\"principal\"'),\n",
       " (36,\n",
       "  '0.054*\"natural\" + 0.040*\"signal\" + 0.040*\"air\" + 0.036*\"significance\" + 0.035*\"dose\" + 0.031*\"mechanism\" + 0.031*\"biochemical\" + 0.030*\"developmental\" + 0.029*\"toxicity\" + 0.027*\"separation\"'),\n",
       " (37,\n",
       "  '0.063*\"radiation\" + 0.044*\"scattering\" + 0.043*\"roles\" + 0.041*\"lines\" + 0.033*\"alterations\" + 0.032*\"qualitative\" + 0.031*\"market\" + 0.028*\"comprehensive\" + 0.024*\"soft\" + 0.022*\"background\"'),\n",
       " (38,\n",
       "  '0.050*\"test\" + 0.050*\"combination\" + 0.042*\"work\" + 0.041*\"laboratory\" + 0.040*\"variants\" + 0.036*\"predict\" + 0.036*\"measuring\" + 0.031*\"mutation\" + 0.027*\"conventional\" + 0.024*\"reducing\"'),\n",
       " (39,\n",
       "  '0.037*\"and\" + 0.033*\"fraction\" + 0.025*\"leptin\" + 0.024*\"labeling\" + 0.022*\"domestic\" + 0.020*\"metallic\" + 0.020*\"ratios\" + 0.020*\"disparities\" + 0.019*\"continuum\" + 0.019*\"toxic\"'),\n",
       " (40,\n",
       "  '0.117*\"de\" + 0.096*\"la\" + 0.079*\"en\" + 0.054*\"y\" + 0.045*\"el\" + 0.042*\"volume\" + 0.040*\"del\" + 0.039*\"index\" + 0.032*\"un\" + 0.026*\"statistical\"'),\n",
       " (41,\n",
       "  '0.090*\"related\" + 0.056*\"recovery\" + 0.051*\"process\" + 0.034*\"regression\" + 0.028*\"maturation\" + 0.021*\"layers\" + 0.018*\"game\" + 0.015*\"physician\" + 0.015*\"usefulness\" + 0.014*\"to\"'),\n",
       " (42,\n",
       "  '0.074*\"public\" + 0.062*\"behavior\" + 0.050*\"health\" + 0.038*\"maintenance\" + 0.032*\"brazilian\" + 0.031*\"mutants\" + 0.029*\"classical\" + 0.026*\"requires\" + 0.024*\"risks\" + 0.021*\"physiology\"'),\n",
       " (43,\n",
       "  '0.115*\"structure\" + 0.087*\"crystal\" + 0.050*\"complexes\" + 0.044*\"implementation\" + 0.030*\"copper\" + 0.028*\"ii\" + 0.028*\"molecular\" + 0.027*\"neutron\" + 0.025*\"complex\" + 0.022*\"signalling\"'),\n",
       " (44,\n",
       "  '0.073*\"transcription\" + 0.065*\"diseases\" + 0.043*\"capacity\" + 0.035*\"formation\" + 0.033*\"tropical\" + 0.032*\"star\" + 0.030*\"factor\" + 0.026*\"cultured\" + 0.023*\"intensity\" + 0.019*\"stages\"'),\n",
       " (45,\n",
       "  '0.091*\"cases\" + 0.055*\"report\" + 0.048*\"description\" + 0.040*\"modern\" + 0.039*\"case\" + 0.037*\"microwave\" + 0.028*\"out\" + 0.028*\"two\" + 0.026*\"revision\" + 0.022*\"a\"'),\n",
       " (46,\n",
       "  '0.083*\"visual\" + 0.050*\"invasive\" + 0.047*\"correlates\" + 0.044*\"environments\" + 0.043*\"perception\" + 0.041*\"device\" + 0.030*\"extracts\" + 0.022*\"present\" + 0.021*\"cat\" + 0.019*\"quantifying\"'),\n",
       " (47,\n",
       "  '0.048*\"spectrum\" + 0.046*\"pancreatic\" + 0.043*\"markers\" + 0.041*\"purification\" + 0.040*\"disorder\" + 0.039*\"contribution\" + 0.034*\"acids\" + 0.025*\"hypertensive\" + 0.022*\"reduce\" + 0.022*\"autism\"'),\n",
       " (48,\n",
       "  '0.098*\"processes\" + 0.044*\"dental\" + 0.042*\"behaviors\" + 0.037*\"lateral\" + 0.029*\"registry\" + 0.025*\"inorganic\" + 0.022*\"epitope\" + 0.021*\"vortex\" + 0.021*\"hidden\" + 0.020*\"arsenic\"'),\n",
       " (49,\n",
       "  '0.079*\"neural\" + 0.045*\"corrigendum\" + 0.044*\"amino\" + 0.041*\"au\" + 0.039*\"acid\" + 0.033*\"complexity\" + 0.032*\"industry\" + 0.029*\"thickness\" + 0.024*\"plasticity\" + 0.024*\"business\"'),\n",
       " (50,\n",
       "  '0.068*\"techniques\" + 0.067*\"total\" + 0.037*\"estimating\" + 0.034*\"costs\" + 0.028*\"comparison\" + 0.027*\"myeloma\" + 0.025*\"score\" + 0.025*\"ndash\" + 0.025*\"fruit\" + 0.023*\"east\"'),\n",
       " (51,\n",
       "  '0.069*\"determination\" + 0.050*\"liquid\" + 0.028*\"resources\" + 0.027*\"water\" + 0.025*\"fuel\" + 0.025*\"performance\" + 0.023*\"occupational\" + 0.021*\"application\" + 0.021*\"chromatography\" + 0.020*\"bound\"'),\n",
       " (52,\n",
       "  '0.118*\"pulmonary\" + 0.056*\"particles\" + 0.049*\"marine\" + 0.040*\"quantification\" + 0.036*\"sensitive\" + 0.033*\"chronic\" + 0.027*\"obstructive\" + 0.026*\"alters\" + 0.017*\"barley\" + 0.016*\"including\"'),\n",
       " (53,\n",
       "  '0.064*\"groups\" + 0.048*\"trends\" + 0.040*\"late\" + 0.037*\"relative\" + 0.033*\"efficacy\" + 0.032*\"therapy\" + 0.025*\"city\" + 0.025*\"antiretroviral\" + 0.024*\"determining\" + 0.022*\"mobility\"'),\n",
       " (54,\n",
       "  '0.071*\"thermal\" + 0.063*\"technology\" + 0.060*\"glucose\" + 0.043*\"tolerance\" + 0.042*\"consumption\" + 0.041*\"intracellular\" + 0.040*\"mri\" + 0.029*\"velocity\" + 0.023*\"universal\" + 0.022*\"methyl\"'),\n",
       " (55,\n",
       "  '0.067*\"target\" + 0.064*\"effective\" + 0.057*\"continuous\" + 0.041*\"dynamic\" + 0.025*\"ethanol\" + 0.024*\"chains\" + 0.024*\"path\" + 0.023*\"opportunities\" + 0.022*\"soils\" + 0.018*\"resistance\"'),\n",
       " (56,\n",
       "  '0.052*\"long\" + 0.045*\"rate\" + 0.044*\"experiments\" + 0.043*\"sensing\" + 0.034*\"term\" + 0.033*\"remote\" + 0.029*\"mining\" + 0.028*\"annual\" + 0.028*\"data\" + 0.027*\"receiving\"'),\n",
       " (57,\n",
       "  '0.185*\"synthesis\" + 0.060*\"localization\" + 0.033*\"real\" + 0.031*\"aqueous\" + 0.031*\"theorem\" + 0.028*\"vertical\" + 0.027*\"eastern\" + 0.024*\"object\" + 0.021*\"good\" + 0.021*\"perceived\"'),\n",
       " (58,\n",
       "  '0.088*\"improved\" + 0.057*\"parallel\" + 0.040*\"s\" + 0.025*\"final\" + 0.022*\"atlas\" + 0.017*\"retraction\" + 0.015*\"tachycardia\" + 0.014*\"suggests\" + 0.014*\"spot\" + 0.012*\"for\"'),\n",
       " (59,\n",
       "  '0.079*\"domain\" + 0.062*\"area\" + 0.058*\"open\" + 0.052*\"replication\" + 0.044*\"four\" + 0.034*\"working\" + 0.023*\"dna\" + 0.023*\"activates\" + 0.018*\"that\" + 0.017*\"antagonists\"'),\n",
       " (60,\n",
       "  '0.042*\"involved\" + 0.040*\"regions\" + 0.039*\"distributed\" + 0.037*\"chemotherapy\" + 0.036*\"predictors\" + 0.034*\"analyses\" + 0.033*\"cortical\" + 0.029*\"undergoing\" + 0.025*\"network\" + 0.024*\"rehabilitation\"'),\n",
       " (61,\n",
       "  '0.072*\"fluid\" + 0.053*\"recurrent\" + 0.040*\"radiotherapy\" + 0.039*\"virtual\" + 0.036*\"nature\" + 0.031*\"practical\" + 0.031*\"technical\" + 0.030*\"decay\" + 0.023*\"kinases\" + 0.023*\"foreign\"'),\n",
       " (62,\n",
       "  '0.080*\"family\" + 0.058*\"men\" + 0.042*\"smooth\" + 0.041*\"have\" + 0.035*\"airway\" + 0.031*\"muscle\" + 0.026*\"actions\" + 0.025*\"measurement\" + 0.019*\"coordination\" + 0.017*\"su\"'),\n",
       " (63,\n",
       "  '0.083*\"heart\" + 0.069*\"failure\" + 0.042*\"dysfunction\" + 0.037*\"renal\" + 0.029*\"congenital\" + 0.026*\"syndrome\" + 0.025*\"with\" + 0.019*\"heavy\" + 0.019*\"patients\" + 0.019*\"in\"'),\n",
       " (64,\n",
       "  '0.078*\"generation\" + 0.044*\"retinal\" + 0.042*\"mixed\" + 0.039*\"shape\" + 0.038*\"planning\" + 0.029*\"transitions\" + 0.025*\"base\" + 0.019*\"cartilage\" + 0.018*\"tensor\" + 0.017*\"derivative\"'),\n",
       " (65,\n",
       "  '0.078*\"general\" + 0.056*\"engineering\" + 0.038*\"university\" + 0.036*\"wind\" + 0.033*\"noise\" + 0.030*\"examination\" + 0.030*\"over\" + 0.027*\"production\" + 0.023*\"property\" + 0.022*\"amyloid\"'),\n",
       " (66,\n",
       "  '0.044*\"death\" + 0.041*\"apoptosis\" + 0.039*\"inflammation\" + 0.038*\"positive\" + 0.037*\"involvement\" + 0.034*\"about\" + 0.032*\"negative\" + 0.030*\"reduces\" + 0.030*\"hormone\" + 0.029*\"chromosome\"'),\n",
       " (67,\n",
       "  '0.092*\"current\" + 0.057*\"motion\" + 0.047*\"antimicrobial\" + 0.043*\"individual\" + 0.041*\"perspectives\" + 0.040*\"electrical\" + 0.034*\"block\" + 0.025*\"metastases\" + 0.021*\"capture\" + 0.019*\"animals\"'),\n",
       " (68,\n",
       "  '0.048*\"differences\" + 0.044*\"variability\" + 0.032*\"retrospective\" + 0.031*\"future\" + 0.031*\"computational\" + 0.029*\"absorption\" + 0.029*\"stars\" + 0.025*\"gender\" + 0.025*\"the\" + 0.024*\"ocean\"'),\n",
       " (69,\n",
       "  '0.064*\"investigation\" + 0.048*\"policy\" + 0.047*\"antibody\" + 0.039*\"experimental\" + 0.032*\"injection\" + 0.032*\"cortex\" + 0.032*\"theoretical\" + 0.031*\"targets\" + 0.023*\"isolated\" + 0.021*\"viruses\"'),\n",
       " (70,\n",
       "  '0.060*\"training\" + 0.046*\"peripheral\" + 0.040*\"nitrogen\" + 0.039*\"forest\" + 0.036*\"context\" + 0.027*\"error\" + 0.026*\"por\" + 0.025*\"v\" + 0.021*\"distinct\" + 0.021*\"further\"'),\n",
       " (71,\n",
       "  '0.057*\"alcohol\" + 0.041*\"may\" + 0.036*\"interventions\" + 0.033*\"pcr\" + 0.031*\"synaptic\" + 0.030*\"academic\" + 0.028*\"procedures\" + 0.026*\"electromagnetic\" + 0.025*\"adjuvant\" + 0.025*\"glass\"'),\n",
       " (72,\n",
       "  '0.063*\"wave\" + 0.044*\"strong\" + 0.032*\"dogs\" + 0.029*\"acquisition\" + 0.029*\"sensors\" + 0.027*\"homeostasis\" + 0.024*\"investigating\" + 0.024*\"professional\" + 0.021*\"cavity\" + 0.019*\"thaliana\"'),\n",
       " (73,\n",
       "  '0.086*\"cohort\" + 0.062*\"integrated\" + 0.037*\"study\" + 0.032*\"risk\" + 0.028*\"association\" + 0.024*\"fixed\" + 0.021*\"atrophy\" + 0.021*\"echocardiography\" + 0.020*\"a\" + 0.014*\"with\"'),\n",
       " (74,\n",
       "  '0.062*\"soil\" + 0.059*\"electron\" + 0.053*\"free\" + 0.043*\"ultrasound\" + 0.040*\"composite\" + 0.039*\"microscopy\" + 0.033*\"synthetic\" + 0.028*\"scanning\" + 0.023*\"nuclei\" + 0.020*\"cross\"'),\n",
       " (75,\n",
       "  '0.063*\"trial\" + 0.054*\"systematic\" + 0.052*\"randomized\" + 0.049*\"a\" + 0.049*\"controlled\" + 0.047*\"cardiovascular\" + 0.046*\"review\" + 0.032*\"clinical\" + 0.031*\"economic\" + 0.027*\"trials\"'),\n",
       " (76,\n",
       "  '0.064*\"optical\" + 0.052*\"literature\" + 0.047*\"inflammatory\" + 0.035*\"sea\" + 0.031*\"project\" + 0.031*\"review\" + 0.030*\"access\" + 0.023*\"overview\" + 0.022*\"uk\" + 0.022*\"ice\"'),\n",
       " (77,\n",
       "  '0.054*\"quantitative\" + 0.044*\"components\" + 0.041*\"infants\" + 0.038*\"reference\" + 0.034*\"light\" + 0.032*\"hip\" + 0.031*\"peptides\" + 0.030*\"knee\" + 0.025*\"dust\" + 0.024*\"aged\"'),\n",
       " (78,\n",
       "  '0.079*\"coronary\" + 0.057*\"artery\" + 0.046*\"intervention\" + 0.042*\"materials\" + 0.038*\"diabetic\" + 0.027*\"coupled\" + 0.026*\"percutaneous\" + 0.026*\"translation\" + 0.024*\"surgery\" + 0.021*\"bypass\"'),\n",
       " (79,\n",
       "  '0.051*\"administration\" + 0.051*\"services\" + 0.048*\"service\" + 0.046*\"assessing\" + 0.040*\"enhancement\" + 0.031*\"fat\" + 0.029*\"computer\" + 0.028*\"giant\" + 0.026*\"discussion\" + 0.022*\"probe\"'),\n",
       " (80,\n",
       "  '0.068*\"measurements\" + 0.059*\"gas\" + 0.054*\"stimulation\" + 0.045*\"language\" + 0.044*\"strain\" + 0.036*\"accumulation\" + 0.033*\"antibiotic\" + 0.031*\"altered\" + 0.029*\"nucleus\" + 0.027*\"external\"'),\n",
       " (81,\n",
       "  '0.074*\"transplantation\" + 0.051*\"design\" + 0.048*\"stem\" + 0.037*\"protocol\" + 0.033*\"hematopoietic\" + 0.031*\"cell\" + 0.029*\"affects\" + 0.026*\"outcome\" + 0.026*\"dehydrogenase\" + 0.026*\"cholesterol\"'),\n",
       " (82,\n",
       "  '0.060*\"experience\" + 0.042*\"sleep\" + 0.042*\"symptoms\" + 0.040*\"urinary\" + 0.035*\"female\" + 0.032*\"macrophages\" + 0.031*\"tract\" + 0.031*\"database\" + 0.025*\"women\" + 0.025*\"barrier\"'),\n",
       " (83,\n",
       "  '0.088*\"practice\" + 0.044*\"hydrogen\" + 0.042*\"various\" + 0.041*\"used\" + 0.035*\"land\" + 0.035*\"sets\" + 0.030*\"gap\" + 0.030*\"position\" + 0.028*\"english\" + 0.023*\"considerations\"'),\n",
       " (84,\n",
       "  '0.063*\"drugs\" + 0.047*\"emergency\" + 0.037*\"eye\" + 0.033*\"contrast\" + 0.032*\"department\" + 0.024*\"multicenter\" + 0.022*\"hierarchical\" + 0.019*\"use\" + 0.018*\"degree\" + 0.017*\"palsy\"'),\n",
       " (85,\n",
       "  '0.050*\"safety\" + 0.046*\"enzyme\" + 0.043*\"hepatic\" + 0.040*\"identification\" + 0.036*\"assay\" + 0.030*\"physiological\" + 0.026*\"conversion\" + 0.024*\"home\" + 0.024*\"adult\" + 0.023*\"sequential\"'),\n",
       " (86,\n",
       "  '0.040*\"microbial\" + 0.037*\"chemistry\" + 0.037*\"solid\" + 0.030*\"adenocarcinoma\" + 0.028*\"combined\" + 0.027*\"endoscopic\" + 0.027*\"financial\" + 0.025*\"flows\" + 0.025*\"effects\" + 0.022*\"ground\"'),\n",
       " (87,\n",
       "  '0.071*\"spinal\" + 0.058*\"efficiency\" + 0.048*\"cord\" + 0.025*\"users\" + 0.020*\"codes\" + 0.020*\"blue\" + 0.018*\"accelerated\" + 0.018*\"injury\" + 0.017*\"testicular\" + 0.017*\"pairs\"'),\n",
       " (88,\n",
       "  '0.064*\"joint\" + 0.058*\"be\" + 0.036*\"should\" + 0.033*\"digital\" + 0.032*\"means\" + 0.028*\"basal\" + 0.028*\"facial\" + 0.025*\"smart\" + 0.023*\"libraries\" + 0.018*\"cyclin\"'),\n",
       " (89,\n",
       "  '0.065*\"prevalence\" + 0.064*\"life\" + 0.055*\"disorders\" + 0.046*\"space\" + 0.044*\"quality\" + 0.041*\"mechanical\" + 0.038*\"construction\" + 0.035*\"spaces\" + 0.034*\"secretion\" + 0.032*\"profiling\"'),\n",
       " (90,\n",
       "  '0.085*\"kinase\" + 0.052*\"regulates\" + 0.044*\"protein\" + 0.037*\"phosphorylation\" + 0.035*\"regional\" + 0.034*\"rare\" + 0.031*\"signaling\" + 0.022*\"invasion\" + 0.022*\"tyrosine\" + 0.019*\"progenitor\"'),\n",
       " (91,\n",
       "  '0.131*\"state\" + 0.055*\"biomarkers\" + 0.053*\"when\" + 0.024*\"disc\" + 0.019*\"your\" + 0.016*\"implantable\" + 0.015*\"biomechanical\" + 0.014*\"platinum\" + 0.013*\"coordinated\" + 0.012*\"fluctuation\"'),\n",
       " (92,\n",
       "  '0.077*\"rates\" + 0.075*\"energy\" + 0.051*\"mammalian\" + 0.048*\"balance\" + 0.034*\"face\" + 0.031*\"delay\" + 0.030*\"m\" + 0.029*\"genotype\" + 0.023*\"trace\" + 0.019*\"variational\"'),\n",
       " (93,\n",
       "  '0.046*\"course\" + 0.044*\"time\" + 0.043*\"schizophrenia\" + 0.036*\"regeneration\" + 0.031*\"k\" + 0.028*\"needs\" + 0.028*\"shear\" + 0.026*\"metals\" + 0.023*\"somatic\" + 0.022*\"attenuation\"'),\n",
       " (94,\n",
       "  '0.050*\"activated\" + 0.049*\"prognosis\" + 0.027*\"persons\" + 0.024*\"demand\" + 0.021*\"gland\" + 0.019*\"pollen\" + 0.018*\"platelets\" + 0.018*\"conduction\" + 0.017*\"connection\" + 0.017*\"professionals\"'),\n",
       " (95,\n",
       "  '0.052*\"antioxidant\" + 0.044*\"methodology\" + 0.042*\"hippocampal\" + 0.041*\"con\" + 0.035*\"und\" + 0.032*\"according\" + 0.030*\"superior\" + 0.030*\"probability\" + 0.022*\"conditional\" + 0.018*\"ensino\"'),\n",
       " (96,\n",
       "  '0.075*\"structures\" + 0.069*\"loss\" + 0.051*\"cloning\" + 0.043*\"dietary\" + 0.038*\"weight\" + 0.032*\"clusters\" + 0.030*\"brazil\" + 0.029*\"hospital\" + 0.027*\"successful\" + 0.026*\"obese\"'),\n",
       " (97,\n",
       "  '0.092*\"stability\" + 0.079*\"international\" + 0.059*\"migration\" + 0.046*\"number\" + 0.041*\"immunity\" + 0.041*\"product\" + 0.034*\"absence\" + 0.030*\"need\" + 0.029*\"squamous\" + 0.021*\"cell\"'),\n",
       " (98,\n",
       "  '0.045*\"tumors\" + 0.039*\"understanding\" + 0.038*\"american\" + 0.035*\"pediatric\" + 0.032*\"mrna\" + 0.030*\"measures\" + 0.028*\"molecules\" + 0.025*\"mesenchymal\" + 0.024*\"organization\" + 0.022*\"regulated\"'),\n",
       " (99,\n",
       "  '0.069*\"efficient\" + 0.063*\"delivery\" + 0.059*\"treated\" + 0.053*\"tool\" + 0.038*\"drug\" + 0.027*\"atherosclerosis\" + 0.027*\"distance\" + 0.024*\"salmonella\" + 0.024*\"measured\" + 0.018*\"probabilistic\"'),\n",
       " (100,\n",
       "  '0.100*\"induces\" + 0.055*\"fetal\" + 0.039*\"attenuates\" + 0.027*\"mexico\" + 0.021*\"foot\" + 0.020*\"extinction\" + 0.018*\"sign\" + 0.017*\"stents\" + 0.017*\"elementary\" + 0.017*\"entanglement\"'),\n",
       " (101,\n",
       "  '0.045*\"arthritis\" + 0.044*\"frequency\" + 0.040*\"regulatory\" + 0.036*\"rheumatoid\" + 0.032*\"improve\" + 0.028*\"constraints\" + 0.027*\"predictive\" + 0.026*\"feedback\" + 0.025*\"biology\" + 0.024*\"parameters\"'),\n",
       " (102,\n",
       "  '0.052*\"students\" + 0.041*\"atrial\" + 0.037*\"uptake\" + 0.036*\"medical\" + 0.034*\"practices\" + 0.034*\"iii\" + 0.030*\"fibrillation\" + 0.030*\"graphs\" + 0.026*\"nursing\" + 0.025*\"feeding\"'),\n",
       " (103,\n",
       "  '0.057*\"prostate\" + 0.042*\"communication\" + 0.041*\"class\" + 0.034*\"simulations\" + 0.033*\"promoter\" + 0.029*\"guidelines\" + 0.028*\"cancer\" + 0.028*\"program\" + 0.027*\"methylation\" + 0.025*\"applied\"'),\n",
       " (104,\n",
       "  '0.062*\"cognitive\" + 0.056*\"algorithm\" + 0.046*\"childhood\" + 0.044*\"transcriptional\" + 0.039*\"yeast\" + 0.037*\"increase\" + 0.033*\"core\" + 0.030*\"impairment\" + 0.026*\"regulation\" + 0.025*\"deletion\"'),\n",
       " (105,\n",
       "  '0.078*\"prevention\" + 0.052*\"scale\" + 0.036*\"characteristics\" + 0.027*\"matrices\" + 0.020*\"version\" + 0.019*\"the\" + 0.019*\"coefficients\" + 0.018*\"province\" + 0.017*\"persistence\" + 0.017*\"district\"'),\n",
       " (106,\n",
       "  '0.086*\"education\" + 0.042*\"child\" + 0.038*\"higher\" + 0.034*\"central\" + 0.030*\"system\" + 0.029*\"who\" + 0.028*\"aerosol\" + 0.028*\"nervous\" + 0.026*\"suppresses\" + 0.024*\"uncertainty\"'),\n",
       " (107,\n",
       "  '0.086*\"density\" + 0.044*\"cost\" + 0.033*\"mineral\" + 0.032*\"consequences\" + 0.031*\"man\" + 0.030*\"alloys\" + 0.029*\"experiences\" + 0.021*\"relativistic\" + 0.021*\"lens\" + 0.016*\"bone\"'),\n",
       " (108,\n",
       "  '0.081*\"environmental\" + 0.059*\"sp\" + 0.057*\"polymorphism\" + 0.048*\"research\" + 0.042*\"aging\" + 0.037*\"nov\" + 0.035*\"morphological\" + 0.032*\"cover\" + 0.031*\"genus\" + 0.027*\"dementia\"'),\n",
       " (109,\n",
       "  '0.056*\"approaches\" + 0.045*\"letter\" + 0.031*\"regarding\" + 0.031*\"article\" + 0.022*\"to\" + 0.021*\"elevation\" + 0.019*\"reperfusion\" + 0.019*\"reasoning\" + 0.014*\"manifestations\" + 0.014*\"allergy\"'),\n",
       " (110,\n",
       "  '0.094*\"critical\" + 0.079*\"linear\" + 0.069*\"sur\" + 0.049*\"kinetics\" + 0.042*\"points\" + 0.028*\"illness\" + 0.024*\"letters\" + 0.024*\"residual\" + 0.024*\"par\" + 0.024*\"immunoglobulin\"'),\n",
       " (111,\n",
       "  '0.069*\"equation\" + 0.044*\"characterization\" + 0.034*\"around\" + 0.033*\"limits\" + 0.033*\"deformation\" + 0.025*\"planar\" + 0.024*\"distal\" + 0.023*\"bond\" + 0.022*\"output\" + 0.021*\"on\"'),\n",
       " (112,\n",
       "  '0.054*\"age\" + 0.046*\"value\" + 0.041*\"prognostic\" + 0.036*\"years\" + 0.031*\"relation\" + 0.030*\"right\" + 0.029*\"cause\" + 0.027*\"gastrointestinal\" + 0.025*\"impacts\" + 0.024*\"colon\"'),\n",
       " (113,\n",
       "  '0.058*\"animal\" + 0.043*\"persistent\" + 0.041*\"revealed\" + 0.041*\"middle\" + 0.036*\"ribosomal\" + 0.029*\"circular\" + 0.021*\"biomedical\" + 0.020*\"leading\" + 0.020*\"psoriasis\" + 0.019*\"aberrant\"'),\n",
       " (114,\n",
       "  '0.060*\"rural\" + 0.039*\"maps\" + 0.036*\"calculation\" + 0.034*\"antigens\" + 0.030*\"proximal\" + 0.030*\"verification\" + 0.028*\"thermodynamic\" + 0.028*\"raman\" + 0.026*\"detector\" + 0.025*\"links\"'),\n",
       " (115,\n",
       "  '0.110*\"structural\" + 0.043*\"profiles\" + 0.042*\"studies\" + 0.038*\"ischemic\" + 0.032*\"root\" + 0.030*\"arabidopsis\" + 0.022*\"innovation\" + 0.018*\"stroke\" + 0.017*\"rotation\" + 0.017*\"regular\"'),\n",
       " (116,\n",
       "  '0.045*\"targeting\" + 0.045*\"incidence\" + 0.044*\"transmission\" + 0.037*\"products\" + 0.033*\"alzheimer\" + 0.032*\"center\" + 0.027*\"biosynthesis\" + 0.027*\"atmospheric\" + 0.026*\"subunit\" + 0.024*\"membranes\"'),\n",
       " (117,\n",
       "  '0.068*\"direct\" + 0.047*\"across\" + 0.042*\"smoking\" + 0.039*\"northern\" + 0.039*\"intake\" + 0.038*\"status\" + 0.027*\"milk\" + 0.025*\"sheep\" + 0.025*\"reveal\" + 0.024*\"patterns\"'),\n",
       " (118,\n",
       "  '0.078*\"required\" + 0.050*\"species\" + 0.035*\"decomposition\" + 0.034*\"coastal\" + 0.031*\"is\" + 0.030*\"aggregation\" + 0.026*\"for\" + 0.021*\"dielectric\" + 0.020*\"paradigm\" + 0.018*\"new\"'),\n",
       " (119,\n",
       "  '0.111*\"board\" + 0.100*\"quantum\" + 0.055*\"peptide\" + 0.035*\"complications\" + 0.032*\"polymer\" + 0.031*\"computing\" + 0.030*\"distributions\" + 0.029*\"india\" + 0.029*\"scheme\" + 0.025*\"axis\"'),\n",
       " (120,\n",
       "  '0.120*\"interactions\" + 0.063*\"iron\" + 0.055*\"sequences\" + 0.041*\"cyclic\" + 0.035*\"tissues\" + 0.034*\"length\" + 0.026*\"r\" + 0.023*\"elegans\" + 0.022*\"formulation\" + 0.020*\"war\"'),\n",
       " (121,\n",
       "  '0.064*\"lymphoma\" + 0.046*\"conservation\" + 0.033*\"fractures\" + 0.028*\"genetics\" + 0.027*\"assess\" + 0.025*\"anaesthesia\" + 0.024*\"femoral\" + 0.018*\"testosterone\" + 0.018*\"evaluate\" + 0.016*\"analgesia\"'),\n",
       " (122,\n",
       "  '0.076*\"introduction\" + 0.059*\"urban\" + 0.056*\"issue\" + 0.048*\"metastatic\" + 0.033*\"this\" + 0.030*\"residues\" + 0.030*\"segmentation\" + 0.020*\"to\" + 0.019*\"isotope\" + 0.017*\"lifetime\"'),\n",
       " (123,\n",
       "  '0.055*\"compounds\" + 0.051*\"proliferation\" + 0.033*\"empirical\" + 0.033*\"kinetic\" + 0.032*\"enzymes\" + 0.031*\"flux\" + 0.027*\"cultures\" + 0.026*\"skills\" + 0.024*\"electrochemical\" + 0.024*\"recommendations\"'),\n",
       " (124,\n",
       "  '0.074*\"young\" + 0.068*\"environment\" + 0.066*\"pathways\" + 0.041*\"adhesion\" + 0.033*\"galaxy\" + 0.033*\"binary\" + 0.029*\"molecule\" + 0.028*\"tools\" + 0.028*\"massive\" + 0.027*\"sampling\"'),\n",
       " (125,\n",
       "  '0.040*\"healthy\" + 0.036*\"mass\" + 0.033*\"improves\" + 0.033*\"diet\" + 0.032*\"software\" + 0.030*\"inhibitory\" + 0.027*\"infrared\" + 0.026*\"influences\" + 0.025*\"estrogen\" + 0.023*\"spectrometry\"'),\n",
       " (126,\n",
       "  '0.073*\"spectroscopy\" + 0.063*\"reconstruction\" + 0.049*\"surfaces\" + 0.041*\"region\" + 0.038*\"expansion\" + 0.031*\"crystals\" + 0.028*\"recombination\" + 0.026*\"symmetric\" + 0.024*\"exploration\" + 0.023*\"endometrial\"'),\n",
       " (127,\n",
       "  '0.103*\"prospective\" + 0.079*\"oxide\" + 0.055*\"nitric\" + 0.031*\"toxin\" + 0.022*\"educação\" + 0.016*\"inclusion\" + 0.015*\"exponential\" + 0.014*\"wavelet\" + 0.013*\"sports\" + 0.013*\"cement\"'),\n",
       " (128,\n",
       "  '0.066*\"mediated\" + 0.046*\"sobre\" + 0.034*\"porous\" + 0.030*\"depth\" + 0.029*\"teachers\" + 0.029*\"commercial\" + 0.028*\"games\" + 0.024*\"pulsed\" + 0.021*\"instrument\" + 0.019*\"efeito\"'),\n",
       " (129,\n",
       "  '0.108*\"stress\" + 0.054*\"oxidative\" + 0.035*\"epidemiology\" + 0.032*\"against\" + 0.029*\"protects\" + 0.025*\"asymmetric\" + 0.015*\"reactor\" + 0.015*\"nurses\" + 0.015*\"neutrophils\" + 0.014*\"methane\"'),\n",
       " (130,\n",
       "  '0.060*\"problem\" + 0.053*\"image\" + 0.051*\"national\" + 0.051*\"nonlinear\" + 0.038*\"survey\" + 0.034*\"physical\" + 0.029*\"river\" + 0.027*\"internal\" + 0.027*\"programming\" + 0.026*\"devices\"'),\n",
       " (131,\n",
       "  '0.050*\"screening\" + 0.042*\"adaptation\" + 0.042*\"exploring\" + 0.037*\"vaccination\" + 0.034*\"beyond\" + 0.033*\"psychological\" + 0.027*\"population\" + 0.024*\"soybean\" + 0.023*\"collection\" + 0.023*\"youth\"'),\n",
       " (132,\n",
       "  '0.056*\"diversity\" + 0.052*\"common\" + 0.048*\"size\" + 0.045*\"plants\" + 0.036*\"brain\" + 0.034*\"reproductive\" + 0.033*\"black\" + 0.030*\"forms\" + 0.027*\"concept\" + 0.026*\"commentary\"'),\n",
       " (133,\n",
       "  '0.068*\"vascular\" + 0.066*\"endothelial\" + 0.055*\"activities\" + 0.037*\"vitro\" + 0.036*\"vivo\" + 0.030*\"lower\" + 0.026*\"in\" + 0.020*\"topological\" + 0.019*\"potent\" + 0.017*\"and\"'),\n",
       " (134,\n",
       "  '0.058*\"obesity\" + 0.050*\"relationships\" + 0.044*\"point\" + 0.042*\"more\" + 0.039*\"multiple\" + 0.039*\"sclerosis\" + 0.035*\"fracture\" + 0.030*\"than\" + 0.026*\"delayed\" + 0.026*\"adolescent\"'),\n",
       " (135,\n",
       "  '0.064*\"how\" + 0.032*\"bayesian\" + 0.031*\"old\" + 0.029*\"can\" + 0.029*\"collagen\" + 0.028*\"limit\" + 0.027*\"zone\" + 0.025*\"composites\" + 0.024*\"active\" + 0.022*\"contributions\"'),\n",
       " (136,\n",
       "  '0.068*\"metabolism\" + 0.056*\"calcium\" + 0.056*\"site\" + 0.047*\"developing\" + 0.043*\"muscle\" + 0.042*\"skeletal\" + 0.037*\"binding\" + 0.022*\"cytochrome\" + 0.020*\"gamma\" + 0.020*\"interferon\"'),\n",
       " (137,\n",
       "  '0.090*\"correction\" + 0.075*\"kidney\" + 0.055*\"major\" + 0.029*\"upon\" + 0.026*\"library\" + 0.021*\"phylogenetic\" + 0.020*\"depressive\" + 0.019*\"disease\" + 0.019*\"operations\" + 0.018*\"fitness\"'),\n",
       " (138,\n",
       "  '0.073*\"malignant\" + 0.072*\"al\" + 0.037*\"nutritional\" + 0.034*\"america\" + 0.032*\"et\" + 0.028*\"end\" + 0.023*\"french\" + 0.021*\"inhibiting\" + 0.020*\"pet\" + 0.018*\"additive\"'),\n",
       " (139,\n",
       "  '0.069*\"reaction\" + 0.069*\"oxygen\" + 0.051*\"origin\" + 0.037*\"steel\" + 0.036*\"reactive\" + 0.021*\"surface\" + 0.018*\"electrode\" + 0.016*\"catalyst\" + 0.016*\"phys\" + 0.015*\"the\"'),\n",
       " (140,\n",
       "  '0.057*\"search\" + 0.049*\"highly\" + 0.040*\"evaluating\" + 0.039*\"sex\" + 0.037*\"embryonic\" + 0.035*\"dual\" + 0.034*\"strength\" + 0.032*\"specificity\" + 0.032*\"defects\" + 0.028*\"fever\"'),\n",
       " (141,\n",
       "  '0.065*\"mutations\" + 0.041*\"sub\" + 0.038*\"toward\" + 0.031*\"signals\" + 0.030*\"second\" + 0.030*\"synthase\" + 0.025*\"onset\" + 0.024*\"ability\" + 0.022*\"auditory\" + 0.020*\"herpes\"'),\n",
       " (142,\n",
       "  '0.077*\"applications\" + 0.051*\"therapeutic\" + 0.050*\"chemical\" + 0.046*\"framework\" + 0.041*\"recent\" + 0.034*\"one\" + 0.030*\"bladder\" + 0.028*\"advances\" + 0.025*\"marker\" + 0.024*\"year\"'),\n",
       " (143,\n",
       "  '0.053*\"validation\" + 0.053*\"intestinal\" + 0.047*\"fast\" + 0.045*\"degradation\" + 0.043*\"coupling\" + 0.029*\"carotid\" + 0.025*\"biomarker\" + 0.024*\"changing\" + 0.022*\"capital\" + 0.020*\"questionnaire\"'),\n",
       " (144,\n",
       "  '0.068*\"some\" + 0.052*\"problems\" + 0.047*\"action\" + 0.044*\"myocardial\" + 0.040*\"metabolic\" + 0.035*\"profile\" + 0.033*\"correlation\" + 0.032*\"stroke\" + 0.031*\"findings\" + 0.031*\"infarction\"'),\n",
       " (145,\n",
       "  '0.076*\"food\" + 0.053*\"recombinant\" + 0.044*\"double\" + 0.043*\"surveillance\" + 0.040*\"relations\" + 0.026*\"connectivity\" + 0.025*\"british\" + 0.025*\"canada\" + 0.024*\"glycoprotein\" + 0.021*\"adenovirus\"'),\n",
       " (146,\n",
       "  '0.081*\"culture\" + 0.055*\"determinants\" + 0.030*\"timing\" + 0.027*\"epidemiological\" + 0.026*\"identity\" + 0.023*\"metric\" + 0.020*\"superconducting\" + 0.019*\"uv\" + 0.017*\"umbilical\" + 0.016*\"reinforced\"'),\n",
       " (147,\n",
       "  '0.273*\"editorial\" + 0.073*\"classification\" + 0.069*\"china\" + 0.034*\"dark\" + 0.029*\"resistant\" + 0.029*\"familial\" + 0.027*\"antibacterial\" + 0.019*\"interleukin\" + 0.019*\"plasmid\" + 0.017*\"matter\"'),\n",
       " (148,\n",
       "  '0.062*\"galaxies\" + 0.047*\"before\" + 0.039*\"map\" + 0.037*\"view\" + 0.034*\"publisher\" + 0.025*\"substitution\" + 0.022*\"establishment\" + 0.022*\"ventilation\" + 0.020*\"surveys\" + 0.018*\"direction\"'),\n",
       " (149,\n",
       "  '0.075*\"memory\" + 0.053*\"exchange\" + 0.040*\"tomography\" + 0.034*\"computed\" + 0.034*\"proton\" + 0.033*\"resection\" + 0.031*\"beam\" + 0.029*\"array\" + 0.026*\"whole\" + 0.024*\"detection\"'),\n",
       " (150,\n",
       "  '0.115*\"functions\" + 0.053*\"predicting\" + 0.044*\"component\" + 0.024*\"cytotoxicity\" + 0.022*\"holes\" + 0.021*\"interfaces\" + 0.015*\"black\" + 0.015*\"autoantibodies\" + 0.015*\"mating\" + 0.014*\"osteosarcoma\"'),\n",
       " (151,\n",
       "  '0.076*\"isolation\" + 0.063*\"lipid\" + 0.049*\"stage\" + 0.041*\"reliability\" + 0.027*\"autophagy\" + 0.024*\"lymphocyte\" + 0.023*\"personal\" + 0.022*\"pig\" + 0.022*\"routine\" + 0.021*\"microrna\"'),\n",
       " (152,\n",
       "  '0.062*\"carcinoma\" + 0.048*\"l\" + 0.037*\"generalized\" + 0.035*\"hepatocellular\" + 0.030*\"breast\" + 0.028*\"metastasis\" + 0.027*\"cancer\" + 0.026*\"methods\" + 0.025*\"differential\" + 0.025*\"survival\"'),\n",
       " (153,\n",
       "  '0.062*\"particle\" + 0.027*\"stent\" + 0.026*\"affinity\" + 0.025*\"sulfate\" + 0.022*\"breeding\" + 0.022*\"invariant\" + 0.021*\"reality\" + 0.018*\"topical\" + 0.014*\"the\" + 0.014*\"manganese\"'),\n",
       " (154,\n",
       "  '0.091*\"prediction\" + 0.055*\"reveals\" + 0.041*\"situ\" + 0.036*\"carbon\" + 0.026*\"diverse\" + 0.022*\"dispersion\" + 0.021*\"nanotubes\" + 0.019*\"origins\" + 0.019*\"analysis\" + 0.018*\"section\"'),\n",
       " (155,\n",
       "  '0.063*\"gastric\" + 0.057*\"inhibitor\" + 0.041*\"short\" + 0.040*\"growth\" + 0.039*\"poly\" + 0.028*\"epidermal\" + 0.026*\"load\" + 0.025*\"factor\" + 0.024*\"statistics\" + 0.023*\"cancers\"'),\n",
       " (156,\n",
       "  '0.047*\"heat\" + 0.032*\"lesions\" + 0.032*\"pilot\" + 0.032*\"random\" + 0.031*\"pain\" + 0.030*\"host\" + 0.029*\"shock\" + 0.028*\"diagnosis\" + 0.027*\"spectra\" + 0.027*\"behavioral\"'),\n",
       " (157,\n",
       "  '0.071*\"matrix\" + 0.067*\"modulation\" + 0.053*\"motor\" + 0.050*\"protection\" + 0.047*\"caused\" + 0.029*\"porcine\" + 0.027*\"cultural\" + 0.025*\"curves\" + 0.024*\"van\" + 0.024*\"integrating\"'),\n",
       " (158,\n",
       "  '0.060*\"induction\" + 0.058*\"due\" + 0.042*\"southern\" + 0.031*\"utilization\" + 0.028*\"estimates\" + 0.027*\"tree\" + 0.026*\"yield\" + 0.023*\"birth\" + 0.022*\"low\" + 0.019*\"to\"'),\n",
       " (159,\n",
       "  '0.055*\"containing\" + 0.044*\"domains\" + 0.042*\"transplant\" + 0.041*\"unusual\" + 0.036*\"programs\" + 0.030*\"g\" + 0.030*\"healthcare\" + 0.028*\"organ\" + 0.028*\"recipients\" + 0.026*\"discharge\"'),\n",
       " (160,\n",
       "  '0.058*\"mutant\" + 0.046*\"update\" + 0.045*\"locus\" + 0.034*\"france\" + 0.031*\"estado\" + 0.029*\"best\" + 0.025*\"cycles\" + 0.022*\"drought\" + 0.022*\"crohn\" + 0.021*\"thermodynamics\"'),\n",
       " (161,\n",
       "  '0.065*\"c\" + 0.059*\"hepatitis\" + 0.058*\"b\" + 0.046*\"hiv\" + 0.039*\"antigen\" + 0.031*\"humans\" + 0.029*\"stochastic\" + 0.028*\"infection\" + 0.028*\"controls\" + 0.028*\"virus\"'),\n",
       " (162,\n",
       "  '0.077*\"hypertension\" + 0.069*\"essential\" + 0.039*\"oil\" + 0.030*\"fatigue\" + 0.029*\"soluble\" + 0.026*\"industrial\" + 0.022*\"autologous\" + 0.021*\"treatments\" + 0.017*\"tumours\" + 0.015*\"modulated\"'),\n",
       " (163,\n",
       "  '0.069*\"transfer\" + 0.050*\"numerical\" + 0.044*\"repair\" + 0.040*\"simultaneous\" + 0.033*\"subjects\" + 0.033*\"abdominal\" + 0.032*\"plant\" + 0.032*\"modeling\" + 0.030*\"fluorescence\" + 0.026*\"catalytic\"'),\n",
       " (164,\n",
       "  '0.044*\"preliminary\" + 0.042*\"european\" + 0.042*\"agents\" + 0.040*\"derivatives\" + 0.036*\"dependence\" + 0.035*\"theory\" + 0.035*\"assembly\" + 0.028*\"unit\" + 0.025*\"infectious\" + 0.025*\"intensive\"'),\n",
       " (165,\n",
       "  '0.068*\"respiratory\" + 0.056*\"strategy\" + 0.052*\"viral\" + 0.038*\"teaching\" + 0.037*\"western\" + 0.033*\"agent\" + 0.030*\"lessons\" + 0.028*\"increasing\" + 0.026*\"notes\" + 0.023*\"hybridization\"'),\n",
       " (166,\n",
       "  '0.058*\"emission\" + 0.042*\"independent\" + 0.038*\"oxidation\" + 0.033*\"heterogeneity\" + 0.031*\"expressed\" + 0.025*\"galactic\" + 0.025*\"differentially\" + 0.023*\"interactive\" + 0.022*\"ecosystem\" + 0.021*\"ligands\"'),\n",
       " (167,\n",
       "  '0.068*\"sensitivity\" + 0.067*\"colorectal\" + 0.053*\"murine\" + 0.034*\"fiber\" + 0.031*\"p\" + 0.030*\"cancer\" + 0.023*\"logic\" + 0.019*\"substance\" + 0.019*\"candida\" + 0.018*\"biliary\"'),\n",
       " (168,\n",
       "  '0.102*\"interaction\" + 0.073*\"line\" + 0.071*\"damage\" + 0.022*\"act\" + 0.020*\"cell\" + 0.019*\"development\" + 0.018*\"germ\" + 0.014*\"the\" + 0.013*\"cerebrovascular\" + 0.012*\"fecal\"'),\n",
       " (169,\n",
       "  '0.064*\"infections\" + 0.049*\"nerve\" + 0.031*\"pseudomonas\" + 0.030*\"challenge\" + 0.029*\"alpha\" + 0.028*\"acoustic\" + 0.024*\"canadian\" + 0.023*\"aeruginosa\" + 0.021*\"inhibit\" + 0.021*\"aids\"'),\n",
       " (170,\n",
       "  '0.063*\"susceptibility\" + 0.058*\"influenza\" + 0.052*\"vaccine\" + 0.040*\"une\" + 0.039*\"web\" + 0.037*\"wall\" + 0.025*\"anterior\" + 0.021*\"trees\" + 0.020*\"chest\" + 0.018*\"personality\"'),\n",
       " (171,\n",
       "  '0.114*\"diabetes\" + 0.061*\"type\" + 0.050*\"channel\" + 0.039*\"media\" + 0.036*\"mellitus\" + 0.036*\"layer\" + 0.031*\"progress\" + 0.023*\"perceptions\" + 0.022*\"consensus\" + 0.021*\"epithelial\"'),\n",
       " (172,\n",
       "  '0.086*\"sequence\" + 0.068*\"genome\" + 0.067*\"transition\" + 0.049*\"sites\" + 0.046*\"reduced\" + 0.046*\"complete\" + 0.023*\"level\" + 0.022*\"symmetry\" + 0.014*\"horizontal\" + 0.014*\"the\"'),\n",
       " (173,\n",
       "  '0.052*\"polymorphisms\" + 0.042*\"ratio\" + 0.039*\"nucleotide\" + 0.037*\"membrane\" + 0.028*\"speech\" + 0.027*\"limb\" + 0.025*\"single\" + 0.025*\"gene\" + 0.020*\"outer\" + 0.018*\"defense\"'),\n",
       " (174,\n",
       "  '0.075*\"estimation\" + 0.047*\"pattern\" + 0.047*\"temporal\" + 0.046*\"cluster\" + 0.040*\"improving\" + 0.039*\"effectiveness\" + 0.037*\"spatial\" + 0.030*\"range\" + 0.025*\"choice\" + 0.024*\"parameter\"'),\n",
       " (175,\n",
       "  '0.075*\"skin\" + 0.074*\"systemic\" + 0.044*\"both\" + 0.037*\"lupus\" + 0.029*\"there\" + 0.028*\"mammary\" + 0.026*\"erythematosus\" + 0.026*\"crisis\" + 0.025*\"underlying\" + 0.020*\"salivary\"'),\n",
       " (176,\n",
       "  '0.149*\"erratum\" + 0.094*\"modelling\" + 0.049*\"impaired\" + 0.030*\"glioma\" + 0.022*\"allergic\" + 0.020*\"motif\" + 0.020*\"specimens\" + 0.019*\"indicator\" + 0.015*\"aspectos\" + 0.015*\"bonding\"'),\n",
       " (177,\n",
       "  '0.067*\"surgical\" + 0.065*\"technique\" + 0.038*\"bovine\" + 0.032*\"online\" + 0.031*\"community\" + 0.029*\"management\" + 0.028*\"exercise\" + 0.027*\"fuzzy\" + 0.023*\"fine\" + 0.020*\"neutral\"'),\n",
       " (178,\n",
       "  '0.089*\"et\" + 0.088*\"de\" + 0.077*\"la\" + 0.072*\"des\" + 0.068*\"le\" + 0.065*\"les\" + 0.065*\"du\" + 0.046*\"à\" + 0.036*\"dans\" + 0.031*\"abstract\"'),\n",
       " (179,\n",
       "  '0.071*\"progression\" + 0.063*\"sources\" + 0.054*\"variations\" + 0.034*\"induce\" + 0.031*\"bis\" + 0.029*\"nephropathy\" + 0.023*\"mixture\" + 0.020*\"anomalies\" + 0.018*\"intraoperative\" + 0.014*\"make\"'),\n",
       " (180,\n",
       "  '0.100*\"magnetic\" + 0.077*\"serum\" + 0.059*\"resonance\" + 0.050*\"pregnancy\" + 0.041*\"imaging\" + 0.034*\"maternal\" + 0.027*\"approximation\" + 0.027*\"field\" + 0.026*\"ve\" + 0.026*\"presenting\"'),\n",
       " (181,\n",
       "  '0.071*\"reduction\" + 0.055*\"part\" + 0.055*\"longitudinal\" + 0.052*\"ion\" + 0.048*\"drosophila\" + 0.039*\"society\" + 0.038*\"radio\" + 0.027*\"chloride\" + 0.025*\"internet\" + 0.024*\"ocular\"'),\n",
       " (182,\n",
       "  '0.068*\"selection\" + 0.068*\"optimization\" + 0.043*\"bacteria\" + 0.039*\"initial\" + 0.038*\"extraction\" + 0.031*\"automatic\" + 0.023*\"past\" + 0.022*\"better\" + 0.020*\"cytoplasmic\" + 0.018*\"nuclear\"'),\n",
       " (183,\n",
       "  '0.091*\"increased\" + 0.084*\"composition\" + 0.055*\"temperature\" + 0.045*\"upper\" + 0.045*\"selected\" + 0.027*\"precipitation\" + 0.022*\"solving\" + 0.020*\"dry\" + 0.019*\"prostaglandin\" + 0.018*\"mimicking\"'),\n",
       " (184,\n",
       "  '0.055*\"equations\" + 0.055*\"solutions\" + 0.047*\"bone\" + 0.040*\"finite\" + 0.034*\"element\" + 0.028*\"marrow\" + 0.027*\"communities\" + 0.023*\"difference\" + 0.022*\"gravity\" + 0.021*\"platform\"'),\n",
       " (185,\n",
       "  '0.072*\"history\" + 0.036*\"alloy\" + 0.036*\"phase\" + 0.032*\"new\" + 0.032*\"distribution\" + 0.031*\"dynamics\" + 0.031*\"heterogeneous\" + 0.024*\"canine\" + 0.021*\"adsorption\" + 0.020*\"semantic\"'),\n",
       " (186,\n",
       "  '0.057*\"south\" + 0.044*\"resolution\" + 0.040*\"africa\" + 0.034*\"green\" + 0.032*\"variant\" + 0.032*\"editor\" + 0.032*\"variable\" + 0.032*\"compared\" + 0.026*\"which\" + 0.024*\"pharmacological\"'),\n",
       " (187,\n",
       "  '0.097*\"relationship\" + 0.043*\"concentration\" + 0.041*\"between\" + 0.026*\"tissue\" + 0.026*\"the\" + 0.026*\"transport\" + 0.026*\"important\" + 0.024*\"epilepsy\" + 0.021*\"potassium\" + 0.021*\"dimensions\"'),\n",
       " (188,\n",
       "  '0.135*\"social\" + 0.083*\"features\" + 0.032*\"amplification\" + 0.032*\"transforming\" + 0.027*\"stellar\" + 0.026*\"hemodialysis\" + 0.023*\"microscopic\" + 0.021*\"immunological\" + 0.019*\"structured\" + 0.017*\"responsible\"'),\n",
       " (189,\n",
       "  '0.058*\"cellular\" + 0.046*\"medicine\" + 0.043*\"contents\" + 0.034*\"strains\" + 0.032*\"bacterial\" + 0.030*\"table\" + 0.029*\"induced\" + 0.023*\"potentials\" + 0.021*\"traffic\" + 0.021*\"equilibrium\"'),\n",
       " (190,\n",
       "  '0.078*\"power\" + 0.043*\"melanoma\" + 0.041*\"sodium\" + 0.039*\"sexual\" + 0.035*\"mode\" + 0.034*\"standard\" + 0.031*\"movement\" + 0.030*\"fatty\" + 0.028*\"osteoarthritis\" + 0.028*\"liver\"'),\n",
       " (191,\n",
       "  '0.055*\"promotes\" + 0.045*\"neonatal\" + 0.037*\"activation\" + 0.037*\"tests\" + 0.036*\"presentation\" + 0.032*\"pathway\" + 0.029*\"lead\" + 0.024*\"fluorescent\" + 0.023*\"through\" + 0.023*\"orientation\"'),\n",
       " (192,\n",
       "  '0.067*\"inhibitors\" + 0.059*\"male\" + 0.056*\"severe\" + 0.045*\"aortic\" + 0.038*\"valve\" + 0.032*\"histone\" + 0.027*\"implantation\" + 0.024*\"stenosis\" + 0.016*\"analogues\" + 0.016*\"intact\"'),\n",
       " (193,\n",
       "  '0.067*\"note\" + 0.065*\"simple\" + 0.058*\"possible\" + 0.053*\"improvement\" + 0.050*\"dos\" + 0.037*\"comparing\" + 0.032*\"estudo\" + 0.029*\"saúde\" + 0.028*\"juvenile\" + 0.026*\"caso\"'),\n",
       " (194,\n",
       "  '0.052*\"chinese\" + 0.047*\"basis\" + 0.040*\"interface\" + 0.031*\"electric\" + 0.029*\"radical\" + 0.028*\"brief\" + 0.025*\"immunohistochemical\" + 0.024*\"recurrence\" + 0.021*\"peritoneal\" + 0.020*\"dialysis\"'),\n",
       " (195,\n",
       "  '0.070*\"adults\" + 0.049*\"older\" + 0.040*\"learning\" + 0.036*\"importance\" + 0.031*\"t\" + 0.031*\"types\" + 0.027*\"deposition\" + 0.026*\"lymphocytes\" + 0.018*\"antitumor\" + 0.016*\"detected\"'),\n",
       " (196,\n",
       "  '0.069*\"receptors\" + 0.062*\"adaptive\" + 0.054*\"alternative\" + 0.050*\"spontaneous\" + 0.041*\"asthma\" + 0.041*\"chain\" + 0.035*\"pathogenesis\" + 0.034*\"building\" + 0.031*\"geometry\" + 0.030*\"supply\"'),\n",
       " (197,\n",
       "  '0.044*\"elderly\" + 0.043*\"depression\" + 0.037*\"spectral\" + 0.033*\"aspects\" + 0.032*\"red\" + 0.032*\"head\" + 0.028*\"neck\" + 0.024*\"indian\" + 0.023*\"tobacco\" + 0.022*\"cutaneous\"'),\n",
       " (198,\n",
       "  '0.139*\"evolution\" + 0.032*\"extension\" + 0.029*\"agricultural\" + 0.025*\"fabrication\" + 0.024*\"landscape\" + 0.023*\"standards\" + 0.021*\"decisions\" + 0.021*\"treating\" + 0.020*\"dysplasia\" + 0.019*\"combustion\"'),\n",
       " (199,\n",
       "  '0.066*\"ventricular\" + 0.060*\"left\" + 0.032*\"cardiac\" + 0.025*\"identifies\" + 0.025*\"function\" + 0.023*\"inequalities\" + 0.023*\"minimal\" + 0.021*\"colitis\" + 0.019*\"antagonist\" + 0.015*\"with\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
